# Configuration de l'Agent RL pour ADAN

# Paramètres généraux de l'agent
agent:
  # Algorithme RL: 'ppo', 'a2c', 'sac', 'td3'
  algorithm: "ppo"
  # Type de politique: 'MlpPolicy', 'CnnPolicy', 'MultiInputPolicy'
  policy_type: "MultiInputPolicy"
  # Nombre total d'étapes d'entraînement
  total_timesteps: 1000000
  # Graine aléatoire pour la reproductibilité
  seed: 42
  # Niveau de verbosité: 0 (silencieux), 1 (entraînement), 2 (debug)
  verbose: 1
  # Activer/désactiver le mode déterministe pour l'inférence
  deterministic_inference: true
  # Fréquence des logs personnalisés (en nombre de rollouts)
  custom_log_freq_rollouts: 1
  
  # Configuration de l'extracteur de features CNN
  features_extractor_kwargs:
    features_dim: 64  # Dimension de sortie de l'extracteur de features
    num_input_channels: 1  # Nombre de canaux d'entrée pour le CNN
    cnn_config:
      conv_layers:
        - {out_channels: 32, kernel_size: 3, stride: 1, padding: 1}
        - {out_channels: 64, kernel_size: 3, stride: 1, padding: 1}
      pool_layers:
        - {kernel_size: 2, stride: 2}
        - {kernel_size: 2, stride: 2}
      activation: "relu"
      dropout: 0.2
      fc_layers: [128]

# Hyperparamètres communs
policy:
  # Architecture du réseau de neurones
  net_arch:
    pi: [128, 64]  # Réseau de politique (actor)
    vf: [128, 64]  # Réseau de valeur (critic)
  # Fonction d'activation: 'tanh', 'relu', 'elu'
  activation_fn: "tanh"
  # Facteur de dépréciation des récompenses futures
  gamma: 0.99
  # Taux d'apprentissage
  learning_rate: 0.0003
  # Planificateur de taux d'apprentissage: 'constant', 'linear'
  lr_schedule: "constant"
  # Nombre d'environnements parallèles
  n_envs: 1
  # Normalisation des observations
  normalize_observations: true
  # Normalisation des récompenses
  normalize_rewards: true
  # Clip des gradients
  max_grad_norm: 0.5

# Hyperparamètres spécifiques à PPO
ppo:
  # Nombre de pas de temps avant mise à jour
  n_steps: 2048
  # Taille du batch
  batch_size: 64
  # Nombre d'époques par mise à jour
  n_epochs: 10
  # Paramètre de clipping
  clip_range: 0.2
  # Coefficient d'entropie (pour encourager l'exploration)
  ent_coef: 0.01
  # Coefficient de la fonction de valeur
  vf_coef: 0.5
  # Paramètre GAE (Generalized Advantage Estimation)
  gae_lambda: 0.95
  # Normalisation des avantages
  normalize_advantage: true
  # Clip de la fonction de valeur
  clip_vf: true

# Hyperparamètres spécifiques à SAC (si utilisé)
sac:
  # Taille du buffer de replay
  buffer_size: 1000000
  # Taille du batch
  batch_size: 256
  # Taux d'apprentissage pour la politique
  learning_rate_policy: 0.0003
  # Taux d'apprentissage pour la fonction Q
  learning_rate_q: 0.0003
  # Coefficient d'entropie
  ent_coef: "auto"
  # Fréquence de mise à jour de la cible
  target_update_interval: 1
  # Tau pour la mise à jour douce
  tau: 0.005
  # Fréquence d'entraînement
  train_freq: 1
  # Nombre d'étapes de gradient
  gradient_steps: 1

# Architecture du réseau de neurones
network:
  # Architecture de la politique (tailles des couches cachées)
  policy_net_arch: [256, 256]
  # Architecture de la fonction de valeur (tailles des couches cachées)
  value_net_arch: [256, 256]
  # Fonction d'activation: 'tanh', 'relu', 'elu', 'leaky_relu'
  activation_fn: "tanh"
  # Utiliser des réseaux partagés ou séparés pour la politique et la valeur
  shared_network: false
  # Facteur de régularisation L2
  l2_reg: 0.0001
  # Taux de dropout (si > 0)
  dropout_rate: 0.0
  # Normalisation des couches: 'none', 'batch', 'layer'
  normalization: "none"

# Configuration de l'exploration
exploration:
  # Stratégie d'exploration: 'default', 'epsilon_greedy', 'boltzmann'
  exploration_strategy: "default"
  # Valeur initiale d'epsilon (pour epsilon-greedy)
  initial_epsilon: 1.0
  # Valeur finale d'epsilon
  final_epsilon: 0.05
  # Nombre d'étapes pour la décroissance d'epsilon
  epsilon_decay_steps: 100000
  # Température pour l'exploration de Boltzmann
  temperature: 1.0

# Configuration des callbacks
callbacks:
  # Fréquence d'évaluation (en pas de temps)
  eval_freq: 10000
  # Nombre d'épisodes pour l'évaluation
  n_eval_episodes: 5
  # Fréquence de sauvegarde des modèles (en pas de temps)
  save_freq: 50000
  # Activer/désactiver le callback d'arrêt anticipé
  enable_early_stopping: true
  # Patience pour l'arrêt anticipé (en évaluations)
  early_stopping_patience: 5
