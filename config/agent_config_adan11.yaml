# Configuration de l'Agent pour ADAN 1.1

# Type d'agent à utiliser
agent_type: "PPO"

# Configuration PPO pour ADAN 1.1
ppo:
  learning_rate: 3e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: true
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  use_sde: false
  sde_sample_freq: -1
  target_kl: null
  tensorboard_log: "./reports/tensorboard_logs/"
  create_eval_env: false
  policy_kwargs:
    log_std_init: 0.0
    ortho_init: false
    activation_fn: "tanh"
    net_arch:
      pi: [256, 256]
      vf: [256, 256]

# Configuration du device
device: "auto"  # auto, cpu, cuda

# Configuration de l'entraînement pour ADAN 1.1
training:
  total_timesteps: 100000
  eval_freq: 10000
  n_eval_episodes: 5
  eval_log_path: "./reports/eval_logs/"
  save_freq: 10000
  save_path: "./models/"
  
# Configuration de la politique
policy:
  type: "MultiInputPolicy"
  
# Configuration du monitoring
monitoring:
  log_interval: 100
  progress_bar: true
  
# Configuration de l'évaluation
evaluation:
  deterministic: true
  render: false
  
# Configuration des callbacks
callbacks:
  checkpoint:
    save_freq: 10000
    save_path: "./models/checkpoints/"
    name_prefix: "ppo_trading"
  eval:
    eval_freq: 10000
    n_eval_episodes: 5
    best_model_save_path: "./models/best_model/"
    log_path: "./reports/eval_logs/"
    deterministic: true
  tensorboard:
    log_dir: "./reports/tensorboard_logs/"