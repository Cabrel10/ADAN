# Configuration de l'Agent RL pour ADAN - Version 3.0 (Optimisé CPU)

# Paramètres généraux de l'agent
agent:
  # Algorithme RL: 'ppo', 'a2c', 'sac', 'td3'
  algorithm: "ppo"
  # Type de politique: 'MlpPolicy', 'CnnPolicy', 'MultiInputPolicy', 'MultiInputCnnPolicy'
  policy_type: "MultiInputPolicy"
  # Nombre total d'étapes d'entraînement (adapté pour CPU)
  total_timesteps: 500000
  # Graine aléatoire pour la reproductibilité
  seed: 42
  # Niveau de verbosité: 0 (silencieux), 1 (entraînement), 2 (debug)
  verbose: 1
  # Activer/désactiver le mode déterministe pour l'inférence
  deterministic_inference: true
  # Fréquence des logs personnalisés (en nombre de rollouts)
  custom_log_freq_rollouts: 10
  # Fréquence d'évaluation (en nombre de timesteps)
  eval_freq: 25000
  # Fréquence de sauvegarde des checkpoints (en nombre de timesteps)
  checkpoint_freq: 50000
  # Nombre d'environnements parallèles pour l'entraînement
  n_envs: 1
  # Taille du batch pour la mise à jour du modèle
  batch_size: 64
  # Taille du buffer de rejeu (si applicable)
  buffer_size: 100000
  # Taille de la fenêtre d'observation (nombre de chandelles)
  window_size: 100
  
  # Configuration de l'extracteur de features pour données multi-timeframe
  features_extractor_kwargs:
    # Configuration pour chaque canal (timeframe)
    cnn_configs:
      # Configuration pour le timeframe 1m
      "1m":
        input_shape: [1, 100, 10]  # [channels, window_size, num_features]
        conv_layers:
          - {filters: 32, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
          - {filters: 64, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
        pool_layers:
          - {pool_size: 2, strides: 2}
        dropout: 0.2
        
      # Configuration pour le timeframe 1h
      "1h":
        input_shape: [1, 100, 10]  # [channels, window_size, num_features]
        conv_layers:
          - {filters: 32, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
          - {filters: 64, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
        pool_layers:
          - {pool_size: 2, strides: 2}
        dropout: 0.2
        
      # Configuration pour le timeframe 3h
      "3h":
        input_shape: [1, 100, 8]  # [channels, window_size, num_features]
        conv_layers:
          - {filters: 16, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
          - {filters: 32, kernel_size: 3, strides: 1, padding: 'same', activation: 'relu'}
        pool_layers:
          - {pool_size: 2, strides: 2}
        dropout: 0.1
    
    # Couches pleinement connectées après la concaténation des features
    fc_layers: [256, 128]
    dropout: 0.2
    activation: "leaky_relu"

# Hyperparamètres communs
policy:
  # Architecture du réseau de neurones (optimisée pour multi-timeframe)
  net_arch:
    # Réseau partagé (avant la séparation acteur/critique)
    shared: [256, 128]
    # Réseau de politique (actor)
    pi: [64, 32]
    # Réseau de valeur (critic)
    vf: [64, 32]
    
  # Fonction d'activation: 'tanh', 'relu', 'leaky_relu', 'elu'
  activation_fn: "leaky_relu"
  # Facteur de dépréciation des récompenses futures
  gamma: 0.99
  # Facteur GAE (Generalized Advantage Estimation)
  gae_lambda: 0.95
  # Facteur d'entropie (encourage l'exploration)
  ent_coef: 0.01
  # Poids de la fonction de coût de l'entropie
  ent_coef_annealing: True
  # Taux d'apprentissage initial
  learning_rate: 3e-4
  # Planificateur de taux d'apprentissage: 'constant', 'linear', 'cosine'
  learning_rate_schedule: 'linear'
  # Nombre d'étapes pour la décroissance du taux d'apprentissage
  n_steps: 2048
  # Taille du batch pour la mise à jour du modèle
  batch_size: 64
  # Nombre d'époques pour la mise à jour du modèle
  n_epochs: 10
  # Facteur d'échantillonnage avant la mise à jour du modèle
  nminibatches: 4
  # Coefficient de clipping pour PPO
  clip_range: 0.2
  # Coefficient de clipping pour la fonction de valeur
  clip_range_vf: 0.2
  # Coefficient de régularisation L2
  l2_penalty: 0.0
  # Taux de dropout
  dropout: 0.1
  # Utilisation ou non de la normalisation par lots (batch normalization)
  batch_norm: True
  # Taux de mise à jour du réseau cible (pour DDPG/TD3/SAC)
  target_update_interval: 1
  # Fréquence de mise à jour du réseau cible
  target_update_freq: 1000
  # Taux de bruit pour l'exploration
  exploration_noise: 0.1
  # Type de bruit d'exploration: 'ou', 'normal', 'adaptive_parameter'
  exploration_noise_type: 'normal'
  # Paramètres du bruit d'Ornstein-Uhlenbeck (si utilisé)
  ou_params:
    mu: 0.0
    theta: 0.15
    sigma: 0.2
  # Taux d'oubli pour les statistiques de normalisation
  norm_obs_alpha: 0.001
  # Nombre d'étapes initiales avec exploration aléatoire
  random_exploration: 10000
  # Stratégie d'exploration: 'epsilon_greedy', 'gaussian', 'ornstein_uhlenbeck'
  exploration_strategy: 'gaussian'
  # Nombre d'environnements parallèles
  n_envs: 1
  # Normalisation des observations
  normalize_observations: false
  # Normalisation des récompenses
  normalize_rewards: false

# Hyperparamètres spécifiques à PPO (Proximal Policy Optimization)
ppo:
  # Nombre d'étapes de collecte avant mise à jour
  n_steps: 2048  # Augmenté pour plus de stabilité
  # Taille du batch pour l'optimisation
  batch_size: 64
  # Nombre d'époques d'optimisation par mise à jour
  n_epochs: 10
  # Paramètre GAE-Lambda pour l'estimation d'avantage
  gae_lambda: 0.95
  # Paramètre de clipping pour PPO (peut être une valeur ou un tuple (initial, final))
  clip_range: 0.2
  # Clipping de la fonction de valeur (None pour désactiver)
  clip_range_vf: None
  # Coefficient d'entropie (peut être une valeur ou un tuple (initial, final))
  ent_coef: 0.01
  # Coefficient de la fonction de valeur
  vf_coef: 0.5
  # Norme maximale du gradient (pour le clipping du gradient)
  max_grad_norm: 0.5
  # Utiliser la normalisation d'avantage
  normalize_advantage: True
  # Utiliser State-Dependent Exploration (SDE)
  use_sde: False
  # Fréquence de rééchantillonnage pour SDE (-1 pour ne pas rééchantillonner)
  sde_sample_freq: -1
  # Utiliser le reparamétrage du gradient
  use_reparameterization: False

# Hyperparamètres spécifiques à A2C (Advantage Actor-Critic)
a2c:
  # Nombre d'étapes de collecte avant mise à jour
  n_steps: 5
  # Normalisation de l'avantage
  normalize_advantage: True
  # Taux d'apprentissage RMSProp
  rms_prop_eps: 1e-5
  # Utiliser RMS Prop au lieu d'Adam
  use_rms_prop: True
  # Paramètre GAE-Lambda pour l'estimation d'avantage
  gae_lambda: 1.0
  # Coefficient d'entropie (peut être une valeur ou un tuple (initial, final))
  ent_coef: 0.01
  # Coefficient de la fonction de valeur
  vf_coef: 0.5
  # Norme maximale du gradient (pour le clipping du gradient)
  max_grad_norm: 0.5
  # Utiliser State-Dependent Exploration (SDE)
  use_sde: False
  # Fréquence de rééchantillonnage pour SDE (-1 pour ne pas rééchantillonner)
  sde_sample_freq: -1
  # Utiliser le reparamétrage du gradient
  use_reparameterization: False

# Hyperparamètres spécifiques à SAC (Soft Actor-Critic)
sac:
  # Taille du buffer de replay
  buffer_size: 1000000
  # Taille du batch pour l'optimisation
  batch_size: 256
  
  # Taux d'apprentissage
  learning_rate: 0.0003  # Pour l'acteur
  learning_rate_q: 0.0003  # Pour le critique
  learning_rate_entropy: 0.0003  # Pour l'entropie
  
  # Paramètres d'entropie
  alpha: 0.2  # Température d'entropie initiale
  auto_alpha: True  # Ajustement automatique de l'entropie
  target_entropy: 'auto'  # Cible d'entropie (None pour désactiver)
  
  # Paramètres de mise à jour
  gamma: 0.99  # Facteur de dépréciation
  tau: 0.005  # Pour le soft update des réseaux cibles
  target_update_interval: 1  # Fréquence de mise à jour du réseau cible
  
  # Configuration du réseau
  activation_fn: 'relu'  # Fonction d'activation
  net_arch:  # Architecture spécifique à SAC
    pi: [256, 256]  # Réseau de politique
    qf: [256, 256]  # Réseau critique
  
  # Paramètres d'entraînement
  learning_starts: 1000  # Nombre d'étapes avant de commencer l'apprentissage
  train_freq: 1  # Fréquence des mises à jour (étapes d'environnement)
  gradient_steps: 1  # Nombre d'étapes de gradient par mise à jour
  
  # Exploration
  use_sde: False  # State-Dependent Exploration
  sde_sample_freq: -1  # -1 pour ne pas utiliser SDE
  use_reparameterization: True  # Pour le reparamétrage du gradient
  
  # Sauvegarde et log
  checkpoint_freq: 10000  # Fréquence de sauvegarde du modèle
  
  # Paramètres avancés
  policy_delay: 2  # Retard pour la mise à jour de la politique
  target_delay: 2  # Retard pour la mise à jour du critique cible
  n_critics: 2  # Nombre de réseaux critiques à entraîner
