#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test pour valider l'affichage hi√©rarchique et les corrections.

Ce script teste :
1. L'affichage hi√©rarchique avec le nouveau callback
2. La correction de l'erreur JSONL avec clean_worker_id
3. La configuration exposure_range des paliers
4. L'int√©gration avec les m√©triques de trading
"""

import logging
import numpy as np
import time
import sys
import os
from pathlib import Path
from unittest.mock import Mock, MagicMock

# Ajouter le chemin du projet
project_root = Path(__file__).parent
sys.path.append(str(project_root / "bot" / "src"))

try:
    from adan_trading_bot.agent.ppo_agent import HierarchicalTrainingDisplayCallback
    from adan_trading_bot.environment.multi_asset_chunked_env import clean_worker_id
    from adan_trading_bot.common.constants import CAPITAL_TIERS
    print("‚úÖ Imports r√©ussis")
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    sys.exit(1)

# Configuration du logging pour les tests
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_clean_worker_id():
    """Test de la fonction clean_worker_id pour corriger l'erreur JSONL."""
    print("\n" + "="*60)
    print("üß™ TEST 1: Correction erreur JSONL - clean_worker_id")
    print("="*60)

    test_cases = [
        ("w0", 0),
        ("W0", 0),
        ("w1", 1),
        ("W5", 5),
        ("w99", 99),
        (0, 0),
        (42, 42),
        ("invalid", 0),
        ("", 0),
        (None, 0)
    ]

    passed = 0
    for input_val, expected in test_cases:
        result = clean_worker_id(input_val)
        if result == expected:
            print(f"‚úÖ {input_val} -> {result} (attendu: {expected})")
            passed += 1
        else:
            print(f"‚ùå {input_val} -> {result} (attendu: {expected})")

    print(f"\nüìä R√©sultat: {passed}/{len(test_cases)} tests r√©ussis")
    return passed == len(test_cases)

def test_exposure_range_config():
    """Test de la configuration exposure_range dans les paliers."""
    print("\n" + "="*60)
    print("üß™ TEST 2: Configuration exposure_range des paliers")
    print("="*60)

    required_tiers = ["Micro", "Small", "Medium", "Large", "Enterprise"]
    passed = 0

    for tier_name in required_tiers:
        if tier_name in CAPITAL_TIERS:
            tier = CAPITAL_TIERS[tier_name]
            if 'exposure_range' in tier:
                exposure_range = tier['exposure_range']
                if isinstance(exposure_range, list) and len(exposure_range) == 2:
                    print(f"‚úÖ {tier_name}: exposure_range = {exposure_range}")
                    passed += 1
                else:
                    print(f"‚ùå {tier_name}: exposure_range invalide = {exposure_range}")
            else:
                print(f"‚ùå {tier_name}: exposure_range manquant")
        else:
            print(f"‚ùå {tier_name}: palier manquant")

    print(f"\nüìä R√©sultat: {passed}/{len(required_tiers)} paliers configur√©s")
    return passed == len(required_tiers)

class MockModel:
    """Mod√®le fictif pour tester le callback."""

    def __init__(self):
        self.logger = Mock()
        self.logger.name_to_value = {
            "train/loss": 0.1234,
            "train/policy_loss": 0.0567,
            "train/value_loss": 0.0456,
            "train/entropy_loss": 0.7890
        }

class MockCallback(HierarchicalTrainingDisplayCallback):
    """Callback modifi√© pour les tests."""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_logs = []

        # Mock du logger pour capturer les sorties
        original_logger = logger
        self.mock_logger = Mock()
        self.mock_logger.info = lambda msg: self.test_logs.append(msg)
        self.mock_logger.warning = lambda msg: self.test_logs.append(f"WARNING: {msg}")
        self.mock_logger.error = lambda msg: self.test_logs.append(f"ERROR: {msg}")

        # Remplacer temporairement le logger global
        import adan_trading_bot.agent.ppo_agent as ppo_module
        ppo_module.logger = self.mock_logger

def test_hierarchical_callback():
    """Test du callback d'affichage hi√©rarchique."""
    print("\n" + "="*60)
    print("üß™ TEST 3: Callback d'affichage hi√©rarchique")
    print("="*60)

    # Cr√©er le callback avec des param√®tres de test
    callback = MockCallback(
        verbose=1,
        display_freq=100,
        total_timesteps=1000,
        initial_capital=50.0
    )

    # Simuler le mod√®le
    callback.model = MockModel()
    callback.num_timesteps = 500

    # Simuler des donn√©es d'environnement
    callback.locals = {
        "infos": [{
            "portfolio_value": 55.25,
            "cash": 25.30,
            "drawdown": 2.5,
            "positions": {
                "BTCUSDT": {
                    "size": 0.001,
                    "entry_price": 45000.0,
                    "value": 45.0,
                    "sl": 43000.0,
                    "tp": 50000.0
                }
            },
            "sharpe": 1.25,
            "sortino": 1.42,
            "profit_factor": 1.18,
            "max_dd": 3.2,
            "cagr": 15.5,
            "win_rate": 65.0,
            "trades": 12
        }]
    }

    try:
        # Test de d√©marrage
        callback._on_training_start()
        print("‚úÖ _on_training_start() r√©ussi")

        # Test d'affichage des m√©triques
        callback._log_detailed_metrics()
        print("‚úÖ _log_detailed_metrics() r√©ussi")

        # Test de fin de rollout
        callback._on_rollout_end()
        print("‚úÖ _on_rollout_end() r√©ussi")

        # Test de fin d'entra√Ænement
        callback._on_training_end()
        print("‚úÖ _on_training_end() r√©ussi")

        # V√©rifier que des logs ont √©t√© g√©n√©r√©s
        if len(callback.test_logs) > 0:
            print(f"‚úÖ {len(callback.test_logs)} logs g√©n√©r√©s")

            # Afficher quelques exemples de logs
            print("\nüìã Exemples de logs g√©n√©r√©s:")
            for i, log in enumerate(callback.test_logs[:5]):  # Premiers 5 logs
                print(f"   {i+1}. {log[:80]}...")

            if len(callback.test_logs) > 5:
                print(f"   ... et {len(callback.test_logs) - 5} autres logs")

            return True
        else:
            print("‚ùå Aucun log g√©n√©r√©")
            return False

    except Exception as e:
        print(f"‚ùå Erreur lors du test du callback: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_portfolio_metrics():
    """Test de l'int√©gration avec les m√©triques de portfolio."""
    print("\n" + "="*60)
    print("üß™ TEST 4: Int√©gration m√©triques de portfolio")
    print("="*60)

    # Simuler diff√©rents sc√©narios de portfolio
    test_scenarios = [
        {
            "name": "Portfolio profitable",
            "portfolio_value": 120.0,
            "initial_capital": 100.0,
            "expected_roi": 20.0
        },
        {
            "name": "Portfolio en perte",
            "portfolio_value": 85.0,
            "initial_capital": 100.0,
            "expected_roi": -15.0
        },
        {
            "name": "Portfolio stable",
            "portfolio_value": 100.5,
            "initial_capital": 100.0,
            "expected_roi": 0.5
        }
    ]

    passed = 0
    for scenario in test_scenarios:
        roi = ((scenario["portfolio_value"] - scenario["initial_capital"]) / scenario["initial_capital"]) * 100
        if abs(roi - scenario["expected_roi"]) < 0.01:
            print(f"‚úÖ {scenario['name']}: ROI = {roi:.1f}% (attendu: {scenario['expected_roi']:.1f}%)")
            passed += 1
        else:
            print(f"‚ùå {scenario['name']}: ROI = {roi:.1f}% (attendu: {scenario['expected_roi']:.1f}%)")

    print(f"\nüìä R√©sultat: {passed}/{len(test_scenarios)} sc√©narios r√©ussis")
    return passed == len(test_scenarios)

def test_progress_bar():
    """Test de la barre de progression."""
    print("\n" + "="*60)
    print("üß™ TEST 5: Barre de progression")
    print("="*60)

    total_timesteps = 1000
    test_steps = [0, 100, 250, 500, 750, 1000]

    print("Simulation de la barre de progression:")
    for step in test_steps:
        progress = step / total_timesteps * 100
        progress_bar_length = 30
        filled_length = int(progress_bar_length * progress // 100)
        bar = "‚îÅ" * filled_length + "‚îÄ" * (progress_bar_length - filled_length)

        print(f"üöÄ ADAN Training {bar} {progress:6.1f}% ({step:4d}/{total_timesteps})")

    print("‚úÖ Barre de progression fonctionnelle")
    return True

def run_integration_test():
    """Test d'int√©gration complet."""
    print("\n" + "="*80)
    print("üéØ TEST D'INT√âGRATION COMPLET")
    print("="*80)

    print("Simulation d'un entra√Ænement complet avec affichage hi√©rarchique...")

    # Cr√©er le callback
    callback = MockCallback(
        verbose=1,
        display_freq=50,
        total_timesteps=200,
        initial_capital=25.0
    )

    callback.model = MockModel()

    # Simuler une session d'entra√Ænement
    for step in range(0, 201, 50):
        callback.num_timesteps = step

        # Simuler l'√©volution du portfolio
        portfolio_value = 25.0 + (step / 200) * 10.0  # Croissance de 25 √† 35

        callback.locals = {
            "infos": [{
                "portfolio_value": portfolio_value,
                "cash": portfolio_value * 0.3,
                "drawdown": max(0, (step / 200) * 5.0),
                "positions": {
                    "ADAUSDT": {
                        "size": 35.0,
                        "entry_price": 0.7092,
                        "value": portfolio_value * 0.7,
                        "sl": 0.67,
                        "tp": 0.82
                    }
                } if step > 0 else {},
                "sharpe": 0.5 + (step / 200) * 1.0,
                "sortino": 0.6 + (step / 200) * 1.0,
                "profit_factor": 1.0 + (step / 200) * 0.5,
                "max_dd": (step / 200) * 5.0,
                "cagr": (step / 200) * 20.0,
                "win_rate": 50.0 + (step / 200) * 20.0,
                "trades": step // 50
            }]
        }

        if step == 0:
            callback._on_training_start()
        elif step > 0:
            callback._log_detailed_metrics()

    callback._on_training_end()

    print(f"\n‚úÖ Test d'int√©gration termin√© - {len(callback.test_logs)} logs g√©n√©r√©s")

    # Analyser les logs pour v√©rifier le contenu attendu
    expected_patterns = [
        "üöÄ D√âMARRAGE ADAN TRAINING",
        "Configuration Flux Mon√©taires",
        "PORTFOLIO",
        "RISK",
        "METRICS",
        "‚úÖ ENTRA√éNEMENT TERMIN√â"
    ]

    found_patterns = 0
    for pattern in expected_patterns:
        if any(pattern in log for log in callback.test_logs):
            print(f"‚úÖ Pattern trouv√©: {pattern}")
            found_patterns += 1
        else:
            print(f"‚ùå Pattern manquant: {pattern}")

    print(f"\nüìä Patterns trouv√©s: {found_patterns}/{len(expected_patterns)}")
    return found_patterns >= len(expected_patterns) * 0.8  # 80% de r√©ussite minimum

def main():
    """Fonction principale de test."""
    print("üéØ VALIDATION COMPL√àTE - AFFICHAGE HI√âRARCHIQUE ADAN")
    print("="*80)
    print("Ce script valide toutes les am√©liorations apport√©es:")
    print("‚Ä¢ Affichage hi√©rarchique structur√©")
    print("‚Ä¢ Correction erreur JSONL (worker_id)")
    print("‚Ä¢ Configuration exposure_range")
    print("‚Ä¢ Int√©gration m√©triques de trading")
    print("‚Ä¢ Barre de progression visuelle")
    print("="*80)

    tests = [
        ("Correction erreur JSONL", test_clean_worker_id),
        ("Configuration exposure_range", test_exposure_range_config),
        ("Callback hi√©rarchique", test_hierarchical_callback),
        ("M√©triques de portfolio", test_portfolio_metrics),
        ("Barre de progression", test_progress_bar),
        ("Test d'int√©gration", run_integration_test)
    ]

    results = {}
    passed_tests = 0

    start_time = time.time()

    for test_name, test_func in tests:
        print(f"\n‚è≥ Ex√©cution: {test_name}...")
        try:
            result = test_func()
            results[test_name] = result
            if result:
                passed_tests += 1
                print(f"‚úÖ {test_name}: R√âUSSI")
            else:
                print(f"‚ùå {test_name}: √âCHOU√â")
        except Exception as e:
            print(f"üí• {test_name}: ERREUR - {e}")
            results[test_name] = False

    end_time = time.time()

    # R√©sum√© final
    print("\n" + "="*80)
    print("üìä R√âSUM√â FINAL")
    print("="*80)
    print(f"Tests r√©ussis: {passed_tests}/{len(tests)}")
    print(f"Taux de r√©ussite: {(passed_tests/len(tests)*100):.1f}%")
    print(f"Temps d'ex√©cution: {end_time-start_time:.2f}s")

    print("\nüìã D√©tail des r√©sultats:")
    for test_name, result in results.items():
        status = "‚úÖ R√âUSSI" if result else "‚ùå √âCHOU√â"
        print(f"  {test_name:<30} {status}")

    if passed_tests == len(tests):
        print("\nüéâ TOUS LES TESTS SONT R√âUSSIS!")
        print("‚úÖ L'affichage hi√©rarchique est pr√™t pour l'entra√Ænement")
        print("\nCommande recommand√©e pour l'entra√Ænement:")
        print("conda run -n trading_env python bot/scripts/train_parallel_agents.py --config bot/config/config.yaml --timeout 120 --checkpoint-dir checkpoints")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {len(tests)-passed_tests} test(s) ont √©chou√©")
        print("Veuillez corriger les probl√®mes avant l'entra√Ænement")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
