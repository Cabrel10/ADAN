#!/usr/bin/env python3
"""
Script de validation finale pour toutes les corrections effectuÃ©es sur ADAN.
Ce script vÃ©rifie que le systÃ¨me est prÃªt pour l'entraÃ®nement long.
"""

import sys
import os
import pandas as pd
import numpy as np

# Ajouter le chemin src au PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from adan_trading_bot.environment.multi_asset_env import MultiAssetEnv
from adan_trading_bot.environment.state_builder import StateBuilder
from adan_trading_bot.environment.order_manager import OrderManager
from adan_trading_bot.common.utils import get_logger, load_config

logger = get_logger()

def create_mock_merged_dataframe(assets, base_features, timeframe='1h', num_rows=100):
    """CrÃ©er un DataFrame mock fusionnÃ© pour les tests."""
    columns = []
    for asset in assets:
        for feature in base_features:
            columns.append(f"{feature}_{asset}")
    
    df = pd.DataFrame(np.random.randn(num_rows, len(columns)), columns=columns)
    df.index = pd.date_range('2024-01-01', periods=num_rows, freq=timeframe)
    
    # S'assurer que les prix sont positifs
    for asset in assets:
        for price_col in ['open', 'high', 'low', 'close']:
            col_name = f"{price_col}_{asset}"
            if col_name in df.columns:
                df[col_name] = np.abs(df[col_name]) + 50  # Prix positifs autour de 50
    
    return df

def test_lot1_environment():
    """Test complet de l'environnement Lot 1."""
    print("ðŸ”§ Test Lot 1 - Environnement complet")
    
    try:
        # Charger configuration
        data_config = load_config('config/data_config_cpu_lot1.yaml')
        env_config = load_config('config/environment_config.yaml')
        config = {'data': data_config, 'environment': env_config}
        
        assets = data_config['assets']
        training_tf = data_config.get('training_timeframe', '1h')
        
        # Construire les features attendues dynamiquement
        expected_features = ["open", "high", "low", "close", "volume"]
        indicators_config = data_config.get('indicators_by_timeframe', {}).get(training_tf, [])
        
        for indicator in indicators_config:
            output_col_name = indicator.get('output_col_name')
            output_col_names = indicator.get('output_col_names')
            
            if output_col_name:
                expected_features.append(f"{output_col_name}_{training_tf}")
            elif output_col_names:
                for col_name in output_col_names:
                    expected_features.append(f"{col_name}_{training_tf}")
        
        # CrÃ©er DataFrame mock
        df = create_mock_merged_dataframe(assets, expected_features, timeframe='1h')
        
        # CrÃ©er environnement
        env = MultiAssetEnv(df_received=df, config=config, max_episode_steps_override=50)
        
        # VÃ©rifications
        assert len(env.base_feature_names) == len(expected_features), f"Features mismatch: {len(env.base_feature_names)} vs {len(expected_features)}"
        assert env.training_timeframe == training_tf, f"Timeframe mismatch: {env.training_timeframe} vs {training_tf}"
        
        # Test reset
        obs, info = env.reset()
        assert 'image_features' in obs, "Missing image_features in observation"
        assert 'vector_features' in obs, "Missing vector_features in observation"
        
        expected_img_shape = (1, data_config.get('cnn_input_window_size', 20), len(expected_features) * len(assets))
        assert obs['image_features'].shape == expected_img_shape, f"Image shape mismatch: {obs['image_features'].shape} vs {expected_img_shape}"
        
        # Test step avec action BUY
        action = 1  # BUY premier actif
        obs, reward, done, truncated, info = env.step(action)
        
        print(f"   âœ… Features: {len(env.base_feature_names)} construites dynamiquement")
        print(f"   âœ… Observation shape: {obs['image_features'].shape}")
        print(f"   âœ… Action BUY testÃ©e, reward: {reward:.4f}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur Lot 1: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_lot2_environment():
    """Test complet de l'environnement Lot 2."""
    print("ðŸ”§ Test Lot 2 - Environnement complet")
    
    try:
        # Charger configuration
        data_config = load_config('config/data_config_cpu_lot2.yaml')
        env_config = load_config('config/environment_config.yaml')
        config = {'data': data_config, 'environment': env_config}
        
        assets = data_config['assets']
        base_features = data_config.get('base_market_features', [])
        
        # CrÃ©er DataFrame mock
        df = create_mock_merged_dataframe(assets, base_features, timeframe='1m')
        
        # CrÃ©er environnement
        env = MultiAssetEnv(df_received=df, config=config, max_episode_steps_override=50)
        
        # VÃ©rifications
        assert len(env.base_feature_names) == len(base_features), f"Features mismatch: {len(env.base_feature_names)} vs {len(base_features)}"
        assert set(env.base_feature_names) == set(base_features), "Features content mismatch"
        
        # Test reset et step
        obs, info = env.reset()
        action = 3  # SELL premier actif (devrait Ãªtre rejetÃ© car pas de position)
        obs, reward, done, truncated, info = env.step(action)
        
        expected_img_shape = (1, data_config.get('cnn_input_window_size', 20), len(base_features) * len(assets))
        assert obs['image_features'].shape == expected_img_shape, f"Image shape mismatch: {obs['image_features'].shape} vs {expected_img_shape}"
        
        print(f"   âœ… Features: {len(base_features)} prÃ©-calculÃ©es utilisÃ©es")
        print(f"   âœ… Observation shape: {obs['image_features'].shape}")
        print(f"   âœ… Action SELL (sans position) testÃ©e, reward: {reward:.4f}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur Lot 2: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_order_manager_normalized_prices():
    """Test OrderManager avec prix normalisÃ©s (nÃ©gatifs)."""
    print("ðŸ”§ Test OrderManager - Prix normalisÃ©s")
    
    try:
        # Configuration simple
        config = {
            'environment': {
                'penalties': {
                    'invalid_order_base': -0.3,
                    'out_of_funds': -0.5,
                    'order_below_tolerable_if_not_adjusted': -0.3
                },
                'order_rules': {
                    'min_value_tolerable': 10.0,
                    'min_value_absolute': 9.0
                },
                'transaction': {
                    'fee_percent': 0.001,
                    'fixed_fee': 0.0
                }
            }
        }
        
        order_manager = OrderManager(config)
        
        # Test BUY avec prix nÃ©gatif (normalisÃ©)
        capital = 100.0
        positions = {}
        current_price = -1.5  # Prix nÃ©gatif (normalisÃ©)
        
        reward, status, info = order_manager.execute_order(
            asset_id='BTCUSDT',
            action_type=1,  # BUY
            current_price=current_price,
            capital=capital,
            positions=positions,
            allocated_value_usdt=50.0
        )
        
        assert status == "BUY_EXECUTED", f"BUY should succeed with negative price, got: {status}"
        assert 'BTCUSDT' in positions, "Position should be created"
        assert positions['BTCUSDT']['qty'] > 0, f"Quantity should be positive: {positions['BTCUSDT']['qty']}"
        assert positions['BTCUSDT']['price'] == current_price, "Price should be stored as-is (normalized)"
        
        # Test SELL avec prix normalisÃ© diffÃ©rent
        new_price = -1.2  # Prix diffÃ©rent pour PnL
        
        reward, status, info = order_manager.execute_order(
            asset_id='BTCUSDT',
            action_type=2,  # SELL
            current_price=new_price,
            capital=info['new_capital'],
            positions=positions,
            quantity=None  # Vendre toute la position
        )
        
        assert status == "SELL_EXECUTED", f"SELL should succeed, got: {status}"
        assert 'BTCUSDT' not in positions, "Position should be closed after full SELL"
        
        # Test avec prix trÃ¨s petit (devrait Ãªtre rejetÃ©)
        tiny_price = 1e-10
        
        reward, status, info = order_manager.execute_order(
            asset_id='ETHUSDT',
            action_type=1,  # BUY
            current_price=tiny_price,
            capital=100.0,
            positions={},
            allocated_value_usdt=50.0
        )
        
        assert status == "PRICE_NOT_AVAILABLE", f"Should reject tiny price, got: {status}"
        
        print(f"   âœ… BUY avec prix nÃ©gatif: rÃ©ussi")
        print(f"   âœ… SELL avec calcul PnL: rÃ©ussi")
        print(f"   âœ… Rejet prix trop petit: rÃ©ussi")
        print(f"   âœ… Gestion quantitÃ©s positives: rÃ©ussi")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur prix normalisÃ©s: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_state_builder_missing_features():
    """Test StateBuilder avec des features manquantes."""
    print("ðŸ”§ Test StateBuilder - Gestion features manquantes")
    
    try:
        config = {
            'data': {
                'training_timeframe': '1h',
                'cnn_input_window_size': 20
            },
            'environment': {
                'initial_capital': 100.0
            }
        }
        
        assets = ['BTCUSDT', 'ETHUSDT']
        base_features = ['open', 'high', 'low', 'close', 'volume', 'rsi_14_1h']
        
        # CrÃ©er DataFrame avec des colonnes manquantes
        available_features = ['open', 'high', 'low', 'close', 'volume']  # rsi_14_1h manquant
        columns = []
        for asset in assets:
            for feature in available_features:
                columns.append(f"{feature}_{asset}")
        
        df = pd.DataFrame(np.random.randn(30, len(columns)), columns=columns)
        
        state_builder = StateBuilder(config, assets, base_feature_names=base_features, cnn_input_window_size=20)
        
        # Test avec features manquantes (devrait gÃ©rer gracieusement)
        window = df.tail(20)
        observation = state_builder.build_observation(window, 100.0, {})
        
        # VÃ©rifier que l'observation est crÃ©Ã©e malgrÃ© les features manquantes
        expected_shape = (1, 20, len(base_features) * len(assets))
        assert observation['image_features'].shape == expected_shape, f"Shape mismatch: {observation['image_features'].shape} vs {expected_shape}"
        
        # VÃ©rifier que les features manquantes sont remplies de NaN
        img_features = observation['image_features'][0]  # Remove channel dimension
        has_nan = np.isnan(img_features).any()
        assert has_nan, "Should have NaN values for missing features"
        
        print(f"   âœ… Gestion features manquantes: NaN insÃ©rÃ©s")
        print(f"   âœ… Shape prÃ©servÃ©e: {observation['image_features'].shape}")
        print(f"   âœ… Observation crÃ©Ã©e malgrÃ© les manques")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur features manquantes: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_environment_logging():
    """Test que les logs sont bien nettoyÃ©s et informatifs."""
    print("ðŸ”§ Test Logs - VÃ©rification nettoyage")
    
    try:
        # Capturer les logs pendant la crÃ©ation d'environnement
        import io
        import logging
        
        # CrÃ©er un handler pour capturer les logs
        log_capture = io.StringIO()
        handler = logging.StreamHandler(log_capture)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        
        # Test avec Lot 1
        data_config = load_config('config/data_config_cpu_lot1.yaml')
        env_config = load_config('config/environment_config.yaml')
        config = {'data': data_config, 'environment': env_config}
        
        assets = data_config['assets']
        expected_features = ["open", "high", "low", "close", "volume", "rsi_14_1h", "ema_20_1h", "ema_50_1h"]
        df = create_mock_merged_dataframe(assets, expected_features)
        
        # CrÃ©er environnement avec export dÃ©sactivÃ© pour Ã©viter les logs visuels
        config['environment']['export_history'] = False
        env = MultiAssetEnv(df_received=df, config=config, max_episode_steps_override=10)
        
        # Faire quelques steps pour gÃ©nÃ©rer des logs (sans affichage)
        obs, info = env.reset()
        for _ in range(2):
            action = 0  # Action HOLD pour Ã©viter trop de logs
            obs, reward, done, truncated, info = env.step(action)
            if done or truncated:
                break
        
        # Analyser les logs
        log_output = log_capture.getvalue()
        
        # VÃ©rifier qu'il n'y a pas trop de logs de debug
        debug_count = log_output.count('DEBUG')
        info_count = log_output.count('INFO')
        
        # Les logs INFO devraient Ãªtre prÃ©sents mais pas excessifs
        assert info_count > 0, "Should have some INFO logs"
        assert info_count < 50, f"Too many INFO logs: {info_count}"
        
        # VÃ©rifier qu'il n'y a pas de logs d'erreur critiques non gÃ©rÃ©s
        assert 'ERREUR CRITIQUE' not in log_output, "Should not have unhandled critical errors"
        assert 'SOLUTION TEMPORAIRE' not in log_output, "Should not have temporary solutions"
        
        # Nettoyer
        logger.removeHandler(handler)
        
        print(f"   âœ… Logs INFO: {info_count} (raisonnable)")
        print(f"   âœ… Pas d'erreurs critiques non gÃ©rÃ©es")
        print(f"   âœ… Pas de solutions temporaires")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur test logs: {e}")
        return False

def run_all_tests():
    """ExÃ©cuter tous les tests de validation finale."""
    
    print("ðŸŽ¯ VALIDATION FINALE - SYSTÃˆME ADAN")
    print("=" * 60)
    
    tests = [
        ("Lot 1 - Environnement", test_lot1_environment),
        ("Lot 2 - Environnement", test_lot2_environment),
        ("OrderManager - Prix normalisÃ©s", test_order_manager_normalized_prices),
        ("StateBuilder - Features manquantes", test_state_builder_missing_features),
        ("Logs - Nettoyage", test_environment_logging)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{test_name}")
        print("-" * 40)
        try:
            success = test_func()
            results.append((test_name, success))
            if success:
                print(f"âœ… {test_name}: SUCCÃˆS")
            else:
                print(f"âŒ {test_name}: Ã‰CHEC")
        except Exception as e:
            print(f"âŒ {test_name}: ERREUR - {e}")
            results.append((test_name, False))
    
    print("\n" + "=" * 60)
    print("ðŸ“Š RÃ‰SULTATS FINAUX:")
    print("=" * 60)
    
    success_count = 0
    for test_name, success in results:
        status = "âœ… SUCCÃˆS" if success else "âŒ Ã‰CHEC"
        print(f"   {test_name:<35} {status}")
        if success:
            success_count += 1
    
    overall_success = success_count == len(results)
    
    print("\n" + "=" * 60)
    if overall_success:
        print("ðŸŽ‰ VALIDATION COMPLÃˆTE RÃ‰USSIE!")
        print("   Le systÃ¨me ADAN est prÃªt pour l'entraÃ®nement long.")
        print("   Toutes les corrections ont Ã©tÃ© validÃ©es avec succÃ¨s.")
    else:
        print("âš ï¸  VALIDATION PARTIELLE")
        print(f"   {success_count}/{len(results)} tests rÃ©ussis.")
        print("   VÃ©rifiez les erreurs ci-dessus avant l'entraÃ®nement long.")
    
    print("=" * 60)
    
    return overall_success

if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)