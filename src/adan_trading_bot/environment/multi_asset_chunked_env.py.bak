#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Multi-asset trading environment with chunked data loading.

This environment extends the base trading environment to support multiple assets
and efficient data loading using ChunkedDataLoader.
"""

import numpy as np
import pandas as pd
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, Any, List, Optional, Tuple
import logging
from pathlib import Path

from ..data_processing.chunked_loader import ChunkedDataLoader
from ..environment.state_builder import StateBuilder
from ..environment.reward_calculator import RewardCalculator
from ..portfolio.portfolio_manager import PortfolioManager
from ..trading.order_manager import OrderManager
from ..trading.order_manager import OrderType, OrderSide, OrderStatus, Order

logger = logging.getLogger(__name__)


class MultiAssetChunkedEnv(gym.Env):
    """
    A multi-asset trading environment with efficient chunked data loading.

    This environment loads data in chunks to manage memory usage and supports
    trading multiple assets simultaneously.
    """

    metadata = {"render_modes": ["human"]}

    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the multi-asset trading environment.

        Args:
            config: Configuration dictionary containing:
                - data: Data loading configuration
                - env: Environment parameters
                - portfolio: Portfolio management settings
                - trading: Trading parameters
        """
        super().__init__()
        self.config = config

        # Initialize core components
        self._initialize_components()

        # Set up action and observation spaces
        self._setup_spaces()

        # Initialize state
        self.current_step = 0
        self.current_chunk = 0
        self.current_data = {}
        self.done = False

        logger.info("MultiAssetChunkedEnv initialized")

    def _initialize_components(self) -> None:
        """Initialize all environment components."""
        # 1. Initialize data loader
        self._init_data_loader()

        # 2. Initialize portfolio manager
        self.portfolio = PortfolioManager(env_config=self.config.get("environment", {}))

        # 3. Initialize order manager
        self.order_manager = OrderManager(portfolio_manager=self.portfolio)

        # 4. Initialize reward calculator
        self.reward_calculator = RewardCalculator(
            env_config=self.config.get("environment", {})
        )

        # 5. Initialize state builder
        state_config = self.config.get("state", {})
        self.state_builder = StateBuilder(
            window_size=state_config.get("window_size", 100),
            timeframes=state_config.get("timeframes", ["5m", "1h", "4h"]),
            features_per_timeframe=state_config.get("features_per_timeframe"),
        )

    def _init_data_loader(self) -> None:
        """Initialize the chunked data loader."""
        data_config = self.config["data"]

        self.data_loader = ChunkedDataLoader(
            data_dir=Path(data_config.get("data_dir", "data/final")),
            chunk_size=data_config.get("chunk_size", 10000),
            assets_list=data_config.get("assets"),
            features_by_timeframe=self._get_features_config(),
        )

        # Store assets list for easy access
        self.assets = self.data_loader.assets_list
        if not self.assets:
            raise ValueError("No assets available for trading")

        logger.info(f"Initialized data loader with {len(self.assets)} assets")

    def _get_features_config(self) -> Dict[str, List[str]]:
        """Extract features configuration from the main config."""
        features_config = {}

        if (
            "feature_engineering" in self.config
            and "timeframes" in self.config["feature_engineering"]
        ):
            for tf in self.config["feature_engineering"]["timeframes"]:
                features_config[tf] = (
                    self.config["feature_engineering"]
                    .get("features", {})
                    .get(tf, ["open", "high", "low", "close", "volume"])
                )
        else:
            # Default features if not specified
            features_config = {
                "5m": ["open", "high", "low", "close", "volume"],
                "1h": ["open", "high", "low", "close"],
                "4h": ["open", "close"],
            }

        return features_config

    def _setup_spaces(self) -> None:
        """Set up action and observation spaces."""
        # Action space: [hold, buy, sell] for each asset
        self.action_space = spaces.MultiDiscrete(
            [3] * len(self.assets)  # 0=hold, 1=buy, 2=sell for each asset
        )

        # Define a default observation space that will be updated after reset
        # This is a temporary space that will be replaced after the first reset
        # We use a simple Box space with dummy dimensions that will be updated
        self.observation_space = spaces.Dict({
            'price_features': spaces.Box(
                low=-np.inf, 
                high=np.inf, 
                shape=(len(self.assets), 1, 1),  # Will be updated in _initialize_observation_space
                dtype=np.float32
            ),
            'portfolio_state': spaces.Box(
                low=0.0,
                high=np.inf,
                shape=(3,),  # cash, total_value, position_value
                dtype=np.float32
            )
        })

        

    def reset(self, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Reset the environment to start a new episode.

        Returns:
            observation: Initial observation
            info: Additional information
        """
        # Reset environment state
        self.current_step = 0
        self.current_chunk = 0
        self.done = False

        # Reset portfolio
        self.portfolio.reset()

        # Load the first chunk of data
        self.current_data = self.data_loader.load_chunk(0)

        # Initialize observation space if not already set
        if self.observation_space is None:
            self._initialize_observation_space()

        # Get initial observation
        observation = self._get_observation()
        info = self._get_info()

        return observation, info

    def _initialize_observation_space(self) -> None:
        """Initialize the observation space based on the first data point."""
        if not self.current_data:
            raise ValueError("No data available to initialize observation space")

        # Get sample data for the first asset and timeframe
        sample_asset = next(iter(self.current_data.keys()))
        if not self.current_data[sample_asset]:
            raise ValueError(f"No timeframe data available for asset {sample_asset}")
            
        # Get the first available timeframe
        sample_tf = next(iter(self.current_data[sample_asset].keys()))
        sample_df = self.current_data[sample_asset][sample_tf]
        
        if sample_df.empty:
            raise ValueError(f"Empty DataFrame for {sample_asset} {sample_tf}")
            
        # Get the expected shape of a single asset's observation from StateBuilder
        # This shape is (num_timeframes, window_size, num_features)
        single_asset_obs_shape = self.state_builder.get_observation_shape()

        # The 'price_features' observation space should be a Box representing all assets stacked
        # Its shape will be (num_assets, num_timeframes, window_size, num_features)
        price_features_shape = (len(self.assets), *single_asset_obs_shape)

        self.observation_space = spaces.Dict({
            'price_features': spaces.Box(
                low=-np.inf,
                high=np.inf,
                shape=price_features_shape,
                dtype=np.float32
            ),
            'portfolio_state': spaces.Box(
                low=0.0,
                high=np.inf,
                shape=(3,),  # cash, total_value, position_value
                dtype=np.float32
            )
        })

    def step(
        self, action: np.ndarray
    ) -> Tuple[Dict[str, Any], float, bool, bool, Dict[str, Any]]:
        """
        Execute one time step within the environment.

        Args:
            action: Action to take (one per asset)

        Returns:
            observation: Next observation
            reward: Reward for the current step
            terminated: Whether the episode is done
            truncated: Whether the episode was truncated
            info: Additional information
        """
        if self.done:
            raise RuntimeError(
                "Episode has already terminated. Call reset() to start a new episode."
            )

        # Execute trades based on actions
        self._execute_trades(action)

        # Move to next step
        self.current_step += 1

        # Check if we need to load the next chunk
        if self.current_step >= len(next(iter(self.current_data.values()))):
            self.current_chunk += 1

            if self.current_chunk >= len(self.data_loader):
                self.done = True
            else:
                self.current_data = self.data_loader.load_chunk(self.current_chunk)
                self.current_step = 0

        # Get next observation
        observation = self._get_observation()

        # Calculate reward
        reward = self._calculate_reward()

        # Check if episode is done
        terminated = self.done
        truncated = False  # Can be set by wrappers

        # Get additional info
        info = self._get_info()

        return observation, reward, terminated, truncated, info

    def _execute_trades(self, action: np.ndarray) -> None:
        """Execute trades based on the action vector."""
        if len(action) != len(self.assets):
            raise ValueError(
                f"Action dimension {len(action)} does not match number of assets {len(self.assets)}"
            )

        current_prices = self._get_current_prices()

        for i, (asset, action_idx) in enumerate(zip(self.assets, action)):
            # Pass the action directly to the order manager, along with current price and asset
            self.order_manager.execute_action(action_idx, current_prices[asset], asset)

        # After all trades for the step, update portfolio metrics
        self.portfolio.update_market_price(current_prices)
        # Rebalance the portfolio after market price update
        self.portfolio.rebalance(current_prices)

    def _get_current_prices(self) -> Dict[str, float]:
        """Get current prices for all assets.
        
        Returns:
            Dictionary mapping asset symbols to their current price.
            If no price is available for an asset, it will not be included.
        """
        prices = {}
        for asset, timeframe_data in self.current_data.items():
            if not timeframe_data:
                continue
                
            # Use the first available timeframe
            tf = next(iter(timeframe_data.keys()))
            df = timeframe_data[tf]
            
            if not df.empty and self.current_step < len(df):
                # Try to get the close price, fall back to the first numeric column
                if 'close' in df.columns:
                    prices[asset] = df.iloc[self.current_step]["close"]
                elif not df.empty:
                    # Fall back to first numeric column
                    numeric_cols = df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        prices[asset] = df.iloc[self.current_step][numeric_cols[0]]
                        
        return prices

    def _get_current_timestamp(self) -> pd.Timestamp:
        """Get the current timestamp.
        
        Returns:
            The current timestamp from the first available asset and timeframe.
            
        Raises:
            RuntimeError: If no timestamp data is available.
        """
        for asset, timeframe_data in self.current_data.items():
            if not timeframe_data:
                continue
                
            # Use the first available timeframe
            tf = next(iter(timeframe_data.keys()))
            df = timeframe_data[tf]
            
            if not df.empty and self.current_step < len(df):
                return df.index[self.current_step]
                
        raise RuntimeError("No timestamp data available")

    def _get_observation(self) -> Dict[str, np.ndarray]:
        """Get the current observation.
        
        The observation is a dictionary where each key is an asset symbol
        and each value is the corresponding state representation.
        """
        observation = {}
        # Collect all asset observations into a list
        asset_observations = []
        for asset in self.assets:
            timeframe_data = self.current_data.get(asset)
            if not timeframe_data:
                # If no data for an asset, provide a zero-filled observation
                # This assumes a consistent shape for all asset observations
                # You might need to get the expected shape from state_builder or observation_space
                # For now, let's assume it's (num_timeframes, window_size, num_features)
                # based on the state_builder's get_observation_shape
                state_shape = self.state_builder.get_observation_shape()
                asset_observations.append(np.zeros(state_shape, dtype=np.float32))
                continue

            tf = next(iter(timeframe_data.keys()))
            df = timeframe_data[tf]

            if self.current_step < len(df):
                start_idx = max(0, self.current_step - self.state_builder.window_size + 1)
                end_idx = self.current_step + 1
                current_data = df.iloc[start_idx:end_idx]
                asset_observations.append(self.state_builder.build_observation(current_data))
            else:
                # If current_step is out of bounds for this asset's data, provide zero-filled
                state_shape = self.state_builder.get_observation_shape()
                asset_observations.append(np.zeros(state_shape, dtype=np.float32))

        # Stack all asset observations to create the 'price_features'
        # This will result in a shape like (num_assets, num_timeframes, window_size, num_features)
        observation['price_features'] = np.array(asset_observations, dtype=np.float32)

        # Add portfolio state to the observation
        portfolio_metrics = self.portfolio.get_metrics()
        current_prices = self._get_current_prices() # Get current prices to calculate positions_value

        positions_value = 0.0
        for asset, pos_info in portfolio_metrics['positions'].items():
            if pos_info['is_open'] and asset in current_prices:
                positions_value += pos_info['size'] * current_prices[asset]

        observation['portfolio_state'] = np.array([
            portfolio_metrics['cash'],
            portfolio_metrics['total_capital'], # Use total_capital instead of total_value
            positions_value
        ], dtype=np.float32)

        return observation

    def _calculate_reward(self, action: np.ndarray) -> float:
        """Calculate the reward for the current step."""
        # Get portfolio value change
        portfolio_metrics = self.portfolio.get_metrics()

        # trade_pnl is currently not directly available per step in this setup.
        # It's accumulated in portfolio_manager.realized_pnl.
        # For now, we'll pass 0.0 and rely on portfolio_metrics for reward shaping.
        trade_pnl = (
            0.0  # This needs to be captured from close_position calls if used directly
        )

        # Get chunk-specific information for bonus calculation
        chunk_id = self.current_chunk

        # Get optimal PnL for each asset in the chunk
        optimal_pnl_per_asset = self.data_loader.get_chunk_optimal_pnl(chunk_id)

        # Aggregate optimal PnL for the chunk (e.g., average across assets)
        if optimal_pnl_per_asset:
            optimal_chunk_pnl = np.mean(list(optimal_pnl_per_asset.values()))
        else:
            optimal_chunk_pnl = 0.0

        performance_ratio = self.portfolio.get_chunk_performance_ratio(
            chunk_id, optimal_chunk_pnl
        )

        # The action passed to reward calculator should be the specific action that led to a trade,
        # or a representative action if multiple assets are traded. For simplicity, we'll pass the first action.
        # A more sophisticated approach might involve a weighted average or separate rewards per asset.
        representative_action = (
            action[0] if len(action) > 0 else 0
        )  # Default to hold if no action

        reward = self.reward_calculator.calculate(
            portfolio_metrics=portfolio_metrics,
            trade_pnl=trade_pnl,
            action=representative_action,
            chunk_id=chunk_id,
            optimal_chunk_pnl=optimal_chunk_pnl,
            performance_ratio=performance_ratio,
        )

        return reward

    def _get_info(self) -> Dict[str, Any]:
        """Get additional information about the environment state."""
        return {
            "step": self.current_step,
            "chunk": self.current_chunk,
            "portfolio_value": self.portfolio.get_portfolio_value(),
            "cash": self.portfolio.cash,
            "positions": self.portfolio.positions,
            "current_prices": self._get_current_prices(),
        }

    def render(self, mode: str = "human") -> None:
        """Render the environment."""
        if mode == "human":
            print(
                f"Step: {self.current_step}, "
                f"Portfolio Value: {self.portfolio.get_portfolio_value():.2f}, "
                f"Cash: {self.portfolio.cash:.2f}, "
                f"Positions: {self.portfolio.positions}"
            )

    def close(self) -> None:
        """Clean up resources."""
        pass  # No special cleanup needed
