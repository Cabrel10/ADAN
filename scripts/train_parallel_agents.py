#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Script d'entra√Ænement parall√®le pour instances ADAN.

Ce script permet d'entra√Æner plusieurs instances du mod√®le ADAN en parall√®le,
chacune avec une configuration de worker diff√©rente.
"""

import argparse
import copy
import logging
import os
import sys
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import gym
import numpy as np
import pandas as pd
import yaml
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecNormalize

from adan_trading_bot.data_processing.data_loader import ChunkedDataLoader
from adan_trading_bot.environment.multi_asset_chunked_env import MultiAssetChunkedEnv
from adan_trading_bot.utils.caching_utils import DataCacheManager


# D√©finir le chemin absolu du projet
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Ajouter le chemin src au PYTHONPATH
sys.path.insert(0, os.path.join(PROJECT_ROOT, "src"))


def make_env(instance_id: int, config: Dict, cache: DataCacheManager):
    """Cr√©e un environnement avec les donn√©es mises en cache.

    Args:
        instance_id: ID de l'instance du worker
        config: Configuration compl√®te
        cache: Instance de DataCacheManager

    Returns:
        Environnement configur√© avec les donn√©es mises en cache
    """
    logger = logging.getLogger(f"Instance_{instance_id}")

    try:
        # V√©rifier la configuration des workers
        worker_keys = list(config["workers"].keys())
        worker_key = worker_keys[instance_id % len(worker_keys)]
        worker_config = copy.deepcopy(config["workers"][worker_key])

        # S√©lectionner un actif et un timeframe pour ce worker
        if not worker_config.get("assets") or not worker_config.get("timeframes"):
            raise ValueError(
                "La configuration du worker doit contenir 'assets' et 'timeframes'"
            )

        # Utiliser l'ID de l'instance pour s√©lectionner un actif et un timeframe de mani√®re d√©terministe
        asset = worker_config["assets"][instance_id % len(worker_config["assets"])]
        timeframe = worker_config["timeframes"][
            instance_id % len(worker_config["timeframes"])
        ]

        logger.info(
            "Chargement des donn√©es pour %s - Asset: %s, Timeframe: %s",
            worker_key,
            asset,
            timeframe,
        )

        # Initialisation du chargeur de donn√©es
        data_loader = ChunkedDataLoader(config, worker_config)

        # Chargement des donn√©es
        logger.info("Chargement des donn√©es pour %s - %s...", asset, timeframe)
        data_dict = data_loader.load_chunk()

        # V√©rification des donn√©es
        if asset not in data_dict or timeframe not in data_dict[asset]:
            err_msg = f"Aucune donn√©e pour {asset} sur {timeframe}"
            logger.error(err_msg)
            raise ValueError(err_msg)

        data = data_dict[asset][timeframe]

        # Cr√©ation d'un scaler factice (√† remplacer par un vrai scaler si n√©cessaire)
        scaler = None

        logger.info(
            "Donn√©es charg√©es: %d points pour %s (%s)", len(data), asset, timeframe
        )

        # Cr√©ation de l'environnement
        return MultiAssetChunkedEnv(
            config=config,
            worker_config=worker_config,
            data_loader_instance=data_loader,
            **worker_config.get("env_params", {}),
        )

    except Exception as e:
        logger.error("Erreur lors de la cr√©ation de l'environnement: %s", str(e))
        logger.exception("D√©tails de l'erreur:")
        raise


def setup_logging() -> logging.Logger:
    """Configure le logging pour l'entra√Ænement parall√®le.

    Returns:
        logging.Logger: L'objet logger configur√©
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"logs/parallel_training_{timestamp}.log"

    # Cr√©er le dossier logs s'il n'existe pas
    os.makedirs("logs", exist_ok=True)

    # Configurer le format des logs
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=logging.DEBUG,  # Set to DEBUG to capture all messages
        format=log_format,
        handlers=[
            logging.FileHandler(log_file, mode="w"),  # Overwrite log file
            logging.StreamHandler(),
        ],
    )

    # Configurer le logger pour ce module
    logger = logging.getLogger(__name__)

    # Set specific log levels for verbose modules
    logging.getLogger("adan_trading_bot").setLevel(logging.DEBUG)
    logging.getLogger("adan_trading_bot.environment.multi_asset_chunked_env").setLevel(
        logging.DEBUG
    )
    logging.getLogger("adan_trading_bot.data_processing.state_builder").setLevel(
        logging.DEBUG
    )

    # Disable excessive logging from libraries
    logging.getLogger("matplotlib").setLevel(logging.WARNING)
    logging.getLogger("tensorflow").setLevel(logging.WARNING)
    logging.getLogger("stable_baselines3").setLevel(logging.INFO)

    return logger


def load_base_config(
    config_override: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """Charge la configuration de base et r√©sout les variables de chemin.

    Args:
        config_override: Dictionnaire de param√®tres pour √©craser la configuration de base.

    Returns:
        Dict[str, Any]: Dictionnaire contenant la configuration charg√©e et fusionn√©e.
    """
    # Charger la configuration de base
    with open(os.path.join(PROJECT_ROOT, "config/config.yaml"), "r") as f:
        config = yaml.safe_load(f)

    # R√©soudre les variables de chemin
    def resolve_paths(node, config_root):
        if isinstance(node, dict):
            for key, value in node.items():
                node[key] = resolve_paths(value, config_root)
        elif isinstance(node, list):
            for i, item in enumerate(node):
                node[i] = resolve_paths(item, config_root)
        elif isinstance(node, str):
            import re

            # Loop to handle multiple and nested variables
            # We limit to 10 iterations to prevent infinite loops
            for _ in range(10):
                match = re.search(r"\$\{(.+?)\}", node)
                if not match:
                    break

                path_variable = match.group(1)
                keys = path_variable.split(".")
                resolved_value = config_root
                try:
                    for k in keys:
                        resolved_value = resolved_value[k]

                    # If the resolved value is a path and not absolute, make it absolute
                    if (
                        isinstance(resolved_value, str)
                        and path_variable.startswith("paths.")
                        and not os.path.isabs(resolved_value)
                    ):
                        resolved_value = os.path.join(PROJECT_ROOT, resolved_value)

                    # Replace the variable with its resolved value
                    node = node.replace(match.group(0), str(resolved_value))
                except (KeyError, TypeError):
                    # If the key is not found, leave the variable as is and break the loop
                    break
        return node

    config = resolve_paths(config, config)

    # Appliquer les param√®tres de l'override si fournis
    if config_override:
        for key, value in config_override.items():
            if key in config:
                config[key] = value

    return config


class TimeoutException(Exception):
    """Exception lev√©e quand le timeout est atteint"""

    pass


def timeout_handler(signum, frame):
    """Gestionnaire de signal pour le timeout"""
    raise TimeoutException("Temps d'entra√Ænement √©coul√©")


def save_checkpoint(model, optimizer, epoch: int, path: str):
    """
    Sauvegarde un checkpoint complet incluant :
    - √âtat du mod√®le
    - √âtat de l'optimiseur
    - √âtat du g√©n√©rateur de nombres al√©atoires
    - M√©tadonn√©es
    """
    import random

    import numpy as np
    import torch

    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.policy.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "rng_state": {
            "python": random.getstate(),
            "numpy": np.random.get_state(),
            "torch": torch.get_rng_state(),
        },
        "timestamp": time.time(),
    }

    # Cr√©er le r√©pertoire parent si n√©cessaire
    os.makedirs(os.path.dirname(path), exist_ok=True)

    # Sauvegarder avec tol√©rance aux erreurs
    try:
        torch.save(checkpoint, path)
        return path
    except Exception as e:
        logging.error(f"Erreur lors de la sauvegarde du checkpoint: {e}")
        return None


def load_checkpoint(model, optimizer, path: str):
    """
    Charge un checkpoint complet et restaure :
    - L'√©tat du mod√®le
    - L'√©tat de l'optimiseur
    - Les √©tats des g√©n√©rateurs de nombres al√©atoires

    Returns:
        int: L'√©poque √† partir de laquelle reprendre l'entra√Ænement
    """
    import random

    import numpy as np
    import torch

    if not os.path.exists(path):
        logging.warning(
            f"Aucun checkpoint trouv√© √† {path}, d√©marrage d'un nouvel entra√Ænement"
        )
        return 0

    try:
        # Charger le checkpoint
        device = model.device if hasattr(model, "device") else "cpu"
        checkpoint = torch.load(path, map_location=device)

        # Restaurer les √©tats du mod√®le et de l'optimiseur
        model.policy.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        # Restaurer les √©tats des g√©n√©rateurs al√©atoires
        if "rng_state" in checkpoint:
            random.setstate(checkpoint["rng_state"]["python"])
            np.random.set_state(checkpoint["rng_state"]["numpy"])
            torch.set_rng_state(checkpoint["rng_state"]["torch"].to("cpu"))

        logging.info(f"Checkpoint charg√© depuis {path} (√©poque {checkpoint['epoch']})")
        return checkpoint["epoch"]

    except Exception as e:
        logging.error(f"Erreur lors du chargement du checkpoint {path}: {e}")
        return 0


def train_single_instance(
    instance_id: int,
    total_timesteps: int,
    config_override: Optional[Dict[str, Any]] = None,
    shared_model_path: str = None,
    checkpoint_path: str = None,
    timeout: int = None,
) -> Dict[str, Any]:
    """Entra√Æne une instance sp√©cifique du mod√®le avec sa configuration de worker d√©di√©e.

    G√®re le timeout et la sauvegarde automatique des checkpoints.
    Utilise un cache pour les donn√©es et les scalers.

    Args:
        instance_id: Identifiant num√©rique de l'instance (1-4)
        total_timesteps: Nombre total de pas d'entra√Ænement
        config_override: Configuration de remplacement optionnelle
        shared_model_path: Chemin vers un mod√®le partag√© pour le fine-tuning
        checkpoint_path: Chemin pour sauvegarder les checkpoints
        timeout: D√©lai maximal d'ex√©cution en secondes

    Returns:
        Dict contenant les r√©sultats de l'entra√Ænement
    """
    logger = setup_logging()
    start_time = time.time()
    last_checkpoint_time = start_time
    checkpoint_interval = 300  # 5 minutes en secondes

    try:
        # Charger la configuration
        config = load_base_config(config_override)

        # Initialiser le cache des donn√©es
        cache_dir = os.path.join(PROJECT_ROOT, "data", "cache")
        os.makedirs(cache_dir, exist_ok=True)
        cache = DataCacheManager(cache_dir)

        logger.info(f"üöÄ D√©marrage de l'entra√Ænement pour l'instance {instance_id}")
        logger.info(f"Utilisation du cache dans : {cache_dir}")

        # Cr√©er l'environnement avec les donn√©es mises en cache
        env = make_env(instance_id, config, cache)

        # Configuration sp√©cifique au worker
        worker_keys = list(config["workers"].keys())
        worker_key = worker_keys[instance_id % len(worker_keys)]
        worker_config = copy.deepcopy(config["workers"][worker_key])
        agent_config = config.get("agent", {})

        # Ajouter la cl√© du worker √† la configuration pour r√©f√©rence
        worker_config["worker_key"] = worker_key

        # 3. Fusionner les configurations (si n√©cessaire)
        # Ici, nous pourrions ajouter une logique pour fusionner des configurations
        # sp√©cifiques du worker avec la configuration de base

        # 4. Journalisation des informations de configuration
        logger.info(f"Instance {instance_id} - {worker_config.get('name', 'Sans nom')}")
        logger.info(f"  - Actifs: {', '.join(worker_config.get('assets', []))}")
        logger.info(f"  - Timeframes: {', '.join(worker_config.get('timeframes', []))}")
        logger.info(f"  - Jeu de donn√©es: {worker_config.get('data_split', 'train')}")

        # 5. Charger la configuration de base
        base_config = load_base_config(config_override)

        # 6. S'assurer que la configuration de l'environnement est correctement structur√©e
        if "environment" not in base_config:
            base_config["environment"] = {}

        # 7. Cr√©er l'environnement avec la configuration fusionn√©e
        env = MultiAssetChunkedEnv(config=base_config, worker_config=worker_config)

        # --- Validation dimensionnelle ---
        logger.info("Performing state dimension validation...")
        try:
            # R√©initialiser l'environnement pour charger les premi√®res donn√©es
            initial_obs, _ = env.reset()
            # Valider les dimensions
            current_data = (
                env.data_loader.load_chunk()
            )  # Utiliser load_chunk() au lieu de get_current_chunk()
            # La validation des dimensions est d√©j√† effectu√©e dans l'environnement
            logger.info("‚úÖ State dimension validation successful.")
        except ValueError as e:
            logger.error(f"‚ùå State dimension validation failed: {e}")
            raise e
        # --- Fin de la validation ---

        logger.info(f"Observation space: {env.observation_space}")
        logger.info(f"Action space: {env.action_space}")

        env = Monitor(env)
        vec_env = DummyVecEnv([lambda: env])

        policy_class = "MultiInputPolicy"
        logger.info(f"Using policy: {policy_class}")

        # Forcer l'observation space √† √™tre un dictionnaire si ce n'est pas d√©j√† le cas
        if not isinstance(env.observation_space, gym.spaces.Dict):
            logger.warning(
                "L'espace d'observation n'est pas un dictionnaire, conversion en cours..."
            )
            # Cr√©er un nouvel espace d'observation de type Dict
            # Extraire la forme de l'espace d'observation existant
            if hasattr(env.observation_space, "shape"):
                obs_shape = env.observation_space.shape
                # Cr√©er un espace Box pour l'observation avec les m√™mes caract√©ristiques
                obs_space = gym.spaces.Box(
                    low=-np.inf, high=np.inf, shape=obs_shape, dtype=np.float32
                )
                # Cr√©er l'espace d'observation final
                env.observation_space = gym.spaces.Dict(
                    {
                        "observation": obs_space,
                        "portfolio_state": gym.spaces.Box(
                            low=-np.inf, high=np.inf, shape=(17,), dtype=np.float32
                        ),
                    }
                )
            else:
                # Si on ne peut pas d√©terminer la forme, utiliser l'espace tel quel
                env.observation_space = gym.spaces.Dict(
                    {
                        "observation": env.observation_space,
                        "portfolio_state": gym.spaces.Box(
                            low=-np.inf, high=np.inf, shape=(17,), dtype=np.float32
                        ),
                    }
                )

        # Les param√®tres de l'agent sont maintenant dans la config du worker
        agent_config = worker_config.get("agent_config", {})
        if not agent_config:
            raise ValueError(f"'agent_config' not found for worker {instance_id}")

        # Initialiser le mod√®le avec MultiInputPolicy pour les espaces d'observation de type dictionnaire
        policy_class = "MultiInputPolicy"
        logger.info(f"Using policy: {policy_class} (for dict observation space)")

        def validate_numeric_param(value, param_name, min_val=None, max_val=None):
            """Valide un param√®tre num√©rique et le convertit en float.

            Args:
                value: Valeur √† valider (peut √™tre une liste, un tuple ou un nombre)
                param_name: Nom du param√®tre pour les messages d'erreur
                min_val: Valeur minimale autoris√©e (inclusive)
                max_val: Valeur maximale autoris√©e (inclusive)

            Returns:
                float: La valeur valid√©e

            Raises:
                ValueError: Si la valeur n'est pas valide
            """
            if isinstance(value, (list, tuple)):
                if len(value) == 0:
                    raise ValueError(f"{param_name} ne peut pas √™tre une liste vide")
                # Pour les listes, on prend la premi√®re valeur
                value = value[0]
                logging.warning(
                    f"Plusieurs valeurs fournies pour {param_name}, utilisation de {value}"
                )

            try:
                value = float(value)
            except (TypeError, ValueError) as e:
                raise ValueError(f"{param_name} doit √™tre un nombre valide") from e

            if min_val is not None and value < min_val:
                raise ValueError(f"{param_name} doit √™tre >= {min_val}")
            if max_val is not None and value > max_val:
                raise ValueError(f"{param_name} doit √™tre <= {max_val}")

            return value

        def make_linear_schedule(initial_value, final_value, param_name="param"):
            """Cr√©e un schedule lin√©aire entre une valeur initiale et finale.

            Args:
                initial_value: Valeur initiale (au d√©but de l'entra√Ænement)
                final_value: Valeur finale (√† la fin de l'entra√Ænement)
                param_name: Nom du param√®tre pour les messages de log et d'erreur

            Returns:
                Une fonction qui prend le progr√®s restant (1->0) et retourne la valeur courante

            Raises:
                ValueError: Si les valeurs fournies ne sont pas valides
            """
            try:
                initial = float(initial_value)
                final = float(final_value)
            except (TypeError, ValueError) as e:
                raise ValueError(
                    f"Les valeurs initiale et finale de {param_name} doivent √™tre des nombres"
                ) from e

            def schedule(progress_remaining: float) -> float:
                return final + (initial - final) * progress_remaining

            return schedule

        # Cr√©er ou charger le mod√®le
        if shared_model_path and os.path.exists(shared_model_path):
            logger.info(f"Chargement du mod√®le partag√© depuis {shared_model_path}")
            model = PPO.load(shared_model_path, env=vec_env, verbose=1)
        else:
            logger.info("Cr√©ation d'un nouveau mod√®le")

            # Extraire la configuration du mod√®le
            model_config = {k: v for k, v in agent_config.items() if k != "policy"}

            try:
                # Fonction utilitaire pour analyser les valeurs num√©riques ou les plages
                def parse_numeric_or_range(value, default_range=None, param_name='param'):
                    """Parse une valeur ou une plage de valeurs num√©riques.

                    Args:
                        value: La valeur √† parser (nombre, cha√Æne, liste, etc.)
                        default_range: Plage par d√©faut si la valeur ne peut pas √™tre pars√©e
                        param_name: Nom du param√®tre pour les messages d'erreur

                    Returns:
                        Une liste [min, max] de valeurs num√©riques valides
                    """
                    import ast
                    import numbers

                    if default_range is None:
                        default_range = [1e-4, 1e-5]

                    # D√©finir les limites en fonction du param√®tre
                    limits = {
                        'learning_rate': (1e-7, 2.5e-5),  # Min: 1e-7, Max: 2.5e-5
                        'ent_coef': (1e-5, 0.1),         # Min: 1e-5, Max: 0.1
                        'gamma': (0.8, 0.9999)           # Min: 0.8, Max: 0.9999
                    }

                    min_val, max_val = limits.get(param_name, (None, None))

                    def clip_value(val):
                        try:
                            val = float(val)
                            if min_val is not None and val < min_val:
                                logger.warning(f"{param_name} {val} inf√©rieur √† la valeur minimale {min_val}, utilisation de {min_val}")
                                return min_val
                            if max_val is not None and val > max_val:
                                logger.warning(f"{param_name} {val} sup√©rieur √† la valeur maximale {max_val}, utilisation de {max_val}")
                                return max_val
                            return val
                        except (ValueError, TypeError):
                            logger.warning(f"Impossible de convertir la valeur {val} en nombre, utilisation de la valeur par d√©faut")
                            return default_range[0]  # Retourne la premi√®re valeur par d√©faut

                    # Gestion des valeurs None
                    if value is None:
                        return [clip_value(x) for x in default_range]

                    # Gestion des cha√Ænes de caract√®res
                    if isinstance(value, str):
                        try:
                            # Essayer de parser la cha√Æne comme une expression Python
                            value = ast.literal_eval(value)
                        except (ValueError, SyntaxError):
                            try:
                                # Si ce n'est pas une expression, essayer de convertir directement en float
                                val = float(value)
                                return [clip_value(val * 0.1), clip_value(val)]
                            except (ValueError, TypeError):
                                logger.warning(f"Impossible de parser la valeur num√©rique : {value}. Utilisation de la valeur par d√©faut.")
                                return [clip_value(x) for x in default_range]

                    # Conversion des types numpy/pandas si n√©cessaire
                    if hasattr(value, 'tolist'):
                        try:
                            value = value.tolist()
                        except Exception as e:
                            logger.warning(f"Erreur lors de la conversion tolist: {e}")

                    # Traitement des nombres simples
                    if isinstance(value, numbers.Number):
                        value = float(value)
                        return [clip_value(value * 0.1), clip_value(value)]

                    # Traitement des listes/tuples
                    if isinstance(value, (list, tuple)):
                        if not value:
                            return [clip_value(x) for x in default_range]

                        # Convertir tous les √©l√©ments en float et les clip
                        values = []
                        for x in value[:2]:
                            try:
                                values.append(float(x))
                            except (ValueError, TypeError):
                                logger.warning(f"Valeur non num√©rique d√©tect√©e: {x}, utilisation de la valeur par d√©faut")
                                values.append(default_range[0])

                        # Si une seule valeur, cr√©er une plage
                        if len(values) == 1:
                            val = values[0]
                            return [clip_value(val * 0.1), clip_value(val)]

                        # Pour deux valeurs ou plus, prendre les deux premi√®res et les clip
                        return [clip_value(x) for x in values[:2]]

                    # Si le type n'est pas reconnu, retourner les valeurs par d√©faut
                    logger.warning(f"Type de valeur non pris en charge: {type(value)}. Utilisation des valeurs par d√©faut.")
                    return [clip_value(x) for x in default_range]

                # R√©cup√©ration et traitement du learning rate
                lr_config = model_config.pop('learning_rate', None)
                lr_range = parse_numeric_or_range(lr_config, [2.5e-5, 2.5e-5], 'learning_rate')

                # Si une seule valeur, cr√©er une plage autour de cette valeur
                if len(lr_range) == 1:
                    lr_val = lr_range[0]
                    lr_range = [lr_val * 0.1, lr_val]

                # S'assurer que nous avons exactement 2 valeurs
                if len(lr_range) != 2:
                    logger.warning(f"Format de plage de learning rate invalide: {lr_range}. Utilisation des valeurs par d√©faut.")
                    lr_range = [2.5e-5, 2.5e-5]

                # Cr√©ation du schedule de learning rate
                learning_rate = make_linear_schedule(lr_range[0], lr_range[1], "learning_rate")
                logger.info(f"Learning rate: {lr_range[0]:.2e} -> {lr_range[1]:.2e}")

                # Configurer ent_coef avec une valeur fixe
                ent_coef = model_config.pop('ent_coef', 0.01)
                ent_coef = validate_numeric_param(ent_coef, "ent_coef", 0.0, 1.0)

                # Configurer gamma avec une valeur fixe
                gamma = model_config.pop('gamma', 0.99)
                gamma = validate_numeric_param(gamma, "gamma", 0.8, 0.9999)

                # Valider les autres param√®tres num√©riques du mod√®le
                if 'n_steps' in model_config:
                    model_config['n_steps'] = int(validate_numeric_param(
                        model_config['n_steps'], 'n_steps', 1, 100000
                    ))
                if 'batch_size' in model_config:
                    model_config['batch_size'] = int(validate_numeric_param(
                        model_config['batch_size'], 'batch_size', 1, 100000
                    ))
                if 'n_epochs' in model_config:
                    model_config['n_epochs'] = int(validate_numeric_param(
                        model_config['n_epochs'], 'n_epochs', 1, 100
                    ))

                logger.info(f"Learning rate: {lr_range[0]:.2e} -> {lr_range[1]:.2e}")
                logger.info(f"Entropy coefficient: {ent_coef}")
                logger.info(f"Gamma: {gamma}")

                model = PPO(
                    policy=policy_class,
                    env=vec_env,
                    verbose=1,
                    learning_rate=learning_rate,
                    ent_coef=ent_coef,
                    gamma=gamma,
                    tensorboard_log=f"logs/tensorboard/instance_{instance_id}",
                    **model_config,
                )

            except ValueError as e:
                logger.error(f"Erreur de configuration du mod√®le: {str(e)}")
                raise

        # Configuration de l'entra√Ænement
        n_steps = agent_config.get("n_steps", 2048)
        epochs = (total_timesteps // n_steps) + 1
        checkpoint_interval = 300  # 5 minutes

        # Charger le checkpoint s'il existe
        start_epoch = 0
        if checkpoint_path and os.path.exists(checkpoint_path):
            start_epoch = load_checkpoint(
                model, model.policy.optimizer, checkpoint_path
            )
            logger.info(
                f"Reprise de l'entra√Ænement √† partir de l'epoch {start_epoch+1}/{epochs}"
            )

        # Boucle d'entra√Ænement
        try:
            for epoch in range(start_epoch, epochs):
                current_time = time.time()
                time_elapsed = current_time - start_time

                # V√©rifier le timeout
                if timeout and time_elapsed >= timeout:
                    logger.info(f"Timeout atteint apr√®s {time_elapsed:.1f} secondes")
                    break

                logger.info(f"D√©but de l'epoch {epoch+1}/{epochs}")

                # Entra√Ænement sur une epoch
                model.learn(
                    total_timesteps=n_steps,
                    tb_log_name=f"instance_{instance_id}_{worker_config['name'].lower()}",
                    progress_bar=True,
                    reset_num_timesteps=False,
                )

                # Sauvegarder le checkpoint p√©riodiquement
                current_time = time.time()
                if (
                    checkpoint_path
                    and (current_time - last_checkpoint_time) >= checkpoint_interval
                ):
                    save_checkpoint(
                        model, model.policy.optimizer, epoch, checkpoint_path
                    )
                    logger.info(f"Checkpoint sauvegard√© √† {checkpoint_path}")
                    last_checkpoint_time = current_time

            # Sauvegarder le mod√®le final
            if checkpoint_path:
                save_checkpoint(model, model.policy.optimizer, epoch, checkpoint_path)
                logger.info(f"Checkpoint final sauvegard√© √† {checkpoint_path}")

        except Exception as e:
            logger.error(f"Erreur pendant l'entra√Ænement: {str(e)}")
            raise

        training_time = time.time() - start_time
        worker_name = worker_config["name"].lower().replace(" ", "_")
        instance_model_path = f"models/instance_{instance_id}_{worker_name}_final.zip"
        model.save(instance_model_path)

        obs = vec_env.reset()
        total_reward = 0
        for _ in range(100):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = vec_env.step(action)
            total_reward += reward[0]
            if done[0]:
                obs = vec_env.reset()

        avg_reward = total_reward / 100

        vec_env.close()

        results = {
            "instance_id": instance_id,
            "name": worker_config["name"],
            "initial_capital": base_config["environment"]["initial_balance"],
            "training_time": training_time,
            "avg_reward": avg_reward,
            "model_path": instance_model_path,
            "timesteps": total_timesteps,
        }

        logger.info(
            f"‚úÖ Instance {instance_id} completed - Avg Reward: {avg_reward:.4f}, Time: {training_time:.1f}s"
        )
        return results

    except Exception as e:
        logger.error(f"‚ùå Instance {instance_id} failed: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"instance_id": instance_id, "error": str(e), "success": False}


def main(
    config_path: str = "config/config.yaml",
    timeout: int = None,
    checkpoint_dir: str = "checkpoints",
    shared_model_path: str = None,
):
    """Fonction principale d'entra√Ænement parall√®le"""
    logger = setup_logging()
    logger.info("üöÄ Starting ADAN Parallel Training")

    # Charger la configuration
    config = load_base_config()  # Load base config without override initially

    num_instances = config["training"]["num_instances"]
    timesteps_per_instance = config["training"]["timesteps_per_instance"]

    # Cr√©er les r√©pertoires n√©cessaires
    os.makedirs("models", exist_ok=True)
    os.makedirs("logs/tensorboard", exist_ok=True)

    logger.info("Training configuration:")
    logger.info(f"  - Timesteps per instance: {timesteps_per_instance}")
    logger.info(f"  - Parallel workers: {num_instances}")
    logger.info(f"  - Total training steps: {timesteps_per_instance * num_instances}")

    # Lancement de l'entra√Ænement parall√®le
    start_time = time.time()
    results = []

    # Cr√©er le dossier de checkpoints si n√©cessaire
    if checkpoint_dir:
        os.makedirs(checkpoint_dir, exist_ok=True)

    with ProcessPoolExecutor(max_workers=num_instances) as executor:
        # Soumettre les t√¢ches d'entra√Ænement avec les param√®tres de timeout
        futures = {
            executor.submit(
                train_single_instance,
                instance_id=i,
                total_timesteps=timesteps_per_instance,
                config_override=None,
                shared_model_path=shared_model_path,
                checkpoint_path=os.path.join(
                    checkpoint_dir, f"instance_{i}_checkpoint.pt"
                )
                if checkpoint_dir
                else None,
                timeout=timeout,
            ): i
            for i in range(1, num_instances + 1)
        }

        # Collecter les r√©sultats
        for future in as_completed(futures):
            i = futures[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Instance {i} completed successfully")
            except Exception as e:
                logger.error(f"Instance {i} failed with error: {e}")
                results.append({"instance_id": i, "error": str(e), "success": False})

    total_time = time.time() - start_time

    # Analyser les r√©sultats
    successful_results = [r for r in results if "error" not in r]
    failed_results = [r for r in results if "error" in r]

    logger.info("üìä Training Results Summary:")
    logger.info(f"  - Total time: {total_time:.1f}s")
    logger.info(f"  - Successful instances: {len(successful_results)}/{len(results)}")
    logger.info(f"  - Failed instances: {len(failed_results)}/{len(results)}")

    if successful_results:
        logger.info("  - Instance Performance:")
        for result in successful_results:
            logger.info(
                f"    * {result['name']}: Reward={result['avg_reward']:.4f}, Time={result['training_time']:.1f}s"
            )

        # Fusionner les mod√®les r√©ussis
        model_paths = [
            r["model_path"]
            for r in successful_results
            if os.path.exists(r["model_path"])
        ]
        if len(model_paths) > 1:
            # Assuming merge_models function exists and is imported
            # from .utils import merge_models # Example import
            # merge_models(model_paths, merged_model_path)
            logger.info(
                "Skipping model merge for now. Implement merge_models if needed."
            )

    # Sauvegarder les r√©sultats d√©taill√©s
    results_path = (
        f"logs/parallel_training_results_{int(datetime.now().timestamp())}.json"
    )
    import json

    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)

    logger.info(f"üìã Detailed results saved to: {results_path}")
    logger.info("üéâ Parallel training completed!")

    return len(successful_results) == len(results)


if __name__ == "__main__":
    # Parse arguments if run from command line
    import argparse

    parser = argparse.ArgumentParser(
        description="Train ADAN trading bot with timeout and checkpoint support"
    )
    parser.add_argument(
        "--config", type=str, default="config/config.yaml", help="Path to config file"
    )
    parser.add_argument(
        "--timeout", type=int, default=None, help="Maximum training time in seconds"
    )
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints",
        help="Directory to save checkpoints",
    )
    parser.add_argument(
        "--shared-model",
        type=str,
        default=None,
        help="Path to shared model for distributed training",
    )
    parser.add_argument(
        "--resume", action="store_true", help="Resume training from latest checkpoint"
    )
    args = parser.parse_args()

    # Call main with all arguments
    success = main(
        config_path=args.config,
        timeout=args.timeout,
        checkpoint_dir=args.checkpoint_dir,
        shared_model_path=args.shared_model,
    )
    sys.exit(0 if success else 1)
