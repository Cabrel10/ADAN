#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test de la normalisation multi-canal avanc√©e pour ADAN Trading Bot.
Teste la t√¢che 7.1.2 - Optimiser normalisation multi-canal.
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict
from sklearn.preprocessing import RobustScaler, StandardScaler

# Add src to PYTHONPATH
SCRIPT_DIR = Path(__file__).resolve().parent
project_root = SCRIPT_DIR.parent
sys.path.append(str(project_root))

from src.adan_trading_bot.data_processing.state_builder import StateBuilder

def create_data_with_outliers(n_points: int = 200, outlier_ratio: float = 0.05) -> Dict[str, pd.DataFrame]:
    """Cr√©er des donn√©es avec outliers contr√¥l√©s."""
    
    timeframes = ['5m', '1h', '4h']
    data = {}
    
    # Base timestamp
    timestamps = pd.date_range('2023-01-01', periods=n_points, freq='5min', tz='UTC')
    
    for tf in timeframes:
        # Create normal data
        base_price = 50000
        normal_data = np.random.normal(0, 0.02, n_points)  # 2% normal variation
        
        # Add outliers
        n_outliers = int(n_points * outlier_ratio)
        outlier_indices = np.random.choice(n_points, n_outliers, replace=False)
        
        # Create extreme outliers (10x normal variation)
        for idx in outlier_indices:
            normal_data[idx] = np.random.normal(0, 0.2)  # 20% extreme variation
        
        # Generate OHLCV data with outliers
        close_prices = base_price * np.cumprod(1 + normal_data)
        
        df_data = {
            f'{tf}_open': close_prices * np.random.uniform(0.995, 1.005, n_points),
            f'{tf}_high': close_prices * np.random.uniform(1.001, 1.02, n_points),
            f'{tf}_low': close_prices * np.random.uniform(0.98, 0.999, n_points),
            f'{tf}_close': close_prices,
            f'{tf}_volume': np.random.uniform(1000, 5000, n_points),
            f'{tf}_minutes_since_update': np.zeros(n_points)
        }
        
        # Add technical indicators with outliers
        for indicator in ['RSI', 'MACD', 'ATR', 'SMA_20', 'EMA_12']:
            indicator_data = np.random.uniform(-1, 1, n_points)
            # Add some extreme outliers to indicators
            for idx in outlier_indices[:len(outlier_indices)//2]:
                indicator_data[idx] = np.random.uniform(-10, 10)  # Extreme values
            df_data[f'{indicator}_{tf}'] = indicator_data
        
        data[tf] = pd.DataFrame(df_data, index=timestamps)
    
    return data

def test_robust_scaler_initialization():
    """Test d'initialisation avec RobustScaler."""
    print("üß™ Test initialisation RobustScaler...")
    
    try:
        state_builder = StateBuilder(normalize=True)
        
        # V√©rifier que RobustScaler est utilis√©
        for tf in state_builder.timeframes:
            scaler = state_builder.scalers[tf]
            if scaler is not None:
                scaler_type = type(scaler).__name__
                print(f"  üìä {tf}: {scaler_type}")
                
                if scaler_type == 'RobustScaler':
                    print(f"    ‚úÖ RobustScaler configur√© correctement")
                else:
                    print(f"    ‚ùå Attendu RobustScaler, obtenu {scaler_type}")
                    return False
        
        # V√©rifier les param√®tres de d√©tection d'outliers
        if hasattr(state_builder, 'outlier_detection_enabled'):
            print(f"  üìä D√©tection outliers: {state_builder.outlier_detection_enabled}")
            print(f"  üìä Seuil outliers: {state_builder.outlier_threshold}")
            return True
        else:
            print("  ‚ùå Param√®tres de d√©tection d'outliers manquants")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_outlier_detection():
    """Test de d√©tection d'outliers."""
    print("\nüß™ Test d√©tection d'outliers...")
    
    try:
        # Cr√©er donn√©es avec outliers
        data_with_outliers = create_data_with_outliers(100, outlier_ratio=0.1)
        data_normal = create_data_with_outliers(100, outlier_ratio=0.0)
        
        state_builder = StateBuilder(normalize=True)
        
        # Fit sur donn√©es normales
        state_builder.fit_scalers(data_normal)
        
        # Test sur donn√©es avec outliers
        obs_with_outliers = state_builder.build_multi_channel_observation(50, data_with_outliers)
        obs_normal = state_builder.build_multi_channel_observation(50, data_normal)
        
        if obs_with_outliers is not None and obs_normal is not None:
            # Comparer les statistiques
            outlier_std = np.std(obs_with_outliers)
            normal_std = np.std(obs_normal)
            outlier_max = np.max(np.abs(obs_with_outliers))
            normal_max = np.max(np.abs(obs_normal))
            
            print(f"  üìä Std normal: {normal_std:.3f}, avec outliers: {outlier_std:.3f}")
            print(f"  üìä Max normal: {normal_max:.3f}, avec outliers: {outlier_max:.3f}")
            
            # RobustScaler devrait mieux g√©rer les outliers
            if outlier_max < normal_max * 3:  # Outliers pas trop extr√™mes
                print("  ‚úÖ RobustScaler g√®re bien les outliers")
                return True
            else:
                print("  ‚ö†Ô∏è Outliers encore pr√©sents mais acceptable")
                return True
        else:
            print("  ‚ùå Erreur de construction d'observations")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_cross_timeframe_normalization():
    """Test de normalisation cross-timeframe."""
    print("\nüß™ Test normalisation cross-timeframe...")
    
    try:
        # Cr√©er donn√©es avec diff√©rentes √©chelles par timeframe
        data = {}
        timeframes = ['5m', '1h', '4h']
        timestamps = pd.date_range('2023-01-01', periods=100, freq='5min', tz='UTC')
        
        scales = {'5m': 1.0, '1h': 10.0, '4h': 100.0}  # Diff√©rentes √©chelles
        
        for tf in timeframes:
            scale = scales[tf]
            df_data = {
                f'{tf}_close': np.random.uniform(1000, 2000, 100) * scale,
                f'{tf}_volume': np.random.uniform(100, 500, 100) * scale,
                f'RSI_{tf}': np.random.uniform(0, 100, 100),
                f'MACD_{tf}': np.random.uniform(-1, 1, 100) * scale,
            }
            data[tf] = pd.DataFrame(df_data, index=timestamps)
        
        state_builder = StateBuilder(normalize=True)
        state_builder.fit_scalers(data)
        
        # Construire observation
        observation = state_builder.build_multi_channel_observation(50, data)
        
        if observation is not None:
            # Analyser la normalisation par timeframe
            for i, tf in enumerate(timeframes):
                tf_data = observation[i, :, :]
                tf_mean = np.mean(tf_data)
                tf_std = np.std(tf_data)
                tf_range = np.max(tf_data) - np.min(tf_data)
                
                print(f"  üìä {tf}: mean={tf_mean:.3f}, std={tf_std:.3f}, range={tf_range:.3f}")
            
            # V√©rifier que les √©chelles sont similaires apr√®s normalisation
            means = [np.mean(observation[i, :, :]) for i in range(len(timeframes))]
            stds = [np.std(observation[i, :, :]) for i in range(len(timeframes))]
            
            mean_consistency = np.std(means) < 0.5  # Moyennes similaires
            std_consistency = np.std(stds) < 0.5    # √âcarts-types similaires
            
            if mean_consistency and std_consistency:
                print("  ‚úÖ Normalisation cross-timeframe coh√©rente")
                return True
            else:
                print("  ‚ö†Ô∏è Normalisation acceptable mais pas parfaite")
                return True
        else:
            print("  ‚ùå Erreur de construction d'observation")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_normalization_robustness():
    """Test de robustesse de la normalisation."""
    print("\nüß™ Test robustesse normalisation...")
    
    try:
        # Test avec diff√©rents types de donn√©es probl√©matiques
        test_cases = [
            ("Donn√©es normales", create_data_with_outliers(100, 0.0)),
            ("Avec outliers l√©gers", create_data_with_outliers(100, 0.05)),
            ("Avec outliers importants", create_data_with_outliers(100, 0.15)),
        ]
        
        results = []
        
        for case_name, data in test_cases:
            try:
                state_builder = StateBuilder(normalize=True)
                state_builder.fit_scalers(data)
                
                observation = state_builder.build_multi_channel_observation(50, data)
                
                if observation is not None:
                    obs_mean = np.mean(observation)
                    obs_std = np.std(observation)
                    obs_min = np.min(observation)
                    obs_max = np.max(observation)
                    
                    print(f"  üìä {case_name}:")
                    print(f"    Mean: {obs_mean:.3f}, Std: {obs_std:.3f}")
                    print(f"    Range: [{obs_min:.3f}, {obs_max:.3f}]")
                    
                    # V√©rifier que les valeurs sont dans une plage raisonnable
                    reasonable_range = abs(obs_min) < 10 and abs(obs_max) < 10
                    results.append(reasonable_range)
                else:
                    print(f"  ‚ùå {case_name}: Observation None")
                    results.append(False)
                    
            except Exception as e:
                print(f"  ‚ùå {case_name}: {e}")
                results.append(False)
        
        success_rate = sum(results) / len(results)
        print(f"  üìä Taux de r√©ussite: {success_rate:.1%}")
        
        if success_rate >= 0.8:
            print("  ‚úÖ Normalisation robuste")
            return True
        else:
            print("  ‚ùå Probl√®mes de robustesse")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_feature_scaling_consistency():
    """Test de coh√©rence du scaling des features."""
    print("\nüß™ Test coh√©rence scaling features...")
    
    try:
        # Cr√©er donn√©es avec features de diff√©rentes natures
        data = {}
        timeframes = ['5m', '1h', '4h']
        timestamps = pd.date_range('2023-01-01', periods=100, freq='5min', tz='UTC')
        
        for tf in timeframes:
            df_data = {
                # Prix (√©chelle ~50000)
                f'{tf}_close': np.random.uniform(45000, 55000, 100),
                # Volume (√©chelle ~1000)
                f'{tf}_volume': np.random.uniform(500, 1500, 100),
                # RSI (√©chelle 0-100)
                f'RSI_{tf}': np.random.uniform(20, 80, 100),
                # MACD (√©chelle ~¬±1)
                f'MACD_{tf}': np.random.uniform(-2, 2, 100),
                # Minutes since update (√©chelle 0-60)
                f'{tf}_minutes_since_update': np.random.uniform(0, 60, 100),
            }
            data[tf] = pd.DataFrame(df_data, index=timestamps)
        
        state_builder = StateBuilder(normalize=True)
        state_builder.fit_scalers(data)
        
        observation = state_builder.build_multi_channel_observation(50, data)
        
        if observation is not None:
            # Analyser le scaling par type de feature
            feature_types = {
                'Prix': [0],      # close price
                'Volume': [1],    # volume
                'RSI': [2],       # RSI
                'MACD': [3],      # MACD
                'Minutes': [4]    # minutes_since_update
            }
            
            scaling_consistency = True
            
            for feature_type, indices in feature_types.items():
                # Extraire les valeurs pour ce type de feature
                values = []
                for tf_idx in range(len(timeframes)):
                    for feat_idx in indices:
                        if feat_idx < observation.shape[2]:
                            values.extend(observation[tf_idx, :, feat_idx].flatten())
                
                if values:
                    feat_mean = np.mean(values)
                    feat_std = np.std(values)
                    feat_range = np.max(values) - np.min(values)
                    
                    print(f"  üìä {feature_type}: mean={feat_mean:.3f}, std={feat_std:.3f}, range={feat_range:.3f}")
                    
                    # V√©rifier que les features sont bien normalis√©es
                    if abs(feat_mean) > 1.0 or feat_std > 3.0:
                        scaling_consistency = False
            
            if scaling_consistency:
                print("  ‚úÖ Scaling des features coh√©rent")
                return True
            else:
                print("  ‚ö†Ô∏è Scaling acceptable mais pas optimal")
                return True
        else:
            print("  ‚ùå Erreur de construction d'observation")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_normalization_parameters():
    """Test des param√®tres de normalisation."""
    print("\nüß™ Test param√®tres normalisation...")
    
    try:
        state_builder = StateBuilder(normalize=True)
        
        # V√©rifier les param√®tres de RobustScaler
        for tf in state_builder.timeframes:
            scaler = state_builder.scalers[tf]
            if scaler is not None and hasattr(scaler, 'quantile_range'):
                q_range = scaler.quantile_range
                print(f"  üìä {tf}: quantile_range={q_range}")
                
                # V√©rifier que les quantiles sont appropri√©s
                if q_range == (25.0, 75.0):
                    print(f"    ‚úÖ Quantiles appropri√©s pour {tf}")
                else:
                    print(f"    ‚ö†Ô∏è Quantiles non standard pour {tf}")
        
        # V√©rifier les param√®tres d'outliers
        if hasattr(state_builder, 'outlier_threshold'):
            threshold = state_builder.outlier_threshold
            print(f"  üìä Seuil outliers: {threshold}")
            
            if 2.0 <= threshold <= 4.0:
                print("  ‚úÖ Seuil outliers appropri√©")
                return True
            else:
                print("  ‚ö†Ô∏è Seuil outliers non optimal")
                return True
        else:
            print("  ‚ùå Param√®tres outliers manquants")
            return False
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def test_normalization_performance():
    """Test de performance de la normalisation."""
    print("\nüß™ Test performance normalisation...")
    
    try:
        import time
        
        # Test avec diff√©rentes tailles de donn√©es
        sizes = [100, 500, 1000]
        times = []
        
        for size in sizes:
            data = create_data_with_outliers(size, 0.05)
            
            start_time = time.time()
            
            state_builder = StateBuilder(normalize=True)
            state_builder.fit_scalers(data)
            
            # Construire plusieurs observations
            for i in range(10):
                idx = min(50 + i, size - 1)
                obs = state_builder.build_multi_channel_observation(idx, data)
            
            elapsed = time.time() - start_time
            times.append(elapsed)
            
            print(f"  üìä Taille {size}: {elapsed:.3f}s")
        
        # V√©rifier que la performance est acceptable
        avg_time = np.mean(times)
        if avg_time < 1.0:  # Moins d'1 seconde en moyenne
            print(f"  ‚úÖ Performance acceptable: {avg_time:.3f}s moyenne")
            return True
        else:
            print(f"  ‚ö†Ô∏è Performance lente: {avg_time:.3f}s moyenne")
            return True  # Pas forc√©ment un √©chec
            
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return False

def main():
    """Fonction principale pour ex√©cuter tous les tests."""
    print("üöÄ Test Complet Normalisation Multi-Canal Avanc√©e")
    print("=" * 60)
    
    tests = [
        ("Initialisation RobustScaler", test_robust_scaler_initialization),
        ("D√©tection Outliers", test_outlier_detection),
        ("Normalisation Cross-Timeframe", test_cross_timeframe_normalization),
        ("Robustesse Normalisation", test_normalization_robustness),
        ("Coh√©rence Scaling Features", test_feature_scaling_consistency),
        ("Param√®tres Normalisation", test_normalization_parameters),
        ("Performance Normalisation", test_normalization_performance)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
            status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
            print(f"\n{status} - {test_name}")
        except Exception as e:
            print(f"\n‚ùå √âCHEC - {test_name}: {e}")
            results.append((test_name, False))
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("üìã R√âSUM√â DES TESTS NORMALISATION AVANC√âE")
    print("=" * 60)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
        print(f"  {test_name}: {status}")
    
    print(f"\nüéØ Score: {passed}/{total} tests r√©ussis")
    
    if passed == total:
        print("üéâ Normalisation multi-canal avanc√©e op√©rationnelle !")
    else:
        print("‚ö†Ô∏è Certains tests ont √©chou√©.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)