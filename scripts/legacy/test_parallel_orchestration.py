#!/usr/bin/env python3
"""
Test de performance r√©aliste pour l'orchestration parall√®le avec de vrais agents.
Mesure les gains de performance r√©els avec des environnements parall√®les.
"""

import time
import psutil
import os
import sys
import json
import threading
import multiprocessing
from datetime import datetime
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from unittest.mock import MagicMock

# Add src to path
sys.path.insert(0, '/home/morningstar/Documents/trading/ADAN/src')

# Mock dependencies
sys.modules['adan_trading_bot.online_learning_agent'] = MagicMock()
sys.modules['adan_trading_bot.environment.multi_asset_env'] = MagicMock()

from adan_trading_bot.training_orchestrator import TrainingOrchestrator


class RealisticAgent:
    """Agent r√©aliste qui simule une charge de travail d'entra√Ænement authentique"""
    
    def __init__(self, model=None, env=None, config=None, experience_buffer=None, agent_id=0):
        self.model = model
        self.env = env
        self.config = config
        self.experience_buffer = experience_buffer
        self.agent_id = agent_id
        self.training_steps = config.get('training_steps', 1000) if config else 1000
        self.computation_intensity = config.get('computation_intensity', 1.0) if config else 1.0
        
    def run(self):
        """Simule un entra√Ænement r√©aliste avec calculs intensifs"""
        print(f"ü§ñ Agent {self.agent_id} d√©marr√© - {self.training_steps} steps")
        start_time = time.time()
        
        for step in range(self.training_steps):
            # Simulation calculs de forward pass
            self._simulate_forward_pass()
            
            # Simulation calculs de backward pass
            self._simulate_backward_pass()
            
            # Simulation interaction avec experience buffer
            if self.experience_buffer and hasattr(self.experience_buffer, 'add'):
                self._add_experience(step)
            
            # Simulation d'apprentissage p√©riodique
            if step % 100 == 0:
                self._simulate_learning_update()
        
        end_time = time.time()
        print(f"‚úÖ Agent {self.agent_id} termin√© en {end_time - start_time:.2f}s")
        return end_time - start_time
    
    def _simulate_forward_pass(self):
        """Simule les calculs de forward pass (CNN, attention, etc.)"""
        # Simulation calculs matriciels intensifs
        import numpy as np
        
        # Simule traitement d'observations 3D (timeframes x window x features)
        obs_shape = (3, 100, 28)  # 3 timeframes, 100 timesteps, 28 features
        observation = np.random.randn(*obs_shape) * self.computation_intensity
        
        # Simule convolutions 3D
        for _ in range(3):  # 3 couches conv
            kernel = np.random.randn(3, 3, 3) * 0.1
            # Simulation convolution simplifi√©e
            result = np.sum(observation * kernel[0, 0, 0]) * 0.001
        
        # Simule m√©canisme d'attention
        attention_weights = np.random.rand(3)  # 3 timeframes
        attention_weights = attention_weights / np.sum(attention_weights)
        weighted_obs = observation * attention_weights.reshape(3, 1, 1)
        
        # Petite pause pour simuler le temps de calcul
        time.sleep(0.001 * self.computation_intensity)
    
    def _simulate_backward_pass(self):
        """Simule les calculs de backward pass (gradients)"""
        import numpy as np
        
        # Simule calcul de gradients
        grad_shape = (128, 64)  # Taille typique des couches denses
        gradients = np.random.randn(*grad_shape) * 0.01
        
        # Simule mise √† jour des poids
        weights = np.random.randn(*grad_shape)
        updated_weights = weights - 0.001 * gradients
        
        # Petite pause pour simuler le temps de calcul
        time.sleep(0.0005 * self.computation_intensity)
    
    def _add_experience(self, step):
        """Simule l'ajout d'exp√©rience au buffer"""
        try:
            self.experience_buffer.add(
                obs=f"obs_{self.agent_id}_{step}",
                action=step % 3,
                reward=np.random.randn() * 0.1,
                next_obs=f"next_obs_{self.agent_id}_{step}",
                done=False,
                priority=np.random.rand()
            )
        except:
            pass  # Buffer mock peut ne pas avoir toutes les m√©thodes
    
    def _simulate_learning_update(self):
        """Simule une mise √† jour d'apprentissage"""
        import numpy as np
        
        # Simule sampling du buffer et calcul de loss
        batch_size = 32
        states = np.random.randn(batch_size, 3, 100, 28)
        actions = np.random.randint(0, 3, batch_size)
        rewards = np.random.randn(batch_size)
        
        # Simule calcul de Q-values et loss
        q_values = np.random.randn(batch_size, 3)
        target_q = rewards + 0.99 * np.max(q_values, axis=1)
        loss = np.mean((q_values[range(batch_size), actions] - target_q) ** 2)
        
        # Pause plus longue pour simuler l'apprentissage
        time.sleep(0.005 * self.computation_intensity)


class ParallelOrchestrationBenchmark:
    """Benchmark de performance pour l'orchestration parall√®le r√©aliste"""
    
    def __init__(self):
        self.results = {}
        self.cpu_count = multiprocessing.cpu_count()
        
    def measure_resource_usage(self, func, *args, **kwargs):
        """Mesure l'usage des ressources pendant l'ex√©cution"""
        process = psutil.Process(os.getpid())
        
        # Mesures initiales
        cpu_before = process.cpu_percent()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        start_time = time.time()
        
        # Ex√©cution de la fonction
        result = func(*args, **kwargs)
        
        end_time = time.time()
        
        # Mesures finales
        cpu_after = process.cpu_percent()
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        return {
            'result': result,
            'execution_time': end_time - start_time,
            'cpu_usage': max(cpu_after, cpu_before),
            'memory_usage_mb': memory_after,
            'memory_delta_mb': memory_after - memory_before
        }
    
    def benchmark_sequential_training(self, num_agents: int = 4, training_steps: int = 500):
        """Benchmark d'entra√Ænement s√©quentiel (1 agent √† la fois)"""
        print(f"üîÑ Benchmark s√©quentiel avec {num_agents} agents...")
        
        config = {
            'training_steps': training_steps,
            'computation_intensity': 1.0
        }
        
        def sequential_training():
            total_time = 0
            for i in range(num_agents):
                agent = RealisticAgent(config=config, agent_id=i)
                agent_time = agent.run()
                total_time += agent_time
            return total_time
        
        metrics = self.measure_resource_usage(sequential_training)
        
        self.results['sequential'] = {
            'num_agents': num_agents,
            'training_steps_per_agent': training_steps,
            'total_training_steps': training_steps * num_agents,
            'execution_mode': 'sequential',
            **metrics
        }
        
        print(f"‚úÖ S√©quentiel termin√© en {metrics['execution_time']:.2f}s")
        return metrics
    
    def benchmark_threaded_training(self, num_agents: int = 4, training_steps: int = 500):
        """Benchmark d'entra√Ænement avec threads (parall√©lisme I/O)"""
        print(f"üîÑ Benchmark thread√© avec {num_agents} agents...")
        
        config = {
            'training_steps': training_steps,
            'computation_intensity': 0.8  # L√©g√®rement r√©duit pour threading
        }
        
        def threaded_training():
            def run_agent(agent_id):
                agent = RealisticAgent(config=config, agent_id=agent_id)
                return agent.run()
            
            with ThreadPoolExecutor(max_workers=num_agents) as executor:
                futures = [executor.submit(run_agent, i) for i in range(num_agents)]
                results = [future.result() for future in futures]
            
            return max(results)  # Temps du plus lent
        
        metrics = self.measure_resource_usage(threaded_training)
        
        self.results['threaded'] = {
            'num_agents': num_agents,
            'training_steps_per_agent': training_steps,
            'total_training_steps': training_steps * num_agents,
            'execution_mode': 'threaded',
            **metrics
        }
        
        print(f"‚úÖ Thread√© termin√© en {metrics['execution_time']:.2f}s")
        return metrics
    
    def benchmark_orchestrated_training(self, num_agents: int = 4, training_steps: int = 500):
        """Benchmark avec TrainingOrchestrator (notre impl√©mentation)"""
        print(f"üîÑ Benchmark orchestr√© avec {num_agents} agents...")
        
        env_configs = [{'id': f'env{i}'} for i in range(num_agents)]
        agent_config = {
            'training_steps': training_steps,
            'computation_intensity': 1.0,
            'batch_size': 64
        }
        
        def orchestrated_training():
            # Utilise des agents r√©alistes au lieu de mocks
            orchestrator = TrainingOrchestrator(
                env_configs=env_configs,
                agent_config=agent_config,
                agent_class=RealisticAgent
            )
            
            orchestrator.setup_environments()
            
            # Assigne des IDs aux agents pour le tracking
            for i, agent in enumerate(orchestrator.agents):
                agent.agent_id = i
            
            # Entra√Ænement orchestr√© (actuellement s√©quentiel dans notre impl√©mentation)
            start_time = time.time()
            orchestrator.train()
            end_time = time.time()
            
            return end_time - start_time
        
        metrics = self.measure_resource_usage(orchestrated_training)
        
        self.results['orchestrated'] = {
            'num_agents': num_agents,
            'training_steps_per_agent': training_steps,
            'total_training_steps': training_steps * num_agents,
            'execution_mode': 'orchestrated',
            **metrics
        }
        
        print(f"‚úÖ Orchestr√© termin√© en {metrics['execution_time']:.2f}s")
        return metrics
    
    def calculate_performance_gains(self):
        """Calcule les gains de performance entre les diff√©rentes approches"""
        if len(self.results) < 2:
            return None
        
        gains = {}
        
        # Comparaison s√©quentiel vs thread√©
        if 'sequential' in self.results and 'threaded' in self.results:
            seq = self.results['sequential']
            thr = self.results['threaded']
            
            gains['threaded_vs_sequential'] = {
                'speedup_factor': seq['execution_time'] / thr['execution_time'],
                'time_saved_seconds': seq['execution_time'] - thr['execution_time'],
                'time_saved_percentage': ((seq['execution_time'] - thr['execution_time']) / seq['execution_time']) * 100,
                'memory_overhead_mb': thr['memory_usage_mb'] - seq['memory_usage_mb'],
                'cpu_overhead': thr['cpu_usage'] - seq['cpu_usage']
            }
        
        # Comparaison s√©quentiel vs orchestr√©
        if 'sequential' in self.results and 'orchestrated' in self.results:
            seq = self.results['sequential']
            orch = self.results['orchestrated']
            
            gains['orchestrated_vs_sequential'] = {
                'speedup_factor': seq['execution_time'] / orch['execution_time'],
                'time_saved_seconds': seq['execution_time'] - orch['execution_time'],
                'time_saved_percentage': ((seq['execution_time'] - orch['execution_time']) / seq['execution_time']) * 100,
                'memory_overhead_mb': orch['memory_usage_mb'] - seq['memory_usage_mb'],
                'cpu_overhead': orch['cpu_usage'] - seq['cpu_usage']
            }
        
        # Comparaison thread√© vs orchestr√©
        if 'threaded' in self.results and 'orchestrated' in self.results:
            thr = self.results['threaded']
            orch = self.results['orchestrated']
            
            gains['orchestrated_vs_threaded'] = {
                'speedup_factor': thr['execution_time'] / orch['execution_time'],
                'time_saved_seconds': thr['execution_time'] - orch['execution_time'],
                'time_saved_percentage': ((thr['execution_time'] - orch['execution_time']) / thr['execution_time']) * 100,
                'memory_overhead_mb': orch['memory_usage_mb'] - thr['memory_usage_mb'],
                'cpu_overhead': orch['cpu_usage'] - thr['cpu_usage']
            }
        
        self.results['performance_gains'] = gains
        return gains
    
    def run_comprehensive_benchmark(self, num_agents: int = 4, training_steps: int = 500):
        """Ex√©cute un benchmark complet avec toutes les approches"""
        print("üöÄ Benchmark Complet Orchestration Parall√®le")
        print(f"Agents: {num_agents} | Steps par agent: {training_steps}")
        print(f"CPUs disponibles: {self.cpu_count}")
        print("=" * 60)
        
        # Benchmark s√©quentiel
        self.benchmark_sequential_training(num_agents, training_steps)
        
        # Benchmark thread√©
        self.benchmark_threaded_training(num_agents, training_steps)
        
        # Benchmark orchestr√©
        self.benchmark_orchestrated_training(num_agents, training_steps)
        
        # Calcul des gains
        gains = self.calculate_performance_gains()
        
        # Affichage des r√©sultats
        self.print_comprehensive_results()
        
        # Sauvegarde
        self.save_results()
        
        return self.results
    
    def print_comprehensive_results(self):
        """Affiche les r√©sultats complets du benchmark"""
        print("\n" + "=" * 60)
        print("üìä R√âSULTATS BENCHMARK ORCHESTRATION")
        print("=" * 60)
        
        # R√©sultats individuels
        for mode, data in self.results.items():
            if mode != 'performance_gains':
                print(f"\nüîπ Mode {data['execution_mode'].upper()}:")
                print(f"   Temps d'ex√©cution: {data['execution_time']:.2f}s")
                print(f"   Usage m√©moire: {data['memory_usage_mb']:.1f} MB")
                print(f"   Usage CPU: {data['cpu_usage']:.1f}%")
        
        # Gains de performance
        if 'performance_gains' in self.results:
            gains = self.results['performance_gains']
            print(f"\nüöÄ GAINS DE PERFORMANCE:")
            
            for comparison, metrics in gains.items():
                print(f"\nüìà {comparison.replace('_', ' ').title()}:")
                print(f"   Facteur d'acc√©l√©ration: {metrics['speedup_factor']:.2f}x")
                print(f"   Temps √©conomis√©: {metrics['time_saved_seconds']:.2f}s ({metrics['time_saved_percentage']:.1f}%)")
                print(f"   Surcharge m√©moire: {metrics['memory_overhead_mb']:.1f} MB")
                print(f"   Surcharge CPU: {metrics['cpu_overhead']:.1f}%")
        
        print("\n" + "=" * 60)
    
    def save_results(self):
        """Sauvegarde les r√©sultats du benchmark"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"parallel_orchestration_benchmark_{timestamp}.json"
        filepath = os.path.join("logs", filename)
        
        # Assure que le r√©pertoire logs existe
        os.makedirs("logs", exist_ok=True)
        
        # Ajoute des m√©tadonn√©es
        self.results['metadata'] = {
            'timestamp': timestamp,
            'python_version': sys.version,
            'cpu_count': self.cpu_count,
            'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024
        }
        
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        print(f"üìÅ R√©sultats sauvegard√©s: {filepath}")


def main():
    """Ex√©cution principale du benchmark"""
    benchmark = ParallelOrchestrationBenchmark()
    
    # Benchmark avec param√®tres r√©alistes
    results = benchmark.run_comprehensive_benchmark(
        num_agents=4,
        training_steps=300  # R√©duit pour des tests plus rapides
    )
    
    return results


if __name__ == "__main__":
    main()