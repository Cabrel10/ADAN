#!/usr/bin/env python3
"""
Script de test pour valider les optimisations du DataLoader pour 10+ paires de trading.
"""

import sys
import os
import pandas as pd
import numpy as np
import time
import tempfile
import shutil
from pathlib import Path

# Ajouter le chemin src au PYTHONPATH
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from adan_trading_bot.data_processing.data_loader import ComprehensiveDataLoader

def create_test_parquet_data(temp_dir: Path, assets: list, timeframes: list, num_rows: int = 1000):
    """Cr√©er des donn√©es de test au format parquet."""
    print(f"üìä Cr√©ation de donn√©es de test pour {len(assets)} assets...")
    
    for asset in assets:
        for tf in timeframes:
            # Cr√©er le r√©pertoire pour le timeframe
            tf_dir = temp_dir / tf
            tf_dir.mkdir(parents=True, exist_ok=True)
            
            # G√©n√©rer des donn√©es OHLCV r√©alistes
            np.random.seed(hash(asset + tf) % 2**32)  # Seed bas√©e sur asset et timeframe
            
            base_price = np.random.uniform(1000, 50000)  # Prix de base al√©atoire
            returns = np.random.normal(0, 0.02, num_rows)
            prices = [base_price]
            
            for ret in returns:
                prices.append(prices[-1] * (1 + ret))
            
            prices = np.array(prices[1:])
            
            # Cr√©er DataFrame OHLCV
            data = {
                'timestamp': pd.date_range(start='2023-01-01', periods=num_rows, freq='5T'),
                'open': prices * (1 + np.random.normal(0, 0.001, num_rows)),
                'high': prices * (1 + np.abs(np.random.normal(0, 0.005, num_rows))),
                'low': prices * (1 - np.abs(np.random.normal(0, 0.005, num_rows))),
                'close': prices,
                'volume': np.random.lognormal(10, 1, num_rows)
            }
            
            df = pd.DataFrame(data)
            df['__index_level_0__'] = range(len(df))  # Index pour filtrage parquet
            
            # Sauvegarder en parquet
            file_path = tf_dir / f"{asset}.parquet"
            df.to_parquet(file_path, engine='fastparquet', index=False)
    
    print(f"‚úÖ Donn√©es cr√©√©es dans {temp_dir}")

def test_basic_functionality():
    """Test de la fonctionnalit√© de base du DataLoader optimis√©."""
    print("üß™ Test Fonctionnalit√© de Base")
    print("=" * 50)
    
    # Cr√©er r√©pertoire temporaire
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Configuration de test
        assets = ['BTC', 'ETH', 'SOL']
        timeframes = ['5m', '1h', '4h']
        
        # Cr√©er donn√©es de test
        create_test_parquet_data(temp_dir, assets, timeframes, 500)
        
        # Configuration du DataLoader
        config = {
            'feature_engineering': {
                'timeframes': timeframes
            },
            'data_sources': [
                {'assets': assets}
            ],
            'chunk_size': 100,
            'enable_cache': True,
            'cache_size': 10,
            'enable_parallel_loading': True,
            'max_workers': 2
        }
        
        # Cr√©er DataLoader
        loader = ComprehensiveDataLoader(config, str(temp_dir))
        
        # Charger les chemins
        loader.load_asset_paths()
        
        print(f"üìä Assets d√©tect√©s: {loader.assets}")
        print(f"üìä Timeframes: {loader.timeframes}")
        print(f"üìä Total rows par asset: {loader.asset_total_rows}")
        
        # Tester chargement de chunks
        chunks_loaded = 0
        total_rows = 0
        
        while True:
            chunk = loader.get_next_chunk()
            if chunk is None:
                break
            
            chunks_loaded += 1
            total_rows += len(chunk)
            
            if chunks_loaded <= 3:  # Afficher d√©tails des premiers chunks
                print(f"  Chunk {chunks_loaded}: {len(chunk)} rows, "
                      f"Asset: {loader.current_asset}, "
                      f"Colonnes: {len(chunk.columns)}")
        
        print(f"‚úÖ Total chunks charg√©s: {chunks_loaded}")
        print(f"‚úÖ Total rows trait√©es: {total_rows}")
        
        return chunks_loaded > 0 and total_rows > 0
        
    finally:
        # Nettoyer
        shutil.rmtree(temp_dir)

def test_cache_performance():
    """Test des performances du cache intelligent."""
    print("\nüöÄ Test Performance Cache")
    print("=" * 50)
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Configuration avec plus d'assets pour tester le cache
        assets = ['BTC', 'ETH', 'SOL', 'ADA', 'XRP']
        timeframes = ['5m', '1h', '4h']
        
        create_test_parquet_data(temp_dir, assets, timeframes, 300)
        
        # Test avec cache activ√©
        config_with_cache = {
            'feature_engineering': {'timeframes': timeframes},
            'data_sources': [{'assets': assets}],
            'chunk_size': 50,
            'enable_cache': True,
            'cache_size': 20,
            'enable_parallel_loading': True,
            'max_workers': 3
        }
        
        loader_cached = ComprehensiveDataLoader(config_with_cache, str(temp_dir))
        loader_cached.load_asset_paths()
        
        # Premier passage (cache miss)
        start_time = time.time()
        chunks_first_pass = []
        
        for _ in range(10):  # Charger 10 chunks
            chunk = loader_cached.get_next_chunk()
            if chunk is None:
                break
            chunks_first_pass.append(len(chunk))
        
        first_pass_time = time.time() - start_time
        
        # Reset et deuxi√®me passage (cache hit)
        loader_cached.reset()
        start_time = time.time()
        chunks_second_pass = []
        
        for _ in range(10):  # Recharger les m√™mes 10 chunks
            chunk = loader_cached.get_next_chunk()
            if chunk is None:
                break
            chunks_second_pass.append(len(chunk))
        
        second_pass_time = time.time() - start_time
        
        # Statistiques du cache
        cache_stats = loader_cached.get_cache_statistics()
        
        print(f"üìä Premier passage (cache miss): {first_pass_time:.3f}s")
        print(f"üìä Deuxi√®me passage (cache hit): {second_pass_time:.3f}s")
        print(f"üöÄ Acc√©l√©ration: {first_pass_time / second_pass_time:.1f}x")
        print(f"üìà Cache hit rate: {cache_stats['hit_rate']:.1%}")
        print(f"üìä Cache size: {cache_stats['cache_size']}/{cache_stats['max_cache_size']}")
        
        return (first_pass_time / second_pass_time) > 2.0  # Au moins 2x plus rapide
        
    finally:
        shutil.rmtree(temp_dir)

def test_parallel_loading():
    """Test du chargement parall√®le."""
    print("\n‚ö° Test Chargement Parall√®le")
    print("=" * 50)
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        assets = ['BTC', 'ETH', 'SOL', 'ADA', 'XRP', 'DOT', 'LINK', 'UNI']
        timeframes = ['5m', '1h', '4h']
        
        create_test_parquet_data(temp_dir, assets, timeframes, 200)
        
        # Test avec chargement parall√®le
        config_parallel = {
            'feature_engineering': {'timeframes': timeframes},
            'data_sources': [{'assets': assets}],
            'chunk_size': 50,
            'enable_cache': False,  # D√©sactiver cache pour test pur
            'enable_parallel_loading': True,
            'max_workers': 4
        }
        
        loader_parallel = ComprehensiveDataLoader(config_parallel, str(temp_dir))
        loader_parallel.load_asset_paths()
        
        start_time = time.time()
        parallel_chunks = 0
        
        for _ in range(15):  # Charger 15 chunks
            chunk = loader_parallel.get_next_chunk()
            if chunk is None:
                break
            parallel_chunks += 1
        
        parallel_time = time.time() - start_time
        
        # Test avec chargement s√©quentiel
        config_sequential = config_parallel.copy()
        config_sequential['enable_parallel_loading'] = False
        
        loader_sequential = ComprehensiveDataLoader(config_sequential, str(temp_dir))
        loader_sequential.load_asset_paths()
        
        start_time = time.time()
        sequential_chunks = 0
        
        for _ in range(15):  # Charger 15 chunks
            chunk = loader_sequential.get_next_chunk()
            if chunk is None:
                break
            sequential_chunks += 1
        
        sequential_time = time.time() - start_time
        
        print(f"üìä Chargement parall√®le: {parallel_time:.3f}s ({parallel_chunks} chunks)")
        print(f"üìä Chargement s√©quentiel: {sequential_time:.3f}s ({sequential_chunks} chunks)")
        print(f"üöÄ Acc√©l√©ration parall√®le: {sequential_time / parallel_time:.1f}x")
        
        return sequential_time > parallel_time  # Parall√®le doit √™tre plus rapide
        
    finally:
        shutil.rmtree(temp_dir)

def test_memory_management():
    """Test de la gestion m√©moire."""
    print("\nüíæ Test Gestion M√©moire")
    print("=" * 50)
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Cr√©er beaucoup d'assets pour tester la gestion m√©moire
        assets = [f'ASSET_{i:02d}' for i in range(12)]  # 12 assets
        timeframes = ['5m', '1h', '4h']
        
        create_test_parquet_data(temp_dir, assets, timeframes, 400)
        
        config = {
            'feature_engineering': {'timeframes': timeframes},
            'data_sources': [{'assets': assets}],
            'chunk_size': 100,
            'enable_cache': True,
            'cache_size': 30,
            'memory_threshold_mb': 500,  # Seuil bas pour test
            'enable_parallel_loading': True,
            'max_workers': 4
        }
        
        loader = ComprehensiveDataLoader(config, str(temp_dir))
        loader.load_asset_paths()
        
        print(f"üìä Assets cr√©√©s: {len(assets)}")
        print(f"üìä M√©moire initiale: {loader.get_memory_usage_mb():.1f}MB")
        
        # Charger plusieurs chunks et surveiller la m√©moire
        chunks_loaded = 0
        memory_readings = []
        
        for _ in range(25):  # Charger beaucoup de chunks
            chunk = loader.get_next_chunk()
            if chunk is None:
                break
            
            chunks_loaded += 1
            memory_mb = loader.get_memory_usage_mb()
            memory_readings.append(memory_mb)
            
            # Optimiser le cache p√©riodiquement
            if chunks_loaded % 5 == 0:
                loader.optimize_cache_size()
        
        # Statistiques finales
        perf_stats = loader.get_performance_statistics()
        
        print(f"üìä Chunks charg√©s: {chunks_loaded}")
        print(f"üìä M√©moire finale: {memory_readings[-1]:.1f}MB")
        print(f"üìä M√©moire max: {max(memory_readings):.1f}MB")
        print(f"üìä Cache final: {perf_stats['cache_stats']['cache_size']}")
        print(f"üìä Hit rate: {perf_stats['cache_stats']['hit_rate']:.1%}")
        
        return chunks_loaded > 20 and max(memory_readings) < 1000  # Pas trop de m√©moire
        
    finally:
        shutil.rmtree(temp_dir)

def test_scalability_10_plus_assets():
    """Test de scalabilit√© avec 10+ assets."""
    print("\nüìà Test Scalabilit√© 10+ Assets")
    print("=" * 50)
    
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        # Cr√©er 15 assets pour tester la scalabilit√©
        assets = [
            'BTC', 'ETH', 'SOL', 'ADA', 'XRP', 'DOT', 'LINK', 'UNI', 
            'AVAX', 'MATIC', 'ATOM', 'NEAR', 'FTM', 'ALGO', 'ICP'
        ]
        timeframes = ['5m', '1h', '4h']
        
        create_test_parquet_data(temp_dir, assets, timeframes, 300)
        
        config = {
            'feature_engineering': {'timeframes': timeframes},
            'data_sources': [{'assets': assets}],
            'chunk_size': 75,
            'enable_cache': True,
            'cache_size': 50,
            'memory_threshold_mb': 2000,
            'enable_parallel_loading': True,
            'max_workers': 6,
            'preload_next_chunks': 3
        }
        
        loader = ComprehensiveDataLoader(config, str(temp_dir))
        
        # Mesurer le temps d'initialisation
        start_time = time.time()
        loader.load_asset_paths()
        init_time = time.time() - start_time
        
        print(f"üìä Assets: {len(assets)}")
        print(f"üìä Temps d'initialisation: {init_time:.3f}s")
        print(f"üìä Rows totales: {sum(loader.asset_total_rows.values())}")
        
        # Test de chargement avec pr√©chargement
        start_time = time.time()
        chunks_loaded = 0
        total_rows = 0
        
        # Pr√©charger quelques chunks
        loader.preload_chunks(5)
        
        # Charger des chunks de tous les assets
        for _ in range(30):  # Charger 30 chunks
            chunk = loader.get_next_chunk()
            if chunk is None:
                break
            
            chunks_loaded += 1
            total_rows += len(chunk)
            
            # Afficher progr√®s pour les premiers assets
            if chunks_loaded <= 5:
                print(f"  Chunk {chunks_loaded}: {len(chunk)} rows, "
                      f"Asset: {loader.current_asset}")
        
        total_time = time.time() - start_time
        
        # Statistiques finales
        perf_stats = loader.get_performance_statistics()
        
        print(f"üìä Chunks charg√©s: {chunks_loaded}")
        print(f"üìä Rows trait√©es: {total_rows}")
        print(f"üìä Temps total: {total_time:.3f}s")
        print(f"üìä Vitesse: {total_rows / total_time:.0f} rows/sec")
        print(f"üìä Cache hit rate: {perf_stats['cache_stats']['hit_rate']:.1%}")
        print(f"üìä Temps moyen/chunk: {perf_stats.get('avg_load_time', 0):.4f}s")
        
        return (chunks_loaded >= 25 and 
                total_rows > 1000 and 
                perf_stats['cache_stats']['hit_rate'] > 0.1)
        
    finally:
        shutil.rmtree(temp_dir)

def main():
    """Fonction principale de test."""
    print("üöÄ Test Complet DataLoader Optimis√© pour 10+ Paires")
    print("=" * 60)
    
    tests = [
        ("Fonctionnalit√© de base", test_basic_functionality),
        ("Performance cache", test_cache_performance),
        ("Chargement parall√®le", test_parallel_loading),
        ("Gestion m√©moire", test_memory_management),
        ("Scalabilit√© 10+ assets", test_scalability_10_plus_assets)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*60}")
            success = test_func()
            results.append((test_name, success))
            status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
            print(f"\n{status} - {test_name}")
        except Exception as e:
            print(f"\n‚ùå √âCHEC - {test_name}: {str(e)}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("üìã R√âSUM√â DES TESTS DATALOADER")
    print("=" * 60)
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
        print(f"  {test_name}: {status}")
    
    print(f"\nüéØ Score: {passed}/{total} tests r√©ussis")
    
    if passed == total:
        print("üéâ DataLoader optimis√© op√©rationnel pour 10+ paires !")
        return True
    else:
        print("‚ö†Ô∏è Certains tests ont √©chou√©.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)