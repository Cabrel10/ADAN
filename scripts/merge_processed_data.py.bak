#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script to merge processed data from multiple timeframes into a single
multi-timeframe dataset for each asset and perform train/val/test split.

Optimized version with better performance and file handling.
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import argparse
from typing import Dict, List, Optional, Any, Tuple, Set
import yaml
from datetime import datetime, timedelta
import gc
from tqdm import tqdm
import pyarrow.parquet as pq
import pyarrow as pa
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# Configure logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('merge_processed_data.log')
    ]
)
logger = logging.getLogger(__name__)

# Default configuration path
DEFAULT_CONFIG_PATH = Path(__file__).parent.parent / 'config' / 'data_config.yaml'

# Optimize pandas
pd.set_option('mode.chained_assignment', None)  # Disable SettingWithCopyWarning

class DataValidationError(Exception):
    """Exception raised for errors in the data validation."""
    pass

def load_config(config_path: Path) -> Dict[str, Any]:
    """
    Load and validate the configuration file with optimized defaults.
    
    Args:
        config_path: Path to the YAML configuration file
        
    Returns:
        Dictionary containing the configuration with optimized defaults
        
    Raises:
        ValueError: If configuration is invalid
        FileNotFoundError: If config file doesn't exist
        yaml.YAMLError: If YAML parsing fails
    """
    try:
        if not config_path.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")
            
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f) or {}
        
        # Set default values with performance optimizations
        config.setdefault('data_pipeline', {})
        config['data_pipeline'].setdefault('processed_data_dir', 'data/processed')
        config['data_pipeline'].setdefault('final_data_dir', 'data/final')
        
        # Set default timeframes if not specified
        config.setdefault('timeframes_for_observation', ['5m', '1h', '4h'])
        
        # Set default split configuration with date-based splitting
        split_config = config.get('splits', {})
        split_config.setdefault('train_end_date', '2023-01-01')
        split_config.setdefault('val_end_date', '2023-04-01')
        config['splits'] = split_config
        
        # Performance optimization settings
        perf_config = config.setdefault('performance', {})
        perf_config.setdefault('use_multiprocessing', True)
        perf_config.setdefault('max_workers', max(1, multiprocessing.cpu_count() - 1))
        perf_config.setdefault('chunksize', 10000)  # For processing in chunks
        perf_config.setdefault('memory_efficient', True)  # Trade CPU for memory
        
        # Data validation settings
        val_config = config.setdefault('validation', {})
        val_config.setdefault('min_rows', 100)  # Minimum rows required to process an asset
        val_config.setdefault('max_missing_pct', 0.3)  # Max % of missing values allowed
        
        # Logging configuration
        log_config = config.setdefault('logging', {})
        log_config.setdefault('level', 'INFO')
        log_config.setdefault('file', 'merge_processed_data.log')
        
        # Set logging level
        logger.setLevel(getattr(logging, log_config['level'].upper(), logging.INFO))
        
        # Add file handler if not already added
        if not any(isinstance(h, logging.FileHandler) for h in logger.handlers):
            file_handler = logging.FileHandler(log_config['file'])
            file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            logger.addHandler(file_handler)
        
        return config
        
    except yaml.YAMLError as e:
        logger.error(f"Error parsing YAML config: {e}", exc_info=True)
        raise
    except Exception as e:
        logger.error(f"Error loading config: {e}", exc_info=True)
        raise

def load_single_dataframe(file_path: Path, tf: str) -> Optional[pd.DataFrame]:
    """
    Load and preprocess a single parquet file with performance optimizations.
    
    Args:
        file_path: Path to the parquet file
        tf: Timeframe (e.g., '5m', '1h', '4h')
        
    Returns:
        Preprocessed DataFrame or None if loading fails
    """
    try:
        if not file_path.exists():
            logger.debug(f"File not found: {file_path}")
            return None
            
        # Read only necessary columns if possible
        columns_to_read = None
        try:
            # Try to read schema first to check available columns
            schema = pq.read_schema(file_path)
            available_columns = set(schema.names)
            
            # Define required columns
            required_cols = {'timestamp', 'open', 'high', 'low', 'close', 'volume'}
            
            # Only read columns that exist in the file
            columns_to_read = [col for col in required_cols if col in available_columns]
            if not columns_to_read:
                logger.warning(f"No required columns found in {file_path}")
                return None
                
        except Exception as e:
            logger.warning(f"Could not read schema for {file_path}, falling back to full load: {e}")
        
        # Read the data with optimizations
        try:
            # Use pyarrow backend for faster reading
            table = pq.read_table(
                file_path,
                columns=columns_to_read,
                use_threads=True,
                memory_map=True
            )
            df = table.to_pandas()
        except Exception as e:
            logger.error(f"Error reading {file_path} with pyarrow: {e}")
            # Fallback to pandas read_parquet if pyarrow fails
            try:
                df = pd.read_parquet(file_path, columns=columns_to_read)
            except Exception as e2:
                logger.error(f"Error reading {file_path} with pandas: {e2}")
                return None
        
        if df.empty:
            logger.warning(f"Empty DataFrame loaded from {file_path}")
            return None
            
        # Set timestamp as index if it exists
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
            df = df.set_index('timestamp')
        elif not isinstance(df.index, pd.DatetimeIndex):
            logger.warning(f"No timestamp column in {file_path}")
            return None
            
        # Add timeframe prefix to columns
        df = df.add_prefix(f"{tf}_")
        
        return df
            
    except Exception as e:
        logger.error(f"Error loading {file_path}: {e}", exc_info=True)
        return None

def merge_timeframes(asset_data: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:
    """
    Merge multiple timeframes for a single asset using optimized methods.
    
    Args:
        asset_data: Dictionary of {timeframe: DataFrame} pairs
        
    Returns:
        Merged DataFrame with all timeframes or None if merge fails
    """
    if not asset_data:
        return None
        
    try:
        # Sort timeframes from highest to lowest resolution (e.g., ['4h', '1h', '5m'])
        timeframes = sorted(
            asset_data.keys(),
            key=lambda x: pd.Timedelta(x).total_seconds() if x != '1m' else 60,
            reverse=True
        )
        
        # Start with the lowest resolution (most aggregated) data
        merged = asset_data[timeframes[0]].copy()
        
        # Merge with higher resolution data
        for tf in timeframes[1:]:
            df = asset_data[tf]
            
            # Forward fill higher resolution data to match timestamps
            merged = merged.join(df, how='left')
            
            # Forward fill missing values within each timeframe group
            for col in df.columns:
                if col in merged.columns:
                    merged[col] = merged[col].ffill()
        
        # Drop any remaining NA values that couldn't be filled
        merged = merged.dropna(how='all')
        
        # Ensure data is sorted by timestamp
        merged = merged.sort_index()
        
        return merged
        
    except Exception as e:
        logger.error(f"Error merging timeframes: {e}", exc_info=True)
        return None

def load_asset_data(asset: str, timeframes: List[str], data_dir: Path) -> Optional[pd.DataFrame]:
    """
    Load and merge data for a single asset across multiple timeframes.
    
    Args:
        asset: Asset symbol (e.g., 'BTC/USDT')
        timeframes: List of timeframes to load (e.g., ['5m', '1h', '4h'])
        data_dir: Base directory containing the processed data
        
    Returns:
        Merged DataFrame with all timeframes for the asset, or None if no data found
    """
    asset_name = asset.replace('/', '')
    asset_data = {}
    
    # Load data for each timeframe
    for tf in timeframes:
        file_path = data_dir / tf / f"{asset_name}.parquet"
        df = load_single_dataframe(file_path, tf)
        if df is not None and not df.empty:
            asset_data[tf] = df
    
    # If no data was loaded, return None
    if not asset_data:
        logger.warning(f"No valid data found for {asset}")
        return None
    
    # Merge the timeframes
    merged = merge_timeframes(asset_data)
    
    if merged is not None and not merged.empty:
        logger.info(f"Successfully merged {len(merged)} rows for {asset} across {len(asset_data)} timeframes")
    else:
        logger.warning(f"Failed to merge data for {asset}")
    
    return merged

def split_data(df: pd.DataFrame, config: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    """
    Split data into train, validation, and test sets based on dates.
    
    Args:
        df: Input DataFrame with datetime index
        config: Configuration dictionary with split dates
        
    Returns:
        Dictionary with 'train', 'val', 'test' DataFrames
    """
    if df is None or df.empty:
        logger.warning("Cannot split empty or None DataFrame")
        return {}
    
    # Ensure the index is datetime
    if not isinstance(df.index, pd.DatetimeIndex):
        logger.error("DataFrame index must be a DatetimeIndex for date-based splitting")
        return {}
    
    # Get split dates from config
    train_end = pd.to_datetime(config['splits']['train_end_date'])
    val_end = pd.to_datetime(config['splits']['val_end_date'])
    
    # Ensure dates are timezone-aware
    if df.index.tz is not None:
        train_end = train_end.tz_localize(df.index.tz)
        val_end = val_end.tz_localize(df.index.tz)
    
    # Split the data based on dates
    train_df = df[df.index < train_end]
    val_df = df[(df.index >= train_end) & (df.index < val_end)]
    test_df = df[df.index >= val_end]
    
    # Log split information
    logger.info(f"Date-based split for {len(df)} rows:")
    logger.info(f"- Train: {len(train_df)} rows ({train_df.index.min()} to {train_df.index.max()})")
    logger.info(f"- Val:   {len(val_df)} rows ({val_df.index.min() if not val_df.empty else 'N/A'} to {val_df.index.max() if not val_df.empty else 'N/A'})")
    logger.info(f"- Test:  {len(test_df)} rows ({test_df.index.min() if not test_df.empty else 'N/A'} to {test_df.index.max() if not test_df.empty else 'N/A'})")
    
    return {
        'train': train_df,
        'val': val_df,
        'test': test_df
    }

def save_datasets(asset: str, datasets: Dict[str, pd.DataFrame], output_dir: Path) -> bool:
    """
    Save train/val/test datasets to disk with optimizations.
    
    Args:
        asset: Asset symbol (e.g., 'BTC_USDT')
        datasets: Dictionary with 'train', 'val', 'test' DataFrames
        output_dir: Base output directory
        
    Returns:
        bool: True if all datasets were saved successfully, False otherwise
    """
    if not datasets:
        logger.warning(f"No datasets provided for {asset}")
        return False
        
    try:
        # Create asset directory
        asset_dir = output_dir / asset.replace('/', '_')
        asset_dir.mkdir(parents=True, exist_ok=True)
        
        # Track success/failure
        success = True
        
        # Save each split with optimizations
        for split_name, df in datasets.items():
            if df is None or df.empty:
                logger.warning(f"No data to save for {asset} {split_name} split")
                success = False
                continue
                
            try:
                file_path = asset_dir / f"{split_name}.parquet"
                
                # Use optimal compression and other parquet options
                df.to_parquet(
                    file_path,
                    engine='pyarrow',
                    compression='snappy',  # Good balance of speed and compression
                    index=True,
                    coerce_timestamps='ms',
                    allow_truncated_timestamps=True
                )
                
                # Verify the file was written correctly
                if not file_path.exists() or file_path.stat().st_size == 0:
                    logger.error(f"Failed to write {file_path}")
                    success = False
                else:
                    logger.info(f"Saved {split_name} data to {file_path} ({len(df):,} rows, {file_path.stat().st_size / (1024*1024):.2f} MB)")
                    
            except Exception as e:
                logger.error(f"Error saving {split_name} data for {asset}: {e}", exc_info=True)
                success = False
        
        return success
        
    except Exception as e:
        logger.error(f"Error in save_datasets for {asset}: {e}", exc_info=True)
        return False

def clean_data(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    Clean and preprocess the merged DataFrame.
    
    Args:
        df: Input DataFrame
        config: Configuration dictionary
        
    Returns:
        Cleaned DataFrame
    """
    if df is None or df.empty:
        return df
    
    # Make a copy to avoid SettingWithCopyWarning
    df = df.copy()
    
    # Remove duplicate indices
    df = df[~df.index.duplicated(keep='first')]
    
    # Sort by index
    df = df.sort_index()
    
    # Handle missing values based on configuration
    max_missing_pct = config.get('validation', {}).get('max_missing_pct', 0.3)
    
    # Calculate missing percentage for each column
    missing_pct = df.isnull().mean()
    
    # Drop columns with too many missing values
    cols_to_drop = missing_pct[missing_pct > max_missing_pct].index.tolist()
    if cols_to_drop:
        logger.warning(f"Dropping columns with >{max_missing_pct:.0%} missing values: {', '.join(cols_to_drop)}")
        df = df.drop(columns=cols_to_drop)
    
    # Forward fill missing values for each column within its timeframe group
    for col in df.columns:
        if df[col].isnull().any():
            # Only fill if the column has a timeframe prefix
            if '_' in col:
                df[col] = df[col].ffill()
    
    # Drop any remaining rows with missing values
    initial_rows = len(df)
    df = df.dropna()
    
    if len(df) < initial_rows:
        logger.warning(f"Dropped {initial_rows - len(df)} rows with missing values")
    
    return df

def process_asset(asset: str, config: Dict[str, Any]) -> Tuple[bool, str]:
    """
    Process a single asset by merging timeframes for each data split (train, val, test).
    
    Args:
        asset: Asset symbol (e.g., 'BTC')
        config: Configuration dictionary
        
    Returns:
        Tuple[bool, str]: (Success status, asset symbol)
    """
    start_time = pd.Timestamp.now()
    asset_slug = asset.replace('/', '_')
    
    processed_data_dir = Path(config['data_pipeline']['processed_data_dir'])
    output_dir = Path(config['data_pipeline']['final_data_dir'])
    timeframes = config.get('timeframes_for_observation', [])
    splits = ['train', 'val', 'test']

    logger.info(f"\n{'='*80}\nProcessing asset: {asset}\n{'-'*80}")

    try:
        for split in splits:
            logger.info(f"Processing '{split}' split for {asset}...")
        
        # Split the data
        datasets = split_data(merged_df, config)
        
        if not datasets or any(df is None or df.empty for df in datasets.values()):
            logger.warning(f"Incomplete datasets for {asset_name}")
            return False
            
        # Save the datasets
        success = save_datasets(asset_name, datasets, final_dir)
        
        # Log processing time
        duration = (pd.Timestamp.now() - start_time).total_seconds()
        status = "SUCCESS" if success else "FAILED"
        logger.info(f"{'-'*40}\n{asset_name} processing {status} in {duration:.2f} seconds\n{'='*80}")
        
        return success
        
    except Exception as e:
        duration = (pd.Timestamp.now() - start_time).total_seconds()
        logger.error(f"Error processing {asset_name} after {duration:.2f}s: {e}", exc_info=True)
        return False
        
        # For each higher timeframe, merge with forward-fill
        for tf in timeframes[1:]:
            tf_file = data_dir / tf / f"{asset}.parquet"
            if not tf_file.exists():
                logger.warning(f"Timeframe file not found: {tf_file}")
                continue
                
            logger.info(f"Merging with {tf} data from {tf_file}")
            
            # Load timeframe data
            tf_df = pd.read_parquet(tf_file)
            if 'timestamp' not in tf_df.columns:
                logger.error(f"Timestamp column not found in {tf_file}")
                continue
                
            # Prepare timeframe data
            tf_df['timestamp'] = pd.to_datetime(tf_df['timestamp'])
            tf_df = tf_df.sort_values('timestamp')
            
            # Rename columns with timeframe prefix
            for col in tf_df.columns:
                if col != 'timestamp':
                    tf_df = tf_df.rename(columns={col: f"{tf}_{col}"})
            
            # Merge with forward-fill
            df = pd.merge_asof(
                df.sort_values('timestamp'),
                tf_df.sort_values('timestamp'),
                on='timestamp',
                direction='forward'
            )
            
            # Calculate minutes_since_update for this timeframe
            if f'{tf}_minutes_since_update' in df.columns:
                period_minutes = int(tf.replace('h', '00').replace('m', ''))
                df[f'{tf}_minutes_since_update'] = (df.groupby(
                    (df['timestamp'] - df['timestamp'].iloc[0]) // 
                    pd.Timedelta(f'{period_minutes}min')
                ).cumcount() * 5) % period_minutes
        
        # Save merged data
        merged_file = output_dir / f"{asset}_merged.parquet"
        df.to_parquet(merged_file, index=False)
        logger.info(f"Saved merged data to {merged_file} with {len(df)} rows")
        
        # Create and save splits
        splits = config.get('splits', {
            'train_ratio': 0.7,
            'val_ratio': 0.15,
            'test_ratio': 0.15
        })
        
        datasets = split_data(df, splits)
        
        # Save datasets
        for split_name, split_df in datasets.items():
            if split_df is not None and not split_df.empty:
                split_file = output_dir / f"{asset}_{split_name}.parquet"
                split_df.to_parquet(split_file, index=False)
                logger.info(f"Saved {split_name} data to {split_file} with {len(split_df)} rows")
        
        return True
        
    except Exception as e:
        logger.error(f"Error processing {asset}: {e}", exc_info=True)
        return False

def process_asset_wrapper(args):
    """Wrapper function for parallel processing of assets."""
    asset, config = args
    try:
        return process_asset(asset, config), asset
    except Exception as e:
        logger.error(f"Error in process_asset_wrapper for {asset}: {e}", exc_info=True)
        return False, asset

def main():
    """
    Main function to run the data merging pipeline with parallel processing.
    
    Returns:
        int: Exit code (0 for success, 1 for errors)
    """
    start_time = pd.Timestamp.now()
    
    try:
        # Parse command line arguments
        parser = argparse.ArgumentParser(description='Merge processed data from multiple timeframes')
        parser.add_argument('--config', type=str, default=str(DEFAULT_CONFIG_PATH),
                          help='Path to configuration file')
        parser.add_argument('--assets', type=str, nargs='+',
                          help='Specific assets to process (default: all in config)')
        parser.add_argument('--force', action='store_true',
                          help='Force reprocessing even if output files exist')
        parser.add_argument('--max-workers', type=int, default=None,
                          help='Maximum number of worker processes (default: CPU count - 1)')
        parser.add_argument('--chunksize', type=int, default=1,
                          help='Number of assets to process in each chunk (default: 1)')
        args = parser.parse_args()
        
        # Load configuration
        config = load_config(Path(args.config))
        
        # Override config with command line arguments
        if args.assets:
            config['assets'] = args.assets
        if args.force:
            config['force_reprocess'] = True
        if args.max_workers is not None:
            config['performance']['max_workers'] = args.max_workers
        if args.chunksize is not None:
            config['performance']['chunksize'] = args.chunksize
        
        # Get list of assets to process
        assets = config.get('assets', [])
        if not assets:
            logger.error("No assets specified in configuration")
            return 1
            
        logger.info(f"Starting data merging for {len(assets)} assets")
        logger.info(f"Using {config['performance']['max_workers']} worker processes")
        logger.info(f"Chunk size: {config['performance']['chunksize']}")
        
        # Prepare arguments for parallel processing
        process_args = [(asset, config) for asset in assets]
        
        # Process assets in parallel
        success_count = 0
        if config['performance']['use_multiprocessing'] and len(assets) > 1:
            with ProcessPoolExecutor(max_workers=config['performance']['max_workers']) as executor:
                # Use tqdm for progress bar
                results = list(tqdm(
                    executor.map(process_asset_wrapper, process_args, 
                               chunksize=config['performance']['chunksize']),
                    total=len(assets),
                    desc="Processing assets",
                    unit="asset"
                ))
                
                # Count successful results
                for success, asset in results:
                    if success:
                        success_count += 1
                    else:
                        logger.warning(f"Failed to process asset: {asset}")
        else:
            # Sequential processing (for debugging or small datasets)
            for asset in tqdm(assets, desc="Processing assets", unit="asset"):
                success, _ = process_asset_wrapper((asset, config))
                if success:
                    success_count += 1
                else:
                    logger.warning(f"Failed to process asset: {asset}")
        
        # Calculate and log summary
        duration = (pd.Timestamp.now() - start_time).total_seconds()
        success_rate = (success_count / len(assets)) * 100 if assets else 0
        
        logger.info(f"\n{'='*80}")
        logger.info(f"PROCESSING SUMMARY")
        logger.info(f"{'='*80}")
        logger.info(f"Total assets:           {len(assets)}")
        logger.info(f"Successfully processed: {success_count}")
        logger.info(f"Failed:                {len(assets) - success_count}")
        logger.info(f"Success rate:          {success_rate:.1f}%")
        logger.info(f"Total duration:        {duration:.2f} seconds")
        logger.info(f"Output directory:      {config['data_pipeline']['final_data_dir']}")
        logger.info(f"{'='*80}")
        
        # Return non-zero exit code if any assets failed to process
        return 0 if success_count == len(assets) else 1
        
    except Exception as e:
        duration = (pd.Timestamp.now() - start_time).total_seconds()
        logger.error(f"Fatal error after {duration:.2f} seconds: {e}", exc_info=True)
        return 1

if __name__ == "__main__":
    sys.exit(main())