#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Test Op√©rationnel Final ADAN
Validation compl√®te du syst√®me de trading automatis√© pour production.
"""

import os
import sys
import yaml
import pandas as pd
import numpy as np
from datetime import datetime
import traceback
import subprocess
import time
from pathlib import Path

# Ajouter le r√©pertoire parent au path
from adan_trading_bot.common.utils import load_config
from adan_trading_bot.environment.multi_asset_env import MultiAssetEnv
from stable_baselines3 import PPO

class OperationalTester:
    def __init__(self):
        self.results = {}
        self.critical_errors = []
        self.warnings = []
        self.start_time = datetime.now()

    def log_test(self, name, success, details="", critical=False):
        """Enregistre un r√©sultat de test."""
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        self.results[name] = {"success": success, "details": details, "critical": critical}

        if not success and critical:
            self.critical_errors.append(name)
            status = "üö® CRITICAL FAIL"

        print(f"{status}: {name}")
        if details:
            print(f"    {details}")

        return success

    def test_system_requirements(self):
        """Test des pr√©requis syst√®me."""
        print("\nüîß TESTS DES PR√âREQUIS SYST√àME")
        print("=" * 60)

        # Test Python version
        python_version = sys.version_info
        if python_version.major == 3 and python_version.minor >= 8:
            self.log_test("Python Version", True, f"Python {python_version.major}.{python_version.minor}.{python_version.micro}")
        else:
            self.log_test("Python Version", False, f"Python {python_version.major}.{python_version.minor} (requis: 3.8+)", critical=True)

        # Test packages critiques
        critical_packages = ['pandas', 'numpy', 'yaml', 'stable_baselines3']
        for package in critical_packages:
            try:
                __import__(package)
                self.log_test(f"Package {package}", True)
            except ImportError:
                self.log_test(f"Package {package}", False, "Module manquant", critical=True)

        # Test structure des r√©pertoires
        critical_dirs = ['config', 'data/processed/merged/unified', 'models', 'src']
        for dir_path in critical_dirs:
            if os.path.exists(dir_path):
                self.log_test(f"R√©pertoire {dir_path}", True)
            else:
                self.log_test(f"R√©pertoire {dir_path}", False, "R√©pertoire manquant", critical=True)

    def test_configuration_integrity(self):
        """Test de l'int√©grit√© des configurations."""
        print("\n‚öôÔ∏è TESTS DE CONFIGURATION")
        print("=" * 60)

        configs = [
            ('config/main_config.yaml', False),
            ('config/data_config_cpu.yaml', True),
            ('config/environment_config.yaml', True),
            ('config/agent_config_cpu.yaml', False)
        ]

        for config_path, is_critical in configs:
            try:
                config = load_config(config_path)

                # Validations sp√©cifiques
                if 'data_config_cpu.yaml' in config_path:
                    assets = config.get('assets', [])
                    features = config.get('base_market_features', [])
                    expected_features = len(features) * len(assets)

                    self.log_test(f"Config {config_path}", True,
                                f"{len(assets)} assets, {len(features)} features, {expected_features} total", is_critical)

                    if expected_features != 235:
                        self.log_test("Feature Count", False, f"Expected 235, got {expected_features}", is_critical)
                    else:
                        self.log_test("Feature Count", True, "235 features (compatible avec mod√®les)")

                elif 'environment_config.yaml' in config_path:
                    order_rules = config.get('order_rules', {})
                    min_abs = order_rules.get('min_value_absolute', 0)
                    min_tol = order_rules.get('min_value_tolerable', 0)

                    self.log_test(f"Config {config_path}", True, f"Min order: ${min_abs}")

                    if min_abs == min_tol == 10.0:
                        self.log_test("Order Thresholds", True, "Coh√©rents √† 10.0$")
                    else:
                        self.log_test("Order Thresholds", False, f"Incoh√©rents: {min_abs} vs {min_tol}")

                else:
                    self.log_test(f"Config {config_path}", True, f"{len(config)} sections")

            except Exception as e:
                self.log_test(f"Config {config_path}", False, f"Erreur: {str(e)}", is_critical)

    def test_data_availability(self):
        """Test de disponibilit√© des donn√©es."""
        print("\nüìä TESTS DES DONN√âES")
        print("=" * 60)

        data_files = [
            ('data/processed/merged/unified/1m_train_merged.parquet', True),
            ('data/processed/merged/unified/1m_val_merged.parquet', False),
            ('data/processed/merged/unified/1m_test_merged.parquet', True)
        ]

        for file_path, is_critical in data_files:
            if os.path.exists(file_path):
                try:
                    df = pd.read_parquet(file_path)
                    self.log_test(f"Data {os.path.basename(file_path)}", True,
                                f"Shape: {df.shape}", is_critical)

                    # Test colonnes critiques pour fichier de test
                    if 'test' in file_path:
                        required_cols = ['open_ADAUSDT', 'close_ADAUSDT', 'SMA_short_1m_ADAUSDT']
                        missing = [col for col in required_cols if col not in df.columns]
                        if missing:
                            self.log_test("Test Data Columns", False, f"Manquantes: {missing[:3]}...", True)
                        else:
                            self.log_test("Test Data Columns", True, "Colonnes essentielles pr√©sentes")

                except Exception as e:
                    self.log_test(f"Data {os.path.basename(file_path)}", False, f"Lecture √©chou√©e: {str(e)}", is_critical)
            else:
                self.log_test(f"Data {os.path.basename(file_path)}", False, "Fichier manquant", is_critical)

    def test_model_functionality(self):
        """Test de fonctionnalit√© des mod√®les."""
        print("\nü§ñ TESTS DES MOD√àLES")
        print("=" * 60)

        model_path = 'models/final_model.zip'

        if os.path.exists(model_path):
            try:
                model = PPO.load(model_path)
                self.log_test("Model Loading", True, f"Mod√®le charg√©: {os.path.basename(model_path)}")

                # Test de pr√©diction avec donn√©es synth√©tiques
                test_obs = {
                    'image_features': np.random.randn(1, 1, 20, 235).astype(np.float32),
                    'vector_features': np.random.randn(1, 6).astype(np.float32)
                }

                try:
                    action, _ = model.predict(test_obs, deterministic=True)
                    self.log_test("Model Prediction", True, f"Action pr√©dite: {action[0]}")
                except Exception as e:
                    self.log_test("Model Prediction", False, f"Pr√©diction √©chou√©e: {str(e)}", True)

            except Exception as e:
                self.log_test("Model Loading", False, f"Chargement √©chou√©: {str(e)}", True)
        else:
            self.log_test("Model Loading", False, "Mod√®le principal manquant", True)

    def test_environment_functionality(self):
        """Test de fonctionnalit√© de l'environnement."""
        print("\nüåç TESTS DE L'ENVIRONNEMENT")
        print("=" * 60)

        try:
            # Charger configurations
            data_config = load_config('config/data_config_cpu.yaml')
            env_config = load_config('config/environment_config.yaml')

            config = {
                'data': data_config,
                'environment': env_config
            }

            # Charger vraies donn√©es de test
            test_file = "data/processed/merged/unified/1m_test_merged.parquet"
            if os.path.exists(test_file):
                df_test = pd.read_parquet(test_file)

                # Cr√©er environnement
                env = MultiAssetEnv(
                    df_received=df_test,
                    config=config,
                    scaler=None,
                    encoder=None,
                    max_episode_steps_override=100
                )

                self.log_test("Environment Creation", True, "Environnement cr√©√© avec donn√©es r√©elles")

                # Test reset
                obs = env.reset()
                if isinstance(obs, (tuple, list)):
                    obs_shape = obs[0].shape if hasattr(obs[0], 'shape') else "dict"
                elif isinstance(obs, dict):
                    obs_shape = f"dict with {len(obs)} keys"
                else:
                    obs_shape = obs.shape if hasattr(obs, 'shape') else str(type(obs))

                self.log_test("Environment Reset", True, f"Observation: {obs_shape}")

                # Test plusieurs steps
                successful_steps = 0
                for i in range(10):
                    try:
                        action = 0  # HOLD
                        obs, reward, done, info = env.step(action)
                        successful_steps += 1
                        if done:
                            break
                    except Exception as e:
                        break

                if successful_steps >= 5:
                    self.log_test("Environment Steps", True, f"{successful_steps} steps r√©ussis")
                else:
                    self.log_test("Environment Steps", False, f"Seulement {successful_steps} steps r√©ussis")

            else:
                self.log_test("Environment Creation", False, "Donn√©es de test manquantes", True)

        except Exception as e:
            self.log_test("Environment Creation", False, f"Erreur: {str(e)}", True)

    def test_trading_logic(self):
        """Test de la logique de trading."""
        print("\nüí∞ TESTS DE LOGIQUE DE TRADING")
        print("=" * 60)

        try:
            env_config = load_config('config/environment_config.yaml')

            # Test des seuils d'ordre
            order_rules = env_config.get('order_rules', {})
            min_abs = order_rules.get('min_value_absolute', 0)
            min_tol = order_rules.get('min_value_tolerable', 0)

            self.log_test("Order Thresholds", True, f"Absolu: ${min_abs}, Tol√©rable: ${min_tol}")

            # Test des paliers de capital
            tiers = env_config.get('tiers', [])
            if tiers:
                test_capitals = [15.0, 100.0, 1000.0, 15000.0]
                for capital in test_capitals:
                    applicable_tier = None
                    for tier in reversed(tiers):  # Du plus √©lev√© au plus bas
                        if capital >= tier.get('threshold', 0):
                            applicable_tier = tier
                            break

                    if applicable_tier:
                        max_pos = applicable_tier.get('max_positions', 1)
                        alloc = applicable_tier.get('allocation_frac_per_pos', 0.2)
                        self.log_test(f"Capital ${capital}", True, f"{max_pos} pos max, {alloc*100}% alloc")
                    else:
                        self.log_test(f"Capital ${capital}", False, "Aucun palier applicable")
            else:
                self.log_test("Capital Tiers", False, "Aucun palier d√©fini")

        except Exception as e:
            self.log_test("Trading Logic", False, f"Erreur: {str(e)}")

    def test_script_execution(self):
        """Test d'ex√©cution des scripts critiques."""
        print("\nüìú TESTS D'EX√âCUTION")
        print("=" * 60)

        # Test script de statut (critique pour monitoring)
        try:
            result = subprocess.run(['python', 'status_adan.py'],
                                  capture_output=True, timeout=30, text=True)
            if result.returncode == 0:
                self.log_test("Status Script", True, "Statut g√©n√©r√© avec succ√®s")
            else:
                self.log_test("Status Script", False, f"Code retour: {result.returncode}", True)
        except subprocess.TimeoutExpired:
            self.log_test("Status Script", False, "Timeout", True)
        except Exception as e:
            self.log_test("Status Script", False, f"Erreur: {str(e)}", True)

        # Test script d'√©valuation rapide
        try:
            result = subprocess.run(['python', 'scripts/quick_eval.py', '--help'],
                                  capture_output=True, timeout=10, text=True)
            if "usage:" in result.stdout or result.returncode == 0:
                self.log_test("Eval Script", True, "Script d'√©valuation accessible")
            else:
                self.log_test("Eval Script", False, "Script d'√©valuation inaccessible")
        except Exception as e:
            self.log_test("Eval Script", False, f"Erreur: {str(e)}")

    def test_exchange_readiness(self):
        """Test de pr√©paration pour exchange."""
        print("\nüîó TESTS DE PR√âPARATION EXCHANGE")
        print("=" * 60)

        try:
            import ccxt
            self.log_test("CCXT Library", True, "Biblioth√®que d'exchange install√©e")

            # Test cr√©ation exchange (mode sandbox)
            try:
                exchange = ccxt.binance({
                    'sandbox': True,
                    'enableRateLimit': True,
                })
                self.log_test("Exchange Setup", True, "Configuration exchange r√©ussie")

                # Test de connectivit√© de base (sans cl√©s)
                try:
                    markets = exchange.load_markets()
                    self.log_test("Exchange Connectivity", True, f"{len(markets)} march√©s disponibles")
                except ccxt.AuthenticationError:
                    self.log_test("Exchange Connectivity", True, "Connectivit√© OK (cl√©s API requises)")
                except Exception as e:
                    if "api" in str(e).lower() or "auth" in str(e).lower():
                        self.log_test("Exchange Connectivity", True, "Connectivit√© OK")
                    else:
                        self.log_test("Exchange Connectivity", False, f"Probl√®me r√©seau: {str(e)}")

            except Exception as e:
                self.log_test("Exchange Setup", False, f"Configuration √©chou√©e: {str(e)}")

        except ImportError:
            self.log_test("CCXT Library", False, "Biblioth√®que manquante", True)

    def test_performance_validation(self):
        """Test de validation des performances."""
        print("\nüöÄ TESTS DE PERFORMANCE")
        print("=" * 60)

        # Test vitesse de chargement du mod√®le
        model_path = 'models/final_model.zip'
        if os.path.exists(model_path):
            start_time = time.time()
            try:
                model = PPO.load(model_path)
                load_time = time.time() - start_time

                if load_time < 10:
                    self.log_test("Model Load Speed", True, f"{load_time:.2f}s (< 10s)")
                else:
                    self.log_test("Model Load Speed", False, f"{load_time:.2f}s (> 10s)")

                # Test vitesse de pr√©diction
                test_obs = {
                    'image_features': np.random.randn(1, 1, 20, 235).astype(np.float32),
                    'vector_features': np.random.randn(1, 6).astype(np.float32)
                }

                prediction_times = []
                for _ in range(10):
                    start = time.time()
                    action, _ = model.predict(test_obs, deterministic=True)
                    prediction_times.append(time.time() - start)

                avg_pred_time = np.mean(prediction_times) * 1000  # en ms
                if avg_pred_time < 100:  # < 100ms
                    self.log_test("Prediction Speed", True, f"{avg_pred_time:.1f}ms/prediction")
                else:
                    self.log_test("Prediction Speed", False, f"{avg_pred_time:.1f}ms/prediction (> 100ms)")

            except Exception as e:
                self.log_test("Model Load Speed", False, f"Erreur: {str(e)}")

    def run_operational_tests(self):
        """Ex√©cute tous les tests op√©rationnels."""
        print("üéØ TESTS OP√âRATIONNELS FINAUX - SYST√àME ADAN")
        print("=" * 80)
        print(f"D√©marr√© le: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Environnement: {os.getcwd()}")

        # Ex√©cution s√©quentielle des tests
        test_functions = [
            self.test_system_requirements,
            self.test_configuration_integrity,
            self.test_data_availability,
            self.test_model_functionality,
            self.test_environment_functionality,
            self.test_trading_logic,
            self.test_script_execution,
            self.test_exchange_readiness,
            self.test_performance_validation
        ]

        for test_func in test_functions:
            try:
                test_func()
            except Exception as e:
                test_name = test_func.__name__.replace('test_', '').replace('_', ' ').title()
                self.log_test(f"Test {test_name}", False, f"Exception: {str(e)}", True)

        self.generate_final_report()

    def generate_final_report(self):
        """G√©n√®re le rapport final."""
        print("\n" + "=" * 80)
        print("üìä RAPPORT OP√âRATIONNEL FINAL")
        print("=" * 80)

        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results.values() if r['success'])
        failed_tests = total_tests - passed_tests
        critical_failed = len(self.critical_errors)

        # Statistiques
        print(f"üîç Tests Ex√©cut√©s: {total_tests}")
        print(f"‚úÖ Tests R√©ussis: {passed_tests}")
        print(f"‚ùå Tests √âchou√©s: {failed_tests}")
        print(f"üö® Erreurs Critiques: {critical_failed}")

        # Score de r√©ussite
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        print(f"üìà Taux de R√©ussite: {success_rate:.1f}%")

        # Temps d'ex√©cution
        duration = datetime.now() - self.start_time
        print(f"‚è±Ô∏è Dur√©e d'Ex√©cution: {duration.total_seconds():.1f}s")

        # Verdict final
        print("\n" + "=" * 80)
        if critical_failed == 0 and success_rate >= 90:
            print("üéâ SYST√àME OP√âRATIONNEL - PR√äT POUR PRODUCTION")
            print("‚ú® Tous les tests critiques sont pass√©s")
            print("üöÄ Le syst√®me peut √™tre d√©ploy√© en toute s√©curit√©")

            print("\nüí° PROCHAINES √âTAPES:")
            print("   1. Configurer les cl√©s API pour trading testnet/live")
            print("   2. Effectuer des tests de trading avec petit capital")
            print("   3. Monitorer les performances en temps r√©el")
            print("   4. Mettre en place l'alerting automatique")

        elif critical_failed == 0:
            print("‚ö†Ô∏è SYST√àME FONCTIONNEL - CORRECTIONS MINEURES REQUISES")
            print("‚úÖ Aucune erreur critique d√©tect√©e")
            print("üîß Quelques ajustements recommand√©s avant production")

        else:
            print("üö® SYST√àME NON OP√âRATIONNEL - CORRECTIONS CRITIQUES REQUISES")
            print(f"‚ùå {critical_failed} erreur(s) critique(s) d√©tect√©e(s)")
            print("üõ†Ô∏è Corrections obligatoires avant mise en production")

            print("\nüö® ERREURS CRITIQUES √Ä CORRIGER:")
            for error in self.critical_errors:
                details = self.results[error].get('details', '')
                print(f"   - {error}: {details}")

        print("\n" + "=" * 80)
        print("üìã R√âSUM√â D√âTAILL√â:")
        for test_name, result in self.results.items():
            status = "‚úÖ" if result['success'] else ("üö®" if result['critical'] else "‚ùå")
            print(f"   {status} {test_name}: {result['details']}")

        return critical_failed == 0 and success_rate >= 90

if __name__ == "__main__":
    tester = OperationalTester()
    success = tester.run_operational_tests()
    sys.exit(0 if success else 1)
