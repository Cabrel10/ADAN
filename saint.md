Notre agent de trading est **un mod√®le d‚Äôapprentissage par renforcement** (Reinforcement Learning) :

1. **Type d‚Äôapprentissage**

   * **Reinforcement Learning** : l‚Äôagent apprend en interagissant directement avec l‚Äôenvironnement de march√© simul√©. √Ä chaque pas :

     * Il observe un √©tat (features de march√© + √©tat du portefeuille),
     * Choisit une action (BUY, SELL, HOLD ou cr√©ation d‚Äôordres avanc√©s),
     * Re√ßoit une r√©compense (reward shaping bas√© sur l‚Äô√©volution du portefeuille, p√©nalit√©s, bonus, time-penalty),
     * Met √† jour sa politique pour maximiser la somme cumul√©e des r√©compenses futures.

2. **Pourquoi pas supervision ou non-supervision ?**

   * **Pas de supervision** : il n‚Äôy a pas de ¬´ vraie ¬ª v√©rit√© ou labels ; on n‚Äôa pas d‚Äôexemples ‚Äúacheter ici c‚Äôest bon‚Äù valid√©s par des experts.
   * **Pas d‚Äôauto-encodage / clustering** : on n‚Äôexplore pas des structures latentes du march√© uniquement.
   * **RL** permet d‚Äôoptimiser directement la m√©trique qui compte (la performance financi√®re), en tenant compte de la s√©quence temporelle et des retours diff√©r√©s.

3. **Algorithme courant**

   * Dans notre code d‚Äôentra√Ænement, nous utilisons g√©n√©ralement **PPO (Proximal Policy Optimization)** de Stable-Baselines3, un algorithme d‚Äôacteur-critique moderne, stable et efficace pour les environnements continus et discrets.

---

### 1. Actions (espace discret)

Notre agent dispose de 11 actions :

| Action Code | Type           | Description                                            |
| ----------- | -------------- | ------------------------------------------------------ |
| 0           | HOLD           | Ne rien faire                                          |
| 1‚Äì5         | BUY\_ASSET\_i  | Passer un ordre d‚Äôachat MARKET sur l‚Äôactif *asset\_i*  |
| 6‚Äì10        | SELL\_ASSET\_j | Passer un ordre de vente MARKET sur l‚Äôactif *asset\_j* |

En plus des MARKET orders, le code supporte 5 types d‚Äôordres avanc√©s (LIMIT, STOP\_LOSS, TAKE\_PROFIT, TRAILING\_STOP) :

* **CREATE\_LIMIT** : place un LIMIT buy ou sell √† un prix fix√©
* **CREATE\_STOP\_LOSS** : place un STOP\_LOSS
* **CREATE\_TAKE\_PROFIT** : place un TAKE\_PROFIT
* **CREATE\_TRAILING\_STOP** : place un TRAILING\_STOP
* **EXECUTED\_...** : lorsque ces ordres deviennent ex√©cut√©s

---

### 2. Fonctions cl√©s

* **`_execute_order(asset_id, action_type, ‚Ä¶)`**

  * V√©rifie **minimum order value** (`self.min_order_value`), **capital disponible**, **nombre max de positions** (palier), puis :

    * Pour MARKET : applique achat/vente, frais (`_calculate_fee`), mise √† jour de `self.capital` et `self.positions`.
    * Pour ordres avanc√©s : stocke dans `self.orders` avec `expiry`, `limit_price`, etc.

* **`_process_pending_orders()`**

  * √Ä chaque step, parcourt `self.orders` et :

    * Expire les ordres au-del√† de `expiry`, applique p√©nalit√©s selon type.
    * V√©rifie conditions d‚Äôex√©cution (prix atteint limit/stop/tp/trailing), ex√©cute via `_execute_order(‚Ä¶, order_type="EXECUTED_‚Ä¶")`, accumule `total_reward_mod`.

* **`_calculate_fee(amount)`**

  * `return amount * transaction_cost_pct + fixed_fee`

* **`_calculate_reward(old_value, new_value, penalties)`**

  * **Log-return** :  $\ln(new/old)$
  * **Shaping** selon palier : multipli√© par `reward_pos_mult` ou `reward_neg_mult`
  * Soustraction de `penalties` + `time_penalty` (p√©nalit√© fixe √† chaque step)
  * Clipping final dans \[‚àí10, +10]

* **`_display_trading_table(‚Ä¶)`**

  * Affiche chaque step en console, color√© via **rich**, avec √©tat avant/apr√®s, positions, PnL, bonus, p√©nalit√©s, ordres pendants, etc.

---

### 3. √âtats & Features

√Ä chaque step, l‚Äôobservation est un vecteur de dimension **N\_market\_features + N\_portfolio\_features** :

1. **Market features** (colonnes num√©riques extraites du dataset)

   * Ex : open, high, low, close, volume, indicateurs techniques (MA, RSI, etc.)
2. **Portfolio features** (6 dims)

   * Capital normalis√© (0‚Äì1)
   * Quantit√©s normalis√©es pour chaque actif (5 dim)

Si un **encodeur** est activ√©, on applique d‚Äôabord un scaler + un auto-encodeur/Keras pour r√©duire ou transformer les features march√©.

---

### 4. R√©compenses (Reward)

* **Reward principal** = $\ln(\frac{\text{new_portfolio}}{\text{old_portfolio}})$
* **Shaping** : positive √ó `reward_pos_mult`, n√©gative √ó `reward_neg_mult` (selon palier)
* **P√©nalit√©s** :

  * Invalidations d‚Äôordres (montant trop petit, pas assez de capital, max positions)
  * Expiration d‚Äôordres LIMIT/STOP\_LOSS (‚àí0.1/‚àí0.05)
  * **Time penalty** fixe (‚àí0.001)
* **Bonus** : 1 % du PnL positif plafonn√© √† 1.0

---

### 5. Donn√©es & Format

* **Source** : fichier Parquet (ou CSV) contenant N\_steps√óN\_assets lignes, colonnes march√© + timestamp + paire
* **Chargement** via `pd.read_parquet` ou `pd.read_csv`
* **Pr√©-calcul** de `self.numeric_cols = select_dtypes(include=[np.number])`
* **V√©rification** de coh√©rence entre nombre de features et l‚Äôattendu du scaler (erreur si mismatch)

---

### 6. R√®gles & Contraintes m√©tier

1. **Montant minimum par ordre** (`min_order_value`, ex. 10\$)
2. **Frais fixes + variable** (`fixed_fee` + `transaction_cost_pct`)
3. **Paliers de capital** (`self.tiers`)

   * D√©finissent **max\_positions**, **allocation\_frac**, **multiplicateurs**
   * L‚Äôallocation minimale est `min_order_value` si l‚Äôallocation palier est trop faible
4. **Nombre maximum de positions** (actives + ordres pendants) selon palier
5. **Pas d‚Äôeffet de levier** : quantit√© maximale = capital √ó allocation\_frac / prix
6. **Pas de fractionalisation extr√™me** : quantit√© ajust√©e pour respecter min\_order\_value
7. **Gestion des ordres avanc√©s** : expiration automatique + p√©nalit√©
8. **Stop trainings** si capital ‚â§ 9 & positions vides (faillite)

---

> ‚úî Avec cette check-list, tu peux d√©sormais v√©rifier que chaque composant du syst√®me respecte les **contraintes minimales** avant de passer √† l‚Äô√©tape suivante.

### 2. Fonctions cl√©s (avec niveaux de montant minimum)

Pour int√©grer un **minimum tol√©rable √† 10 \$** et un **minimum absolu √† 9 \$**, nous allons introduire deux param√®tres et ajuster les v√©rifications dans la couche d‚Äôex√©cution d‚Äôordres :

```python
# Dans __init__ :
self.min_order_value_tolerable = 10.0   # Montant minimal conseill√©
self.min_order_value_absolute  = 9.0    # Seuil en dessous duquel on refuse tout net

# ...  

def _execute_order(self, asset_id, action_type, quantity=None, order_type="MARKET", **kwargs):
    current_price = self._get_asset_price(asset_id)
    
    # Calcul pr√©liminaire de la valeur d‚Äôordre
    if quantity is None:
        quantity = self._get_position_size(asset_id)
    order_value = quantity * current_price

    # 1) Seuil absolu : refus imm√©diat
    if order_value < self.min_order_value_absolute:
        return -0.5, "INVALID_ORDER_TOO_SMALL", {
            "reason": f"Valeur {order_value:.2f} < seuil absolu {self.min_order_value_absolute}"
        }

    # 2) Seuil tol√©rable : ajustement ou avertissement
    if order_value < self.min_order_value_tolerable:
        # Si on a assez de capital, on ajuste la quantit√© pour atteindre 10$
        if self.capital >= self.min_order_value_tolerable:
            quantity = self.min_order_value_tolerable / current_price
            order_value = self.min_order_value_tolerable
        else:
            # Sinon, on consid√®re la transaction impossible, mais avec une p√©nalit√© plus l√©g√®re
            return -0.2, "INVALID_ORDER_BELOW_TOLERABLE", {
                "reason": f"Valeur {order_value:.2f} < tol√©rable {self.min_order_value_tolerable}"
            }

    # 3) Frais et capital
    fee = self._calculate_fee(order_value)
    total_cost = order_value + fee
    if total_cost > self.capital:
        return -0.2, "INVALID_NO_CAPITAL", {
            "reason": f"Co√ªt total {total_cost:.2f} > capital {self.capital:.2f}"
        }

    # 4) V√©rifier max positions selon palier
    tier = self._get_current_tier()
    if action_type == 1:  # BUY
        if len(self.positions) >= tier["max_positions"] and asset_id not in self.positions:
            return -0.2, "INVALID_MAX_POSITIONS", {
                "reason": f"Max positions ({tier['max_positions']}) atteint"
            }
        # On passe l‚Äôachat selon MARKET ou ordres avanc√©s...
```

* **`min_order_value_absolute` (9 \$)** bloque d√©finitivement tout ordre en dessous de ce seuil (p√©nalit√© plus forte, `-0.5`).
* **`min_order_value_tolerable` (10 \$)** d√©clenche un ajustement automatique de la quantit√© pour atteindre exactement 10 \$ si le capital le permet, sinon une p√©nalit√© plus l√©g√®re (`-0.2`).

Le reste des √©tapes de `_execute_order` (gestion des ordres avanc√©s, mise √† jour de `self.capital` et `self.positions`, logging) reste inchang√©.

---

### ‚ûä R√©sum√© des paliers de montant

| Seuil                             | Comportement                                                            | P√©nalit√© |
| --------------------------------- | ----------------------------------------------------------------------- | -------- |
| < 9 \$ (absolu)                   | Rejet imm√©diat, ordre impossible                                        | ‚àí0.5     |
| 9 \$ ‚â§ valeur < 10 \$ (tol√©rable) | Tentative d‚Äôajustement √† 10 \$ (si capital suffisant) sinon rejet l√©ger | ‚àí0.2     |
| ‚â• 10 \$                           | Ex√©cution normale                                                       | 0        |

Avec ce sch√©ma, chaque commande m√©rite sa place et aucune transaction ‚Äútrop petite‚Äù ne passera sans que l‚Äôagent n‚Äôait l‚Äôopportunit√© d‚Äôajuster ou d‚Äô√™tre p√©nalis√©.
### 2) Nature du mod√®le

L‚Äôagent que nous d√©veloppons est **un mod√®le d‚Äôapprentissage par renforcement** (Reinforcement Learning, RL).

---

#### Pourquoi pas supervis√© ?

* **Pas de labels historiques** : en trading on n‚Äôa pas un ¬´ vrai ¬ª signal de sortie (un label ¬´ acheter/vendre/garder ¬ª) g√©n√©r√© par un expert pour chaque √©tat de march√©.
* **D√©cision s√©quentielle** : la qualit√© d‚Äôune action (BUY/SELL/HOLD) n‚Äôest v√©ritablement connue qu‚Äôapr√®s plusieurs pas de temps, au regard du PnL ult√©rieur.

#### Pourquoi pas non supervis√© ?

* **Non supervis√©** sert √† d√©couvrir des structures (clustering, r√©duction de dimension, d√©tection d‚Äôanomalies) mais ne d√©finit pas de politique d‚Äôaction optimale.
* Notre besoin est de **prendre des d√©cisions** et **optimiser un objectif cumulatif** (maximiser le rendement), ce qui rel√®ve sp√©cifiquement de l‚Äôapprentissage par renforcement.

---

### Sp√©cificit√©s du RL dans ce projet

1. **Agent**

   * **Type** : agent PPO (Proximal Policy Optimization) via Stable-Baselines3, un algorithme on-policy bien adapt√© aux environnements continus/discrets.
   * **Architecture** : r√©seau de neurones feed-forward (MLP) entra√Æn√© √† partir des observations d‚Äô√©tat.

2. **Environnement (MultiAssetEnv)**

   * **Observations** : vecteur constitu√© de

     * indicateurs march√© normalis√©s (prix, volumes, indicateurs techniques),
     * proportion de capital disponible,
     * quantit√©s normalis√©es de positions ouvertes pour chaque actif.
   * **Actions** : Discrete(11) ‚Üí {HOLD, BUY\_asset\_i, SELL\_asset\_i} pour 5 actifs.
   * **R√©compense** : reward shaping combinant

     * log-return de portefeuille entre deux steps,
     * multiplicateurs positifs/n√©gatifs selon palier (tiers),
     * p√©nalit√©s pour ordres invalides,
     * p√©nalit√© temporelle fixe pour encourager des d√©cisions actives.

3. **Boucle d‚Äôinteraction**

   * √Ä chaque step :

     1. Agent choisit une action d‚Äôapr√®s sa policy.
     2. Environnement ex√©cute l‚Äôaction (*execute\_order*), met √† jour capital/positions et traite les ordres en attente.
     3. Environnement calcule la r√©compense finale (`_calculate_reward`).
     4. Agent re√ßoit la nouvelle observation, la r√©compense et apprend via PPO.

4. **Gestion des contraintes**

   * **Minima de transaction** (9 \$ absolu, 10 \$ tol√©rable) pour √©viter le slippage et frais disproportionn√©s.
   * **Paliers de capital** (tiers) pour adapter taille de position, nombre max de positions, multiplicateurs de reward.
   * **Types d‚Äôordres avanc√©s** (LIMIT, STOP\_LOSS, TAKE\_PROFIT, TRAILING\_STOP) g√©r√©s en file d‚Äôattente et ex√©cut√©s selon conditions de march√©.

---

‚ùØ **Conclusion** :
Cet agent **apprend par essai/erreur** √† maximiser son capital, en s‚Äôappuyant sur un feedback diff√©r√© (r√©compense cumulative) plut√¥t que sur des exemples √©tiquet√©s. C‚Äôest le c≈ìur de l‚Äôapprentissage par renforcement appliqu√© au trading algorithmique.

### 3) D√©tails de l‚Äôalgorithme et de sa mise en ≈ìuvre

**Algorithme principal : Proximal Policy Optimization (PPO)**

* **Type** : M√©thode on-policy d‚Äôacteur-critique
* **Biblioth√®que** : Stable-Baselines3 (`PPO`)
* **Pourquoi PPO ?**

  * Bon compromis stabilit√©/efficacit√© : contr√¥le du pas de mise √† jour via la ‚Äúclipping‚Äù de l‚Äôimportance sampling.
  * Adapt√© √† la fois aux espaces d‚Äôaction discrets et continus.
  * Large adoption en trading RL et en robotique.

---

#### 3.1. Structures internes

| √âl√©ment               | R√¥le                                                                 |
| --------------------- | -------------------------------------------------------------------- |
| **R√©seau policy (œÄ)** | Produit la distribution de probabilit√© sur les 11 actions (softmax)  |
| **R√©seau value (V)**  | Estime la valeur attendue du portefeuille √† partir de l‚Äô√©tat courant |
| **Optimisation**      |                                                                      |

* **Clip range** : Œµ ‚àà \[0.1, 0.3]
* **Learning rate** : \~2.5e-4 (tunable)
* **Batch size** : par d√©faut 64‚Äì256 transitions
* **Epochs** : 3‚Äì10 par collecte de trajet
  \| **Exploration**        | Garantie via entropie > 0 (p.ex. coefficient ‚âà 0.01) pour √©viter la convergence trop rapide vers une politique d√©terministe. |

---

#### 3.2. √âtats (observations)

Un vecteur concat√©n√© :

1. **Indicateurs march√©** (ùëõ\_features num√©riques)

   * Prix (open, high, low, close)
   * Volume
   * Moyennes mobiles (MA\_10, MA\_50‚Ä¶)
   * Indicateurs techniques (RSI, MACD, Bollinger bands‚Ä¶)
2. **Portefeuille**

   * **Normalized capital** : ùëê/ùëê‚ÇÄ ‚àà \[0, ‚àû)
   * **Positions normalis√©es** : ùëû·µ¢/ùëû\_typique ‚àà \[0, ‚àû) pour chaque actif ùëñ

> **Dimension** totale = nombre d‚Äôindicateurs + 1 (capital) + 5 (positions).

---

#### 3.3. Actions

* **Discret(11)** :

  1. `0 = HOLD`
  2. `1‚Äì5 = BUY` de `asset_0‚Ä¶asset_4`
  3. `6‚Äì10 = SELL` de `asset_0‚Ä¶asset_4`

> Dans la version avanc√©e on peut passer `order_type` en argument pour LIMIT, STOP\_LOSS, etc.

---

#### 3.4. R√©compenses

La fonction `_calculate_reward`:

```python
log_return = log(new_portfolio_value / old_portfolio_value)
if log_return >= 0:
    shaped = log_return * tier["reward_pos_mult"]
else:
    shaped = log_return * tier["reward_neg_mult"]
shaped -= penalties           # p√©nalit√©s pour invalides
shaped -= time_penalty (0.001) 
return clip(shaped, -10, +10)
```

* **Paliers (tiers)** modulent :

  * `reward_pos_mult` (bonus de gain)
  * `reward_neg_mult` (amplification des pertes)

* **P√©nalit√©s** :

  * Ordre invalide ‚Üí |reward\_mod|,
  * Expiration d‚Äôun ordre LIMIT/STOP\_LOSS ‚Üí ‚àí0.1, etc.

---

#### 3.5. Donn√©es utilis√©es

* **Source** : DataFrame Parquet/CSV
* **Lignes** : chaque tick/minute/horaire, selon timeframe
* **Colonnes** :

  * `timestamp` (int ou datetime)
  * `pair` (asset\_i)
  * indicateurs bruts + techniques
* **Pr√©-processing** :

  * Normalisation/simple scaling pour r√©seau
  * Optionnel : auto-encodeur Keras + scaler joblib

---

#### 3.6. Importance & contenu

| Donn√©e                       | Importance                               |
| ---------------------------- | ---------------------------------------- |
| Prix (OHLC)                  | Base de toute d√©cision de buy/sell       |
| Volume                       | Contexte de liquidit√©                    |
| Indicateurs (RSI, MA, MACD‚Ä¶) | Rep√©rage de surachat/survente, tendances |
| Capital normalis√©            | Pour dimensionner taille de position     |
| Positions normalis√©es        | Pour √©viter overexposition               |

---

> **En r√©sum√©**, PPO s‚Äôappuie sur ces **√©tats** pour choisir parmi 11 **actions**, et re√ßoit des **r√©compenses** taill√©es pour refl√©ter performances financi√®res + gestion des risques et contraintes (min/max, paliers, p√©nalit√©s).
### 4) Fonctions cl√©s de l‚Äôenvironnement de trading

Voici un tour d‚Äôhorizon des principales fonctions (m√©thodes) de **`MultiAssetEnv`**, avec leur r√¥le et comment elles s‚Äôarticulent :

| Fonction                                                                                 | R√¥le principal                                                                                                                            |
| ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **`__init__`**                                                                           | - Initialise tous les param√®tres : capital initial, frais, paliers, espace d‚Äôaction/observation, chargement des donn√©es et de l‚Äôencodeur. |
| **`reset(self, *, seed=None, options=None)`**                                            |                                                                                                                                           |
|                                                                                          | - R√©initialise l‚Äôenvironnement au d√©but d‚Äôun √©pisode : capital, positions, historique, step √† 0.                                          |
|                                                                                          | - Renvoie l‚Äôobservation initiale et l‚Äô`info` de d√©part.                                                                                   |
| **`_get_current_tier(self)`**                                                            | - Renvoie le palier courant (allocation, max positions, multiplicateurs) selon le capital.                                                |
| **`_get_asset_price(self, asset_id)`**                                                   | - R√©cup√®re le prix ‚Äúcourant‚Äù d‚Äôun actif (ici simplifi√© √† `asset_i ‚Üí prix = i+1`).                                                         |
| **`_get_position_size(self, asset_id)`**                                                 | - Calcule la quantit√© √† trader en fonction de l‚Äôallocation du palier et du prix.                                                          |
| **`_calculate_fee(self, amount)`**                                                       | - Applique le pourcentage de transaction et le **frais fixe** pour renvoyer les frais totaux.                                             |
| **`_calculate_reward(self, old_val, new_val, penalties=0.0)`**                           |                                                                                                                                           |
|                                                                                          | - Transforme la variation de portefeuille en r√©compense via un log-return fa√ßonn√© par palier, p√©nalit√©s et clipping.                      |
| **`_execute_order(self, asset_id, action_type, quantity=None, order_type="MARKET", ‚Ä¶)`** |                                                                                                                                           |
|                                                                                          | - Coeur des **MARKET** et **ordres avanc√©s** (LIMIT, STOP\_LOSS, TAKE\_PROFIT, TRAILING\_STOP).                                           |
|                                                                                          | - V√©rifie : min\_order\_value, capital suffisant, max positions, expiration.                                                              |
|                                                                                          | - Modifie `self.capital`, `self.positions`, `self.orders`, et renvoie `(reward_mod, status, trade_info)`.                                 |
| **`_process_pending_orders(self)`**                                                      | - It√®re sur `self.orders` chaque step :                                                                                                   |
|                                                                                          | ‚Ä¢ Expire les ordres √† date, p√©nalit√©s associ√©es.                                                                                          |
|                                                                                          | ‚Ä¢ Ex√©cute les conditions LIMIT / STOP\_LOSS / TAKE\_PROFIT / TRAILING\_STOP.                                                              |
|                                                                                          | ‚Ä¢ Agr√®ge les `reward_mod` et `executed_orders_info`.                                                                                      |
| **`_display_trading_table(self, ‚Ä¶)`**                                                    | - G√©n√®re un affichage console riche (couleurs, tableaux) avec **rich** :                                                                  |
|                                                                                          | ‚Ä¢ Contexte temporel, financiers (avant/apr√®s), paliers, positions, ordres en attente, PnL, p√©nalit√©s, r√©compenses.                        |
| **`step(self, action)`**                                                                 | - Point d‚Äôentr√©e RL :                                                                                                                     |
|                                                                                          | 1. Snapshot avant (capital, portefeuille).                                                                                                |
|                                                                                          | 2. Traite `pending_orders`.                                                                                                               |
|                                                                                          | 3. Traduit `action` ‚Üí BUY/SELL/HOLD + asset.                                                                                              |
|                                                                                          | 4. Appelle `_execute_order`.                                                                                                              |
|                                                                                          | 5. Calcule `new_portfolio_value` + `reward` final via `_calculate_reward`.                                                                |
|                                                                                          | 6. Met √† jour `self.history`, `self.cumulative_reward`.                                                                                   |
|                                                                                          | 7. Affiche tableau via `_display_trading_table`.                                                                                          |
|                                                                                          | 8. Incr√©mente `self.current_step`, renvoie `(obs, reward, done, truncated, info)`.                                                        |
| **`_get_obs(self)`**                                                                     | - Construit l‚Äôobservation :                                                                                                               |
|                                                                                          | ‚Ä¢ S√©lectionne `self.numeric_cols` √† l‚Äô√©tape actuelle.                                                                                     |
|                                                                                          | ‚Ä¢ Normalise capital et positions.                                                                                                         |
|                                                                                          | ‚Ä¢ (Optionnel) encodeur/scaler + auto-encodeur.                                                                                            |
|                                                                                          | ‚Ä¢ Concat√®ne tout et renvoie un vecteur `np.ndarray`.                                                                                      |
| **`export_trading_data(self, export_dir=None)`**                                         |                                                                                                                                           |
|                                                                                          | - Exporte `self.history` et `self.trade_log` en CSV / Parquet + calcule statistiques de performance (win-rate, PnL moyen, dur√©e, etc.).   |
| **`render(self, mode="human")`**                                                         | - Affichage simple (optionnel) pour int√©gration OpenAI Gym.                                                                               |
| **`close(self)`**                                                                        | - √Ä la fin, appelle `export_trading_data` si demand√©, puis nettoie.                                                                       |

---

#### Comment ces fonctions s‚Äôencha√Ænent

1. **Initialisation** (`__init__`) ‚Üí d√©finit la logique m√©tier : frais, paliers, min/max, types d‚Äôordres.
2. **Reset** ‚Üí clean state + historique.
3. **Boucle d‚Äôentra√Ænement** (`env.step(action)`):

   * **Pending orders** (\_process\_pending\_orders)
   * **Ex√©cution action directe** (\_execute\_order)
   * **R√©compense** (\_calculate\_reward)
   * **Observation suivante** (\_get\_obs)
   * **Affichage** (\_display\_trading\_table)
   * **Historisation** + **export** √† la cl√¥ture

---

> **NB** : chaque ligne de code dans ces m√©thodes doit m√©riter sa place :
>
> * Vous contr√¥lez **min\_order\_value**, **frais**, **max positions**,
> * Vous tracez et p√©nalisez strictement : invalides, expirations, actions masqu√©es,
> * Vous enrichissez l‚Äôagent d‚Äôun feedback visuel et d‚Äôun historique complet pour le debug et l‚Äôanalyse.
### 5) √âtats (Observations)

L‚Äô**√©tat** (ou **observation**) fourni √† l‚Äôagent √† chaque pas de temps est un vecteur NumPy de dimension fixe, constitu√© de :

| **Bloc**                       | **Composants**                                                                     | **Type & dimension** | **R√¥le / Importance** |
| ------------------------------ | ---------------------------------------------------------------------------------- | -------------------- | --------------------- |
| **1. Caract√©ristiques march√©** | - Prix et volumes historiques √† l‚Äô√©tape courante  (close, open, high, low, volume) |                      |                       |

* Indicateurs techniques (ex : SMA, RSI, MACD, ATR‚Ä¶)
* Toute autre feature extraite ou engineer√©e (momentum, volatilit√©, sentiment‚Ä¶) | `float32[n_mkt]`  (ex. 43)  | Capturent la dynamique du march√© et ses tendances √† court/moyen terme. Crucial pour d√©cider BUY/SELL/HOLD.                                                                       |
  \| **2. Capital normalis√©**  | - `capital / initial_capital`                                                                             | `float32[1]`               | Informe l‚Äôagent de sa **puissance de feu** restante (risque global). Permet d‚Äôadapter l‚Äôagressivit√© de la strat√©gie (plus de capital ‚Üí plus d‚Äôexpositions possibles).          |
  \| **3. Positions normalis√©es** | Pour chaque actif *i* parmi les 5 :
* `position_qty_i / typical_position_size_i`
  o√π `typical_position_size_i = initial_capital / (price_i * factor)`                  | `float32[5]`               | Montre **o√π** et **combien** l‚Äôagent est d√©j√† expos√©.
  Permet d‚Äô√©viter surconcentration, de respecter les `max_positions` et les `allocation_frac`.                                                |
  \| **4. (Optionnel) Encodage Auto-Encoder** | - Encodage / r√©duction de dimension sur les `n_mkt` features via un mod√®le pr√©-entra√Æn√©      | `float32[n_enc]`           | Pour extraire des **repr√©sentations** plus robustes / compactes, r√©duire le bruit et faciliter l‚Äôapprentissage de l‚Äôagent, surtout si `n_mkt` est tr√®s √©lev√©.                   |

---

#### D√©tail technique

1. **Extraction des colonnes**

   ```python
   self.numeric_cols = self.data.select_dtypes(include=[np.number]).columns
   market_row = self.data.iloc[self.current_step][self.numeric_cols]
   market_features = market_row.values.astype(np.float32)
   ```

   * **Qualit√© des donn√©es** : v√©rifier qu‚Äôil n‚Äôy a pas de NaN ou d‚Äôincoh√©rences (ex. nombre de features ‚â† scaler attendu).
   * **Synchronisation** : l‚Äôindex `current_step` doit correspondre √† un horodatage unique pour tous les actifs (alignement multi-actifs).

2. **Capital normalis√©**

   ```python
   normalized_capital = self.capital / self.initial_capital
   ```

   * Conserve la valeur dans \[0, 1], facilite le **generalization** entre √©pisodes de tailles de capital diff√©rentes.

3. **Positions normalis√©es**

   ```python
   positions = np.zeros(len(self.assets), dtype=np.float32)
   for i, a in enumerate(self.assets):
       if a in self.positions:
           typical = self.initial_capital / (self._get_asset_price(a) * 10)
           positions[i] = self.positions[a]["qty"] / typical
   ```

   * Le **facteur** 10 (modifiable) fixe l‚Äô√©chelle typique d‚Äôune position ; on peut ajuster selon la tol√©rance au risque ou la granularit√© souhait√©e.

4. **Encodage (facultatif)**

   ```python
   if self.encoder and self.scaler:
       scaled = self.scaler.transform([market_features])
       encoded = self.encoder.predict(scaled)[0]
       obs = np.concatenate([encoded, [normalized_capital], positions])
   else:
       obs = np.concatenate([market_features, [normalized_capital], positions])
   ```

   * **Attention** : l‚Äôencodeur et le scaler doivent avoir √©t√© form√©s sur le m√™me nombre et ordre de features que ceux fournis ici. Toute incoh√©rence l√®vera une erreur de dimension (‚Äú43 vs 42‚Äù).

---

#### Pourquoi c‚Äôest crucial

* **Repr√©sentation riche** : combiner prix purs + indicateurs + √©tat du portefeuille offre √† l‚Äôagent tout le contexte pour √©valuer opportunit√©s et risques.
* **√âchelle uniforme** : normalisation garantit que l‚Äôagent ne soit pas biais√© par l‚Äôamplitude absolue du capital ou des volumes.
* **Adaptabilit√©** : en cas d‚Äôajout de nouvelles features (ex. regimes de march√© d√©tect√©s, volatilit√© impl√©ment√©e), il suffit de les int√©grer dans `numeric_cols` et de recomposer l‚Äôobs.

---

> **Next steps / V√©rifications**
>
> 1. Valider que `self.numeric_cols` ne change pas en cours d‚Äôex√©cution (m√™me nombre de colonnes).
> 2. S‚Äôassurer que `initial_capital` soit coh√©rent entre training et test (pas de fuite d‚Äôinformation).
> 3. Tester les valeurs limites (capital quasi nul, positions max√©es) pour confirmer que l‚Äôobs reste dans un intervalle raisonnable.
> 4. Si vous utilisez un encodeur, **re‚Äêentra√Æner** scaler+auto-encodeur d√®s que vous modifiez le jeu de features.
### 6) Actions disponibles √† l‚Äôagent

L‚Äô**espace d‚Äôaction** est discret (`spaces.Discrete(11)`) et comprend **11 actions** :

| **Code**                                                          | **Action**         | **Description**                                         | **Type d‚Äôordre**    | **Contraintes cl√©s**                                                                     |
| ----------------------------------------------------------------- | ------------------ | ------------------------------------------------------- | ------------------- | ---------------------------------------------------------------------------------------- |
| `0`                                                               | HOLD               | Ne rien faire (pas d‚Äôouverture/fermeture de position)   | ‚Äî                   | Aucune, mais g√©n√®re une petite p√©nalit√© de temps pour √©viter l‚Äô**inaction prolong√©e**    |
| `1‚Äì5`                                                             | BUY\_ASSET\_i      | Ouvrir (ou ajouter √†) une position LONG sur l‚Äôactif *i* | MARKET (par d√©faut) | ¬∑ **Valeur mini** : \$10   <br>¬∑ **Position Size** ‚â§ `allocation_frac * capital / price` |
| ¬∑ Nb positions ouvertes < `max_positions` de palier courant       |                    |                                                         |                     |                                                                                          |
| `6‚Äì10`                                                            | SELL\_ASSET\_{i-5} | Fermer (ou r√©duire) la position LONG sur l‚Äôactif *i-5*  | MARKET              | ¬∑ Doit **exister** une position ‚â• quantit√© vendue                                        |
| ¬∑ Pas de **short** (pas d‚Äôeffet de levier ou positions n√©gatives) |                    |                                                         |                     |                                                                                          |

---

#### Extensions / Types d‚Äôordres avanc√©s

En plus des `MARKET`, l‚Äôagent peut **cr√©er** (via `_execute_order(..., order_type=...)`) jusqu‚Äô√† 5 autres types d‚Äôordres, chacun avec logique et **conditions** associ√©es :

| **Type d‚Äôordre** | **Cr√©ation**                        | **Ex√©cution**                                                                                                             | **Expiration**            | **P√©nalit√© si expir√©** |
| ---------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------- | ---------------------- |
| `LIMIT`          | BUY/SELL √† un **prix limite** donn√© | Ex√©cution automatique lorsque le prix du march√© atteint (‚â§ pour BUY / ‚â• pour SELL)                                        | TTL par d√©faut = 10 steps | ‚Äì0.1                   |
| `STOP_LOSS`      | SELL √† un **prix stop** sp√©cifi√©    | Active une vente si le prix retombe ‚â§ `stop_price`                                                                        | TTL = 10 steps            | ‚Äì0.1                   |
| `TAKE_PROFIT`    | SELL √† un **prix cible**            | Active une vente si le prix atteint ‚â• `take_profit_price`                                                                 | TTL = 10 steps            | ‚Äì0.0 (aucune p√©nalit√©) |
| `TRAILING_STOP`  | SELL avec **suivi dynamique**       | Met √† jour `stop_price` au fil de la hausse du cours (`highest_price ‚àí trailing_pct%`) et vend si le prix chute ‚â§ ce stop | TTL = 10 steps            | ‚Äì0.05                  |
| `STOP_LIMIT`     | STOP\_LOSS + LIMIT                  | Apr√®s d√©clenchement du stop, passe un ordre LIMIT au prix sp√©cifi√© (√©vite slippage)                                       | TTL = 10 steps            | ‚Äì0.1                   |

> **NB** : Tous les ordres avanc√©s respectent aussi :
>
> * **Montant minimal** de transaction (`min_order_value` = \$10)
> * **Capital disponible** apr√®s frais (`transaction_cost_pct` + `fixed_fee`)
> * **Max positions** du palier courant (en cumul√© : positions ouvertes + ordres en attente)

---

#### S√©lection de la paire et multi-actifs

* **Actifs** : 5 paires (nomm√©es `asset_0` √† `asset_4`)
* L‚Äôagent choisit une seule paire par BUY/SELL, ce qui permet de diversifier les expositions **par palier** :

  * Palier 0 (capital < \$30) ‚Üí ‚â§ 1 position
  * Palier 1 (30‚Äì75) ‚Üí ‚â§ 2 positions
  * etc.

---

#### Pourquoi ces actions ?

1. **Couverture fonctionnelle** : couvre tous les sc√©narios de trading spot (achat, vente, limit, stop, profit, trailing).
2. **Gestion du risque** : respect des minimas, nombre de positions limit√©, frais anticip√©s.
3. **Modularit√©** : on peut ajouter facilement de nouveaux types d‚Äôordres (ex. `OCO`, `ICEBERG`) en reprenant la m√™me structure `_execute_order` / `_process_pending_orders`.

---

> **Checks √† impl√©menter** avant la phase finale de training RL
>
> * **Validation** que chaque action cod√©e entre 0‚Äì10 passe bien toutes les v√©rifications (`min_order_value`, `max_positions`, `capital_available`)
> * **Simulation** d‚Äôexemples d‚Äôordres avanc√©s pour confirmer l‚Äôex√©cution/expiration/annulation
> * **Logging**: conserver en `info["trades"]` le d√©tail complet (`order_type`, `original_order_type`, `reason`, `pnl`, etc.)
> * **Test unitaires** couvrant toutes les combinaisons action‚Äì√©tat‚Äìpalier pour √©viter des **invalid actions loops**

Avec cette d√©finition pr√©cise des actions et de leurs contraintes, votre agent pourra trader de fa√ßon robuste et conforme √† vos r√®gles de gestion. Lorsque vous √™tes pr√™t, dites **7** pour aborder la partie *r√©compenses* (reward shaping) !


### 7) R√©compenses (Reward Shaping)

Pour guider l‚Äôagent vers un trading rentable et disciplin√©, nous d√©finissons une **fonction de r√©compense** combinant plusieurs composantes :

| Composante                                                             | Formule / M√©canisme                                                                      | Objectif                                                                                  |
| ---------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| **Rendement logarithmique**                                            | $r_{\text{log}} = \ln\!\bigl(\tfrac{\text{valeur\_apr√®s}}{\text{valeur\_avant}}\bigr)$   | Capturer la performance relative (gain ou perte en %), sym√©trique pour hausses et baisses |
| **Multiplicateurs de palier**                                          | Si $r_{\text{log}}\ge0$,                                                                 |                                                                                           |
| ‚ÄÉ‚ÄÉ$r_{\text{shaped}} = r_{\text{log}} \times \text{reward\_pos\_mult}$ |                                                                                          |                                                                                           |
| Sinon                                                                  |                                                                                          |                                                                                           |
| ‚ÄÉ‚ÄÉ$r_{\text{shaped}} = r_{\text{log}} \times \text{reward\_neg\_mult}$ | Renforcer les gains et/ou p√©naliser plus fortement les pertes selon la taille du capital |                                                                                           |
| **P√©nalit√©s d‚Äôaction invalides**                                       |                                                                                          |                                                                                           |
| & **Hold forc√©**                                                       | ‚Äì0.2 par ordre rejet√© (min order, pas de position, max positions‚Ä¶)                       |                                                                                           |
| ‚Äì0.05 par step de HOLD invalid√©                                        |                                                                                          |                                                                                           |
| ‚Äì0.001 par step de d√©lai (time penalty)                                | √âviter que l‚Äôagent reste inactif ou encha√Æne des actions impossibles                     |                                                                                           |
| **P√©nalit√©s d‚Äôexpiration**                                             | ‚Äì0.1 pour un LIMIT/STOP\_LOSS expir√©, ‚Äì0.05 pour un TRAILING\_STOP expir√©                | Encourager l‚Äôagent √† placer des ordres r√©alistes et √† suivre leurs ex√©cutions rapidement  |
| **Bonus take-profit**                                                  | +1% du PnL lorsque `pnl > 0`, plafonn√© √† \$1                                             | R√©compenser la prise de profits et la bonne d√©tection des points hauts                    |
| **Clipping final**                                                     | $\text{reward} = \text{clip}(r_{\text{shaped}} - \text{p√©nalit√©s},\; -10,\; +10)$        | √âviter les valeurs extr√™mes qui destabilisent l‚Äôapprentissage                             |

---

#### Processus de calcul dans `step()`

1. **Snapshot ‚Äúavant‚Äù**

   * $V_{\text{avant}} = \text{capital}_{t} + \sum \text{positions}_t$.
2. **Traitement des ordres pendants**

   * Appliquer p√©nalit√©s d‚Äôexpiration et gains de take-profit automatiques, accumuler `pending_reward`.
3. **Ex√©cution de l‚Äôaction choisie**

   * Acheter/vendre ou rejeter (+ p√©nalit√© imm√©diate si invalid).
4. **Snapshot ‚Äúapr√®s‚Äù**

   * $V_{\text{apr√®s}} = \text{capital}_{t+1} + \sum \text{positions}_{t+1}$.
5. **Calcul du reward brut**

   $$
     r_{\text{log}} = \ln\!\bigl(\tfrac{V_{\text{apr√®s}}}{V_{\text{avant}}}\bigr), \quad
     r_{\text{shaped}} = 
       \begin{cases}
         r_{\text{log}} \times \text{pos\_mult}, & r_{\text{log}}\ge0,\\
         r_{\text{log}} \times \text{neg\_mult}, & r_{\text{log}}<0.
       \end{cases}
   $$
6. **D√©duction des p√©nalit√©s**

   * `r = r_shaped ‚Äì penalties ‚Äì time_penalty`.
7. **Bonus**

   * Ajouter 1% du PnL pour les trades gagnants.
8. **Clipping**

   * `r = clip(r, ‚Äì10, +10)`.
9. **Mise √† jour cumulative**

   * `cumulative_reward += r`.

---

#### Pourquoi ce shaping ?

* **Log-return** : bien adapt√© aux finances (invariance multiplicative).
* **Paliers** : adaptent l‚Äôaversion au risque selon la taille du capital.
* **P√©nalit√©s d‚Äôinvalides** : emp√™chent l‚Äôagent d‚Äôexploiter des boucles de HOLD ou d‚Äôordres impossibles pour maximiser la r√©compense‚Ä¶
* **Bonus take-profit** : favorise la r√©alisation concr√®te des gains plut√¥t que la simple valorisation th√©orique.
* **Clipping** : stabilise l‚Äôentra√Ænement RL en √©vitant des extr√™mes.

Lorsque vous √™tes pr√™t, dites **8** pour discuter des **√©tats et features** utilis√©s par l‚Äôagent !


### 8) √âtats & Features (Repr√©sentation du State)

L‚Äô**√©tat** observ√© par l‚Äôagent √† chaque pas de temps combine :

1. **Features de march√©** (historiques/pr√©dictives)
2. **Features de portefeuille** (capital & positions)

---

#### 1. Market Features (ùêå ‚àà ‚Ñù·¥∫)

Ces colonnes sont extraites directement du jeu de donn√©es (CSV / Parquet) et typiquement calcul√©es en amont :

* Cours : `open`, `high`, `low`, `close`
* Volume
* Indicateurs techniques classiques : `ma_10` (moyenne mobile 10 p√©riodes), `rsi` (Relative Strength Index), etc.
* (√âventuellement) autres momentum, volatilit√©, bandes de Bollinger‚Ä¶

> **Dimension** : N = nombre de colonnes num√©riques dans vos donn√©es.
> **Qualit√© & importance** :
>
> * Choisir des indicateurs fiables pour votre horizon (1 min, 5 min, 1 h‚Ä¶).
> * Plus vous donnez de features pertinentes, mieux l‚Äôagent peut capturer la structure du march√©.
> * √âvitez la redondance et le bruit excessif (feature selection).

---

#### 2. Portfolio Features (ùë∑ ‚àà ‚Ñù·µè)

Nous ajoutons **k = 1 + (# actifs)** dimensions pour informer l‚Äôagent sur son portefeuille :

| Feature                   | Description                                  |
| ------------------------- | -------------------------------------------- |
| **Normalized Capital**    | capital / initial\_capital ‚àà \[0,‚àû)          |
| **Positions Normalis√©es** | pour chaque actif *i* : qty·µ¢ / typical\_qty·µ¢ |

* **typical\_qty·µ¢** = (initial\_capital) √∑ (prix·µ¢ √ó facteur), o√π `facteur` est une constante (ex : 10)
  ‚Üí met toutes les quantit√©s sur une m√™me √©chelle relative

> **Dimension** : k = 1 + 5 (pour 5 paires/actifs).
> **Pourquoi ?**
>
> * L‚Äôagent doit savoir **combien** il d√©tient de chaque actif pour d√©cider de SELL vs HOLD vs BUY.
> * Le capital normalis√© l‚Äôaide √† comprendre sa taille relative (paliers).

---

#### 3. Encodage / Scaling (optionnel)

Si vous avez un auto‚Äêencodeur ou un `scaler` entra√Æn√© (PCA, Keras-AE‚Ä¶) :

1. **Scaler** standardise les features de march√© (moyenne 0, √©cart-type 1)
2. **Encodeur** (auto‚Äêencodeur) r√©duit la dimensionnalit√© et extrait des repr√©sentations non-lin√©aires

> **Avantage** : r√©duire le bruit, acc√©l√©rer l‚Äôentra√Ænement, extraire des facteurs latents.
> **Attention** : veillez √† garder coh√©rents le nombre de features et l‚Äôordre lors du `fit` / `transform` pour √©viter les incoh√©rences (erreur ‚Äú43 vs 42‚Äù).

---

### ‚û§ **R√©sum√© du vecteur d‚Äôobservation**

$$
\text{state} =  
\bigl[\,\underbrace{f_1, f_2, \dots, f_N}_{\text{market features}},\;\;
\underbrace{\tfrac{\text{capital}}{\text{initial\_capital}}}_{\text{normalized cap}},\;
\underbrace{\tfrac{\text{qty}_0}{\text{typical}_0},\dots,\tfrac{\text{qty}_4}{\text{typical}_4}}_{\text{positions norm.}}\bigr]
\in \mathbb{R}^{N + 6}.
$$

* **N** = # de colonnes num√©riques
* **+6** = 1 (capital) + 5 (actifs)

---

> **Prochaine √©tape** : dites **9** pour passer aux **actions & espace d‚Äôaction** (ordre, dimension, mapping).

### 9) Actions & Espace d‚ÄôAction

L‚Äôagent dispose de **11 actions discr√®tes** (espace `Discrete(11)`) r√©parties ainsi :

| Code | Action             | Description                                                           |
| ---- | ------------------ | --------------------------------------------------------------------- |
| 0    | **HOLD**           | Ne rien faire ce pas de temps                                         |
| 1    | **BUY\_ASSET\_0**  | Acheter l‚Äôactif `asset_0` au prix du march√© (order\_type=MARKET)      |
| 2    | **BUY\_ASSET\_1**  | Acheter l‚Äôactif `asset_1`                                             |
| 3    | **BUY\_ASSET\_2**  | Acheter l‚Äôactif `asset_2`                                             |
| 4    | **BUY\_ASSET\_3**  | Acheter l‚Äôactif `asset_3`                                             |
| 5    | **BUY\_ASSET\_4**  | Acheter l‚Äôactif `asset_4`                                             |
| 6    | **SELL\_ASSET\_0** | Vendre la position sur `asset_0` (order\_type=MARKET), si elle existe |
| 7    | **SELL\_ASSET\_1** | Vendre la position sur `asset_1`                                      |
| 8    | **SELL\_ASSET\_2** | Vendre la position sur `asset_2`                                      |
| 9    | **SELL\_ASSET\_3** | Vendre la position sur `asset_3`                                      |
| 10   | **SELL\_ASSET\_4** | Vendre la position sur `asset_4`                                      |

---

#### Mapping & Logique interne

1. **Interpr√©ter l‚Äôaction**

   ```python
   if action == 0:          action_type=0, asset_idx=None
   elif 1 ‚â§ action ‚â§ 5:     action_type=1 (BUY),  asset_idx=action-1
   elif 6 ‚â§ action ‚â§ 10:    action_type=2 (SELL), asset_idx=action-6
   ```

2. **V√©rifications avant ex√©cution**

   * **Min Order Value** : chaque ordre (`quantity √ó price`) ‚â• `min_order_value` (ex. 10 \$).
   * **Capital disponible** : co√ªt total (valeur + frais) ‚â§ `capital`.
   * **Max Positions** : nombre de positions ouvertes ‚â§ `tier["max_positions"]`.

3. **Types d‚Äôordres**

   * **MARKET** (ex√©cution imm√©diate)
   * **LIMIT**, **STOP\_LOSS**, **TAKE\_PROFIT**, **TRAILING\_STOP** (ordres avanc√©s via `_process_pending_orders`).

4. **Frais & p√©nalit√©s**

   * `fee = transaction_cost_pct * value + fixed_fee`
   * Invalidations donnent p√©nalit√© fixe (ex. ‚àí0.2) et masque l‚Äôaction en `HOLD` avec petite p√©nalit√© de temps (‚àí0.001).

---

#### Pourquoi ce choix‚Äâ?

* **Discrete** est simple √† entra√Æner (PPO, DQN, etc.).
* **Mapping clair** : chaque index correspond √† (type, actif).
* **Extensible** : on peut ajouter ordres avanc√©s si on enrichit l‚Äôespace (mais complexit√© RL augmente).

---

> **Prochaine √©tape** : tapez **10** pour passer au **Reward Design** (fonction de r√©compense).

### 10) Reward Design (Fonction de R√©compense)

La fonction de r√©compense est au c≈ìur de l‚Äôapprentissage par renforcement : elle guide l‚Äôagent vers le comportement souhait√©. Pour notre environnement de trading, voici les composantes et justifications :

| Composante                     | Formule / Valeur                                                       | But                                                             |
| ------------------------------ | ---------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Rendement Logarithmique**    | $r_t = \ln\!\Big(\frac{V_{t+1} + \varepsilon}{V_t + \varepsilon}\Big)$ | Capture les gains proportionnels, sym√©trique pour gains/pertes. |
| **Multiplicateurs par Palier** |                                                                        |                                                                 |
| ‚Äì Gains                        | $r_t^+ = r_t \times \text{reward\_pos\_mult}$                          | Renforce plus fortement les gains aux paliers sup√©rieurs.       |
| ‚Äì Pertes                       | $r_t^- = r_t \times \text{reward\_neg\_mult}$                          | Dissuade les pertes en les p√©nalisant plus aux paliers √©lev√©s.  |
| **P√©nalit√©s**                  |                                                                        |                                                                 |
| ‚Äì Invalidation d‚Äôordre         | $-0.2$                                                                 | D√©courage l‚Äôagent de placer des ordres non ex√©cutables.         |
| ‚Äì Expiration d‚Äôordre           | $-0.1$ (Limit/SL), $-0.05$ (Trailing Stop)                             | √âvite les ordres dormants trop longs sans ex√©cution.            |
| ‚Äì Temps (pas de trade)         | $-0.001$                                                               | Encourage √† agir plut√¥t qu‚Äô√† rester passif ind√©finiment.        |
| **Clipping**                   | $\text{clip}(r_t, -10, +10)$                                           | Stabilise l‚Äôentra√Ænement en bornant l‚Äôamplitude extr√™me.        |

#### Calcul complet par pas de temps :

1. **Avant** :

   * $V_t =$ valeur du portefeuille au d√©but du step.
2. **Ex√©cution** :

   * Appliquer ordres, frais, PnL, bonus sur trades ponctuels.
3. **Apr√®s** :

   * $V_{t+1} =$ valeur du portefeuille apr√®s ex√©cution.
4. **Log-return** :

   * $r = \ln\frac{V_{t+1}}{V_t}$.
5. **Shaping** :

   * Si $r\ge0$ ‚Üí $r \times \text{reward\_pos\_mult}$
   * Si $r<0$ ‚Üí $r \times \text{reward\_neg\_mult}$
6. **Soustraction des p√©nalit√©s** (invalidations, expiration, inactivit√©).
7. **Clipping** et sortie comme `reward`.

---

#### Pourquoi ?

* **Log-return** normalise les gains/pertes quel que soit le capital (scale invariant).
* **Multiplicateurs** adaptent l‚Äôaversion/acc√©l√©ration du risk-taking selon le capital.
* **P√©nalit√©s** disciplinent le comportement (√©viter spam d‚Äôordres invalides ou passivit√©).
* **Clipping** assure une stabilit√© num√©rique et √©vite les pics extr√™mes qui d√©stabilisent l‚Äôagent.

---

üéØ **Objectif** : maximiser la croissance nette du portefeuille tout en se conformant aux contraintes de risque et de palier.

> Tapez **11** pour passer aux **Observations & Features** (structure des √©tats).

### 11) Observations & Features (√âtats)

L‚Äôagent per√ßoit √† chaque pas un vecteur d‚Äô**√©tat** qui combine :

---

#### A. **Donn√©es de march√©**

Pour chaque actif consid√©r√© (ici 5 paires), on extrait un ensemble de **features techniques** issues du dataset :

* **Cours OHLCV** : open, high, low, close, volume
* **Indicateurs d√©riv√©s** (exemple) :

  * Moyennes mobiles (MA ‚Äì ex. MA\_10, MA\_50)
  * Indicateur de force relative (RSI)
  * Bandes de Bollinger, MACD, etc.
* **Dimension** : si vous gardez 7 indicateurs par actif ‚Üí 5 √ó 7 = 35 features

> **Pourquoi ?**
> Ces indicateurs capturent tendances, momentum et volatilit√©, permettant √† l‚Äôagent de d√©tecter des signaux d‚Äôachat/vente.

---

#### B. **Features de portefeuille**

1. **Capital Normalis√©**

   $$
     c_t^\text{norm} = \frac{\text{capital}_t}{\text{capital}_\text{initial}}
   $$
2. **Positions** (pour chacun des 5 actifs) :

   $$
     p_{i,t} = \frac{\text{qty}_{i,t}}{\text{position\_typique}_i}
     \quad\text{o√π}\quad
     \text{position\_typique}_i = \frac{\text{capital}_\text{initial}}{\text{prix}_i \times 10}
   $$

   ‚Üí donne une valeur relative entre 0 et 1

> **Dimension** : 1 (capital) + 5 (positions) = **6 features**

---

#### C. **Etat de l‚Äôordre & du palier** *(optionnel mais utile)*

* **Tier actuel** (one-hot ou index num√©rique)
* **Nombre de positions ouvertes** vs **max\_positions**
* **Exposition relative** (allocation\_frac)
* **Nombre d‚Äôordres en attente**

> **Pourquoi ?**
> Permet √† l‚Äôagent de savoir s‚Äôil a atteint ses limites de risque/exposition et d‚Äôadapter son comportement.

---

#### D. **Encodage & Normalisation**

* **Scaler** (StandardScaler ou MinMax) sur les features de march√©
* **Encodage** (auto-encodeur ou PCA) pour r√©duire la dimension si n√©cessaire

> **Attention** :
>
> * **Incoh√©rence** ‚Äú43 vs 42 features‚Äù ‚Üí v√©rifier que le scaler et le vecteur d‚Äô√©tat utilisent **exactement** la m√™me dimension et les m√™mes colonnes, dans le m√™me ordre.
> * Toujours recalibrer le **scaler** lorsque vous modifiez le jeu de donn√©es ou le nombre d‚Äôindicateurs.

---

### üëâ **R√©sum√©**

| Cat√©gorie              | Features                    | Dimension  |
| ---------------------- | --------------------------- | ---------- |
| March√©                 | OHLCV + indicateurs (\~7√ó5) | \~35       |
| Portefeuille           | Capital norm. + Positions   | 6          |
| Palier & Ordres (opt.) | Tier, expos., n\_orders     | 3‚Äì5        |
| **Total**              |                             | **‚âà44‚Äì46** |

Ces **√©tats** repr√©sentent le **contexte complet** (march√© + portefeuille + contraintes) n√©cessaire √† l‚Äôagent pour d√©cider et apprendre efficacement.
Voici une proposition de **standardisation** pour vos features, un jeu de **7 indicateurs cl√©s** par timeframe, un **workflow Colab** pour r√©cup√©rer vos donn√©es (en tenant compte des restrictions r√©gionales) et quelques suggestions ¬´ bonus ¬ª pour rendre ADAN encore plus robuste.

---

## 1. Convention de nommage

| Cat√©gorie           | Exemple de nom                             | Description                                     |
| ------------------- | ------------------------------------------ | ----------------------------------------------- |
| Prix bruts          | `open`, `high`, `low`, `close`, `volume`   | Valeurs OHLCV classiques                        |
| Moyennes            | `ema_10`, `ema_20`, `sma_50`               | Moyennes exponentielles (10, 20) et simple (50) |
| Momentum            | `rsi_14`, `macd_12_26_9`                   | RSI sur 14 p√©riodes, MACD (12,26, signal 9)     |
| Volatilit√©          | `bb_upper_20_2`, `bb_lower_20_2`, `atr_14` | Bandes de Bollinger (20, √©cart 2), ATR 14       |
| Tendance            | `adx_14`, `ma_200`                         | ADX 14 pour la force de tendance, MA 200 long   |
| Volume              | `obv`, `fvwap_1h`                          | On-Balance Volume, VWAP intra-horaire           |
| Cycle / Oscillateur | `stoch_k_14_3`, `stoch_d_14_3`             | Stochastique %K et %D                           |

> **Exemple de colonnes** pour 1 timeframe (e.g. 1 h) :
>
> ```
> ['open','high','low','close','volume',
>  'ema_10','ema_20','sma_50',
>  'rsi_14','macd_12_26_9',
>  'bb_upper_20_2','bb_lower_20_2','atr_14',
>  'adx_14','ma_200',
>  'obv','fvwap_1h',
>  'stoch_k_14_3','stoch_d_14_3']
> ```
>
> Vous pouvez bien s√ªr ajuster les p√©riodes.

---

## 2. 7 indicateurs prioritaires par timeframe

1. **Tendance long terme** : `ma_200`
2. **Tendance court terme** : `ema_10`
3. **Volatilit√©** : `atr_14`
4. **Momentum** : `rsi_14`
5. **Cross-momentum** : `macd_12_26_9`
6. **Bandes de Bollinger** : `bb_upper_20_2` / `bb_lower_20_2`
7. **Volume** : `obv`

> Pour chaque timeframe (1 min, 5 min, 1 h, 24 h), dupliquez ces colonnes en suffixant le timeframe (e.g. `rsi_14_5m`, `atr_14_1h`).

---

## 3. Workflow Colab pour assembler votre dataset

1. **Installer les librairies**

   ```python
   !pip install python-binance pandas ta
   ```

2. **Configurer vos cl√©s API**

   ```python
   from binance.client import Client
   import os
   API_KEY = os.environ.get("BINANCE_API_KEY")
   API_SECRET = os.environ.get("BINANCE_API_SECRET")
   client = Client(API_KEY, API_SECRET)
   ```

3. **G√©rer les restrictions g√©ographiques**

   * Certains pays (√âtats-Unis, Canada, UK‚Ä¶) exigent une API d√©di√©e (Binance .US, etc.).
   * **Astuce** : dans Colab, utilisez un VPN ou un proxy autoris√©, ou basculez vers un exchange local (Coinbase Pro, Kraken) si Binance est bloqu√©.

4. **T√©l√©charger les donn√©es OHLCV**

   ```python
   import pandas as pd
   def fetch_ohlcv(symbol, interval, start_str, end_str=None):
       klines = client.get_historical_klines(symbol, interval, start_str, end_str)
       df = pd.DataFrame(klines, columns=[
           'open_time','open','high','low','close','volume', *_ 
       ])  # adapter
       df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
       return df.set_index('open_time').astype(float)

   df_1h = fetch_ohlcv("BTCUSDT", "1h", "1 year ago UTC")
   ```

5. **Calculer les indicateurs**

   ```python
   import ta
   df = df_1h.copy()
   # Exemple : RSI
   df['rsi_14'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()
   # ATR
   df['atr_14'] = ta.volatility.AverageTrueRange(df['high'],df['low'],df['close'],window=14).average_true_range()
   # ... et ainsi de suite pour vos 7 indicateurs
   ```

6. **Assembler plusieurs timeframes**

   * T√©l√©chargez 1 m, 5 m, 24 h de la m√™me fa√ßon.
   * **Merge** sur l‚Äô`open_time` (par exemple en upsampling / forward-fill).
   * Conservez uniquement les pas 1 m pour l‚Äôentra√Ænement, en int√©grant les colonnes des autres timeframes.

7. **Sauvegarde**

   ```python
   df.to_parquet("dataset_adan.parquet")
   ```

---

## 4. Bonus & recommandations ¬´ ADAN ¬ª

* **Backtesting & validation** :

  * S√©parez votre jeu en **train/val/test** (80 %/10 %/10 %).
  * V√©rifiez le **surapprentissage** : comparez performances en backtest & paper-trading.

* **Feature Drift detection** :

  * Int√©grez un module pour d√©tecter les d√©rives de vos indicateurs (p. ex. un changement soudain de volatilit√©).

* **Monitoring & Alerting** :

  * Loggez les d√©cisions d‚ÄôADAN et mettez en place des alertes (Slack, email) si un comportement anormal se d√©clenche.

* **Robustesse g√©o-politique** :

  * Pr√©parez des **fallbacks** d‚Äôexchanges (p. ex. Kraken, FTX US) si l‚Äôacc√®s √† Binance devient impossible.

* **Documentez vos versions** :

  * Versionnez vos colonnes (ex. `v1.0_indicators.yaml`) pour garder la tra√ßabilit√©.

* **Esth√©tique & UX** :

  * Donnez √† ADAN un **dashboard minimaliste** (Plotly, Streamlit) montrant : capital, PnL cumulatif, heatmap de positions, signaux d‚Äôentr√©e/sortie.

---

üéØ Avec cette structure, ADAN aura :

1. **Un √©tat coh√©rent** et complet (44‚Äì46 dims),
2. **7+ indicateurs** par timeframe, bien nomm√©s,
3. **Un pipeline Colab** robuste face aux restrictions r√©gionales,
4. **Des modules bonus** pour garantir tra√ßabilit√©, robustesse et pr√©sentation claire.

Bonne mise en place !

